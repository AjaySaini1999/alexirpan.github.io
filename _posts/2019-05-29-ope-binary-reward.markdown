---
layout: post
title:  "Off-Policy Evaluation of Generalization for Deep Q-Learning in Binary Reward Tasks"
date:   2019-05-26 22:30:00 -0700
---

A paper I've been working on for a while went on arXiv recently. I'm pretty
happy about where it landed.

First, the paper link:

Off-Policy Evaluation of Generalization for Deep Q-Learning in Binary Reward Tasks (LINK)

Alex Irpan, Kanishka Rao, Konstantinos Bousmalis, Chris Harris, Julian Ibarz, Sergey Levine

This paper is primarily an off-policy evaluation paper. I was less familiar
with this problem before working on this paper, but I now believe it's a problem
that's both really important and really understudied.

This post is **not** trying to summarize the entire paper. Instead, it aims to
motivate why off-policy evaluation is an important problem, and provide a
high-level overview of our approach and results.


Why Off-Policy Evaluation?
---------------------------------------------------------------------------

In off-policy evaluation (or OPE), we have a fixed validation set $$\mathcal{D}$$
of real-world interactions, as generated by some behavior policy $$\pi_b$$.
From this dataset, we'd like to estimate or bound performance of a learned policy
$$\pi$$.

This project started around the time we were getting good results from applying
Q-learning to grasping on real robots. As next steps, we discussed ways we could
improve performance or data efficiency of the model. One prime candidate was
improving the model architecture, since our model was still based off an
AlexNet-style CNN.

As we talked about doing more extensive hyperparameter tuning or architecture
tuning, we realized that our bottleneck would be real world evaluation.
In a simulated reinforcement learning setup, because your environment is
defined in simulation, you can simply use more CPUs to speed up evaluation. In
the real world, you can't do this, unless you have more robots to run
evaluation over - and we didn't have more robots.

After thinking about this more, we realized this wasn't just a hyperparameter
tuning problem. It was a general model selection problem. Whenever you change
your learning algorithm, neural net architecture, or dataset in any way, you'll get
a new model. How do you select between those models without an estimate of their
performance?

Robust off-policy evaluation is a deceptively powerful tool. First, you can do
more efficient model selection for real-world environments. Second, you can
evaluate generalization to real-world environments without running evaluation in
the real-world. This is especially important when benchmarking different
sim-to-real algorithms. You can try as many different domain randomizations and
data augmentation techniques as you want, but in the end, you have to estimate
real-world performance one way or another.

Finally, although this is a longer term point, robust OPE has the potential to improve research progress as a whole for
real-world RL problems. One of the problems with real-world RL is that everyone's
real-world environment is somewhat different. This makes it difficult to
truly reproduce someone else's work, because you don't have access to the
real-world evaluation setup they've been using. But, if we had a standardized
validation set $$\mathcal{D}$$ from an environment, and an appropriate OPE
metric over that validation set, it's possible for other researchers to reproduce
and iterate on results for an environment they don't have direct access to. This
fast iteration on a standardized test set is one reason image classification
progressed so quickly.


Prior Work
-------------------------------------------------------------------------

When doing a literature search, we found that OPE methods commonly used either
importance sampling or model learning. In importance sampling, you use
$$\frac{\pi(a|s)}{\pi_b(a|s)}$$ to reweight your data, possibly with additional
tricks to reduce variance. In model learning, you learn a model of the environment,
then rollout your learned $$\pi$$ in that environment. Unfortunately, neither
of these approaches was a good fit for our use case.
We had a validation set $$\mathcal{D}$$, but did not have the $$\pi_b(a|s)$$
of executed actions. Meanwhile, since we were working with image data, learning
a model of the environment would require solving a video prediction problem, which
was a research problem by itself.

So, we set out to derive an OPE method that would work on image data, not
require knowledge of $$\pi_b$$ or importance sampling, and only require
a Q-function estimate $$Q(s,a)$$. Such a method was likely not going to perform
as well as a method that knows $$\pi_b$$ or which can train a good model, since
it doesn't have as much information, but it would be applicable to more
setups - including the robot grasping task that originally motivated this
work.


Method
--------------------------------------------------------------

We made the following observations. First, although our end goal is to estimate
the return of a policy $$R(\pi)$$, for model selection, it's enough to estimate
some statistic that correlates well with $$R(\pi)$$.

Second, if we restrict the
analysis to binary reward MDPs, we could treat off-policy evaluation as a
classification problem. An MDP is a binary reward MDP if it has deterministic dynamics, and a sparse
reward function that is $$0$$ everywhere except the final timestep, where it is
$$0$$ or $$1$$, indicating failure or success at some task. The deterministic
dynamics in such an MDP give the following property: an action $$a$$ either
could lead to success, or is guaranteed to lead to failure. This let us define a label for each $$(s,a)$$.

**Definition:** In a binary reward MDP, $$(s_t,a_t)$$ is **effective** if
an optimal policy $$\pi^*$$ can achieve success, i.e an episode return of 1,
after taking $$a_t$$ in $$s_t$$. Equivalently, there exists a sequence of future
actions that reaches a success state. $$(s_t, a_t)$$ is **catastrophic** if
no such sequence exists.

This definition of effective and catastrophic does not depend on the policy we
are evaluating, and we can relate the return of a policy $$R(\pi)$$ to how
often it selects effective or catastrophic actions. That probability,
in turn, depends on how accurately $$Q(s,a)$$ judges the value of each
$$(s,a)$$. Intuitively,
a policy $$\pi$$ will select actions with high Q-value
$$Q(s,a)$$ more often than actions with low Q-value $$Q(s,a)$$. Thus, if we
treat $$Q(s,a)$$ as a scoring function, we can then define a classifier
between effective $$(s,a)$$ and catastrophic $$(s,a)$$ based on the Q-value,
and relate classification error to total reward. We call this
*off-policy classification for OPE*, or OPC for
short. The OPC is defined with a hard 0-1 loss, and we also derived a
variant with a soft loss, which we call SoftOPC.

To train this classifier, we need labels. Whenever the behavior policy
$$\pi_b$$ succeeds at the task, we know all $$(s,a)$$ along that episode
are effective. But, when $$\pi_b$$ fails at the task, we don't know which
$$(s,a)$$ were catastrophic, and which were recoverable. This means we have
unlabeled data and positive-labeled data.

This positive-unlabled learning setting has been studied before in binary
classification, and you can show that if you know some properties of the
distribution of positive labels, you can learn to classify from just
positively-labeled data and unlabeled data.

See the paper for how all these ideas get combined together. At the end of
it, we get an expression for classification error, and how it relates to the
return of the policy. Due to only having data from some unknown behavior policy
$$\pi_b$$, we aren't able to evaluate this classification error exactly, but
remember that we don't need to exactly compute total reward. It's enough to
compute something that correlates with total reward. So similarly, as long
as the OPC we compute correlates with true classification error, we're
in good shape.


Results
----------------------------------------------------------------------

We tried the OPC and SoftOPC across a few different environments: a
toy binary tree environment, Pong from the Atari suite, a simulated
robot grasping task, and the real-world robot grasping task. Across
all environments, the OPC and SoftOPC outperformed many of our baselines.


(Add Figure 2 from the paper. Not planning to include the table, feels
too noisy for a blog post)

To test we could predict the performance of sim-to-real learning models,
we took 15 models, trained to have varying degrees of robustness to the
gap between simulation and reality. Of these models, 7 of them were
trained purely in simulation. Again, we found correlation was better
with the OPC and SoftOPC metrics.


Future Work
----------------------------------------------------------------------

There's a lot of directions for future work. The most promising one is
to extend the derivaiton to support tasks that don't have binary rewards,
or tasks where the dynamics are stochastic. However, even in just the
binary reward setting, we think the results are promising enough to be
trialed as part of a more practical pipeline for developing real-world
RL.

(Acknowledgements, add another paper link here)

