---
layout: post
title:  "Trip Report ICLR 2019"
date:   2019-05-12 21:05:00 -0800
---

ICLR has done terrible things for my ego. I did not have any papers at ICLR,
I was there just to check out the conference, and then people I haven't met
before are telling me that they know who I am, and friends are joking that I should
change my affliation from "Google" to "Famous Blogger".

Look, take it from me - having a famous blog post (and a weird last name) is
great for name recognition, but it isn't good for much else.
As far as I know, it hasn't gotten people to read any more of my research. I know
for sure that I've said this before, but the reason I blog is because I get
something out of doing it, even if no one actually reads the post. It's why I
don't care about viewership. In fact I actively try *not* to care about viewership.
Of course I want people to read my writing,
but this blog is never going to pay my bills, and my worry is that if I care about
viewership too much, I won't write the posts that I want to write. Instead I'll
write the posts that I think other people want me to write. Those aren't the
same and that seems like the first step towards a blog that's less fun.

In that vein, here are some scattered thoughts on ICLR 2019 for me.


Favorite Papers
----------------------------------------------------------------------------

Every conference, when we're doing small-talk, I get the same question.

> what papers did you like from today?

And every conference I give an answer like this: I don't remember any of them.
I try to use conferences to get a feel for what's hot and what's not in machine
learning, and to catch-up on the subfields that I'm following. So generally,
I have a hard time remembering any paper in particular from the poster sessions.
Instead my eyes glaze over at a few keywords, I type some notes that I know I'll
never read, and maybe, maybe, I'll remember a specific paper that I thought
was neat. In practice I usually don't. I see no problem with this.

People still like deep learning. They still like reinforcement learning.
Generative models - so hot right now. Meta-learning is also pretty healthy,
although I feel it's changing from deriving new meta-learning algorithms and
shifting towards figuring out what problems can be formulated as useful, solvable
meta-learning problems.

Papers about adversarial attacks are still going, although maybe this is colored
by the poster session where it felt like every poster was about GANs or
adversarial attacks. Speaking of which...


Poster Arrangements
---------------------------------------------------------------------------

At this year's ICLR, posters were grouped by topic. One session was a bunch
of RL posters, another was a bunch of sequence model posters, a third had
a bunch of quantized / low-precision network posters, and so on.

This didn't work that well for me. I found there were some sessions where I
wasn't interested in any of the posters, and then there were other sessions
where I wanted to see *all* of them. So I'd go between having nothing to do
for a few hours, and having to skip a bunch of posters because I knew I wouldn't
have the time to read every one of them. This felt horribly inefficient.

I can see this scheme working if you do what you're supposed to at conferences:
read the schedule, identify posters you want to see ahead of time, figure out
which sessions they're in, then schedule everything else around that. That way,
if most posters in a session don't look interesting, you can schedule all your
networking meetings during those poster sessions. I don't do this because I'm
lazy.

A friend of mine pointed out another flaw: the posters they most wanted to see
were in the same time slot as their poster session. That forces a trade-off
between presenting your own work and seeing other related work in your field.

My feeling is that ICLR should cater to the people presenting the posters, and
general attendee experience should be secondary. So let's quickly solve the
optimization problem. Say a subfield has $$N$$ posters, and there are $$k$$ different poster sessions.
As an approximation, every poster is presented by $$1$$ person, and that person
can't see any posters in the same session they're presenting in. We want to
allocate the posters such that the average number of posters each presenter
gets to see is maximized.

I'll leave the formal proof as an exercise (you'll want your Lagrange
multipliers), but the solution you get is that
the $$N$$ posters should be divided evenly between the $$k$$ poster sessions.
Now, in practice posters can overlap between subfields, and distributing them
evenly may be a challenge. But if we assign posters randomly to each poster
session, it should work out to be about a uniform distribution.

This is a long way of saying that from my perspective, the organizers did a
bunch of work grouping posters by topic, and it would have been more efficient
to do no work at all. Open to reasons grouping-by-topic is better.
(CGP Grey link).


Generative Models, Everywhere
-------------------------------------------------------------------------

I'm getting the impressino that generative models are continuing to be a hot
area. The fad of modifying the GAN optimization objective has mostly died by
now, in favor of tricks around regularization and initialization, but that doesn't
mean GAN papers have stopped. Meanwhile, VAE models and flow-based models are
still going strong.

It feels like we're getting to the point of high-enough quality images to talk
some interesting problems that generative models could potentially be used for.
And of course, generating cool samples is one of the ways your paper becomes more
engaging. It's sad that I'm only half-joking. The nature of research is that
some looks cool and some doesn't, and the coolness factor is often closely
tied to your field rather than the quality of your work.


I'm So Over Adversarial Perturbations
--------------------------------------------------------------------------

Speaking of cool factor, wow there were a lot of papers about adversarial
pertubations. Yes, yes, it's cool that you can add imperceptible noise to an
image to change its class - if you define "imperceptible noise" in human terms
rather than machine terms. (Adversarial are features) is one paper that argued
these features genuinely are useful for predicting ImageNet classes.

I used to be really interested in this line of work, but now I'm not. Most of
the change happened from talking to authors on the Adversarial Spheres paper
([Gilmer et al, 2018](https://arxiv.org/abs/1801.02774)). I haven't read the
paper in full, but my understanding is that they prove a bound on average
distance to the closest misclassified point, based solely on classification
error rate, and show that for existing datasets, these bounds are within
the range of imperceptible noise. In other words, the reason adversarial
examples are so plentiful has more to do with the geometry of high-dimensional
spaces than anything else.

This means that it shouldn't be too big a surprise that many adversarial
defenses break as soon as a new attacker tries to break the defense. Most
image classifiers have some amount of classification error that implies the
existence of adversarial examples - the fact that your algorithm discovers them
is, in some sense, not too big a deal.

There's value in learning robustness against adversarial perturbations, but I
it shouldn't be seen as learning techniques that are useful for protecting
against adversaries, or even as techniques that are good first steps for
future research into protecting against adversaries. It should be seen more
as adversarial data augmentation - deliberately probing for current weaknesses
in the model to feed as new training input.

I mean, in practice, it's possible that we can come up with a defense against
adversarial perturbations that both bring average-distance-to-closest-error
close to the theoretical limit, and it's empirically difficult to discover that
closest error. But based on the empirical track record for breaking existing
defenses, this doesn't seem very likely.


Dynamics Defenses, Zero-Knowledge Proof Connection
-----------------------------------------------------------------

I don't want this post to turn into an "Alex parrots Ian Goodfellow's talk"
post, but I agree with a lot of his points and just want to quickly mention
this one.

In his talk at the Reliability of ML systems workshop (LINK), he proposed that
we should consider a new paradigm called dynamic defenses. Essentially,
a classifier learns a distributions $$p(class|input)$$. In a dynamic defense,
this distribution $$p(class|input)$$ can change on every input, even if the same
input is given twice. This breaks a ton of assumptions of existing analysis of
models, but lets you express more flexible defenses.

This may be a completely wild connection, but on hearing this I was reminded
of zero-knowledge proofs. For a simple introduction, I recommend "Zero Knowledge
Proofs for Fourth Graders".

Suppose there is a cave with two entrances that meet in a door in the middle, like
so.

PICTURE

I claim that I know a password that opens the door in the middle, but I don't
want to tell you what the password is. HOw can I prove that I know the password?
The zero-knowledge proof goes like so: first, I walk into the cave. Once I'm
inside the cave, you message me "left" or "right". I walk out of that side of
the cave.

Suppose I didn't know the password. Then, the probability I can walk out the
side you message me is $$1/2$$, because I can only do so if I guess correctly.
If I do know the password, the probability is $$1$$, because if I'm on the wrong
side, I can use the password to cross the door and leave the side I want.

PICTURE

Let's call this a single round. We repeat this process for $$N$$ rounds, until
you're satisfied. After $$N$$ rounds, if I don't know the password, the probability
I fool you is $$1/2^N$$ - essetially zero if we pick $$N$$ to be say, $$N = 40$$.

The connection I see is that by integrating randomness into what you ask of me,
it introduces randomness into whether I use or don't use the password, and I
can leverage this randomness to prove something to you while protecting details
of my internal processing. And with some twisting of logic it sounds like maybe
there's some way to make a classifier useful without sharing any
unnecessary knowledge. The obvious problem I see is that you don't want to prove
your classifier is $$90\%$$ accurate, you want to actually classify things, and
that's likely a very different problem.

Again, probably all junk, but let me know if you get something that's less junk.




The Structure and Priors Debate
------------------------------------------------------------------------

There was a debate at ICLR this year, whose topic was about what should be
given as structure or prior, and what should be learned. I got the impression
that the organizers wanted it to be fiery and passionate. It wasn't.

I'm in a slightly unique position to comment on this, because I was actually
part of the ICML 2018 Debates workshop. I'd rather people not know I was part of
that workshop, because I was really, really winging it, armed with a position
paper I wrote in a day that I no longer fully agree with. Meanwhile, the other
side of the debate was represented by [Katherine](https://www.dropbox.com/s/b356qc8zw2d0z7s/Lee%20-%20Submit%20to%20Journals.pdf?dl=0)
and [Zack](https://www.dropbox.com/s/ao7c090p8bg1hk3/Lipton%20and%20Steinhardt%20-%20Troubling%20Trends%20in%20Machine%20Learning%20Scholarship.pdf?dl=0),
who had considerably better position papers. It was like I was walking into what
I thought was a knife fight with a paring knife, and then I learn that it's
an "anything goes" fight and they're in a fortified bunker 300 meters away with
defensive turrets.

But then the debate started and it all turned out fine - because we agreed on
about 90% of every question. None of us had any reason to pull out
particularly heavy linguistic weaponry. It stayed very civil, and the most
fiery comments came from the audience, not from us. When talking to the ICML
debate organizers afterwards, they said that in retrospect, the mistake was
assuming that if they took people with opposing views, and had them just talk,
then it would naturally turn into a debate. But it doesn't. It only plays out
that way if you continually force the participants to debate the crux of their
disagreements - and this crux is sometimes not very obvious. Without this constant
focus, it's easy to endlessly orbit it instead.

http://phdcomics.com/comics.php?f=1354

Look, let's not mince words. Machine learning researchers tend to be more
introverted, tend to agree more than they disagree, and are usually quite tolerant
of differing opinions over the best way to learn useful agents. We're more likely
to get into fights with each other on Twitter or Reddit rather than real life,
because it's just so much easier to be provocative on those platforms. I think
ML debates are cool in theory, and I'd like to see a few more shots at making
them happen, but if it does happen again, I'd advise the debate moderators to
focus on identifying and repeatedly hitting the crux of the debaters' disagreement.
ML debates need more design and less improv.

