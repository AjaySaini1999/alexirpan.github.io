---
layout: post
title:  "Deep Reinforcement Learning Doesn't Work Yet"
date:   2017-12-11 23:06:00 -0700
---

*This is a rant. Like all rants, this accentuates the negatives and downplays the positives.*

*This mostly cites papers from Berkeley, Google Brain, DeepMind, and OpenAI.
I'm almost certainly missing stuff from less famous instituions. If I missed a
relevant paper, I apologize - I'm just one guy, after all. Feel free to comment.*

I once mused that hypothetically, if people asked me if RL can solve their
problem, I would just tell them it can't, because that would be correct about
70% of the time. I haven't gotten to test this empirically, but I stand by this.

Deep reinforcement learning has mountains and mountains of hype behind it, and
it deserves it. It's an incredibly, incredibly flexible paradigm, and in
principle, a robust and performant RL system should be great at everything - it
is almost certainly going to be part of a general AI. That's the kind of dream
that fuels millions of dollars of funding.

Unfortunately, it doesn't really work.

Now, I believe it can work, otherwise I wouldn't be interested in it.
But there are a lot of problems in the way, many of which feel fundamentally
difficult. Hidden behind the beautiful demos is a towering garbage pile of
pain, failed experiments, statistically insignificant results, and pure, nonsensical
bullshit.

My AGI timelines are long because everyone is looking towards RL
as the key, and to me it looks like a really, really shitty key.

So, to all the people who ask me why RL isn't used on more real-world problems,
or who believe that deep RL actually does reasonable things, or who have the
naive, wide-eyed belief that their new research idea will only take a few weeks to
test out: this post is for you. I take no joy in this. In fact, it pains me that
I have to burst these lofty aspirations and bring them down to Earth. I only
do this because people aren't talking about the dark sides of RL enough.


Reasons Deep Reinforcement Learning Sucks Right Now
------------------------------------------------------------------------------

List them


It Can Be Horribly Sample Inefficient
-------------------------------------------------------------------------------

The most well-known benchmark for deep reinforcement learning is Atari. Throw
DQN at it, and some (but not all!) of the games get solved to superhuman
performance.

Atari games run at 60 frames per second. On the top of your head, do you
know how many frames a state-of-the-art DQN needs to reach human performance?

The answer depends on the game, but let's take a look at a recent Deepmind
paper, [Rainbow DQN](https://arxiv.org/abs/1710.02298). This paper came out
about 2 months ago, and it's a combination
of the several incremental advances made in the past 5 years. Evidently,
somebody finally got curious to see what would happen if you combined all the
advances together. The results come in this handy chart.

![Figure from Rainbow DQN](/public/rl-hard/rainbow_dqn.png)
{: .centered }

The y-axis is "median human-normalized score". This is computed by training
57 DQNs, one for each Atari game, normalizing the score of each agent such that
human performance is 100%, then plotting the median performance across the
57 games. RainbowDQN matches original DQN performance in about 7 million
frames, and passes the 100% threshold at around 18 million frames. This
corresponds to about 83 hours of play experience, which doesn't include the
time needed to actually compute the outputs and gradients for the Q-network.

Mind you, this is actually a pretty great, because the old record (Distributional
DQN) didn't hit 100% median performance until abouut 70 million frames. The
original DQN (the one from the Nature paper) never even hits 100% normalized
performance. After 200 million frames it reaches 79% median performance.

Many common benchmarks require hundreds of thousands or millions of steps. And,
I mean, that's fine, but it feels like many of the benchmarks shouldn't require
this many steps. Here's a plot from another state-of-the-art algorithm,
[PPO](https://blog.openai.com/openai-baselines-ppo/). This was from about 5 months
ago, and was the algorithm behind the famous [parkour paper](https://arxiv.org/abs/1707.02286).

<iframe width="560" height="315" src="https://www.youtube.com/embed/hx_bgoTF7bs" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

These results are really impressive, but based on the paper it took about 100
hours of training with 64 workers to learn the policy for the Humanoid. That's
a lot of computation, considering that online trajectory optimization has done
similarly cool things since 2012.

<iframe width="560" height="315" src="https://www.youtube.com/embed/uRVAX_sFT24" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

This isn't a fair comparison, since model predictive control is doing planning
against a known dynamics model. In a similar vein, you can easily outperform
[DQN in Atari with Monte Carlo Tree Search instead](https://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning).
The point is more to show that RL uses orders of magnitude more computation
to arrive at the same result as other more domain-specific algorithms. This is
a common pattern - generality comes at the price of sample efficiency.


It (Usually) Requires a Reward Oracle
------------------------------------------------------------------------------

RL algorithms assume the existence of a reward function. Usually, this is either
given, or it is learned offline and fixed over the course of learning. Importantly,
for RL to do the right thing, your reward function must capture *exactly* what
you want.
And I mean *exactly*. RL has an annoying tendency to overfit to your reward and
do things you don't expect.

Take Atari, for example. You want to maximize score. You
*always* want to maximize score. It's just how those games work. That's why
Atari games are such a nice benchmark.

The majority of reinforcement learning results come from simulated environments,
and that's because you have perfect knowledge of the state and can define an
appropriate reward. Consider the MuJoCo tasks in OpenAI gym.
In the Reacher task, you control a two-segment arm, that's connected to a central
point, and the goal is to move the end of the arm to a target location.

GIF

You have the exact locations of everything, so reward is simply the distance to
the goal. Easy.

In the HalfCheetah environment, you have a two-legged robot, restricted to a
vertical plane, meaning it can only run forward or backward.

GIF

The goal is to learn
a running gait, so you give reward depending on its velocity. Also straightforward.

**Reward functions that don't have this property tend to work poorly.** There's
a semi-famous boat racing example from an [OpenAI blog post](https://blog.openai.com/faulty-reward-functions/),
where finishing the race gives less points than collecting powerups.

<iframe width="560" height="315" src="https://www.youtube.com/embed/tlOIHko8ySg" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

To be honest, at the time I really didn't understand why this merited a blog
post. It was just so *obvious*. Of course optimizing the wrong objective leads
to weird behavior! What, do people not see this happening in schools?
(Teaching to the test instead of teaching to learn.) In academia? (Publishing
more papers instead of publishing good papers because of publish or perish.)
How is it news that RL does the same thing? Do enough RL and you inevitably
run into an issue like this.

(Then I started writing this blog post, and found that the most compelling
video I could find was the boat racing video from the blog post. So, I now
begrudgingly accept that the post had a point.)

For example, [some researchers at Salesforce](https://www.salesforce.com/products/einstein/ai-research/tl-dr-reinforced-model-abstractive-summarization/)
trained a text summarization model.
This is usually trained with supervised learning. For evaluation, they use an
automated metric called ROUGE, as well as human ratings. ROUGE isn't differentiable,
so they tried using RL to optimize ROUGE. And yes, you can get much better
ROUGE scores if you train with RL, except there's one problem.

> While ROUGE scores have a good correlation with human judgment in general, the summaries with the highest ROUGE aren't necessarily the most readable or natural ones. This became an issue when we trained our model to maximize the ROUGE score with reinforcement learning alone. We observed that our models with the highest ROUGE scores also generated barely-readable summaries.

Again, if your reward function doesn't reflect something you care about,
there's no guarantee the learned model will care about it either.
Reinforcement learning is just a variant of black-box optimization, after all -
it's well-known that black-box optimization will learn whatever junk it
gets rewarded for.

Here's another example. In [this paper from DeepMind](https://arxiv.org/abs/1704.03073),
the authors use DDPG to learn a grasping policy.
The policy gets reward based on how high the block is, which is defined as
the z-coordinate of the bottom face.

The policy ended up learning this.

<iframe width="560" height="315" src="https://www.youtube.com/embed/8QnD8ZM0YCo?start=27" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

Now, clearly this isn't the intended solution...but RL doesn't care.
From the perspective of the learning algorithm, it did something, and got
positive reward, so it's going to keep doing that thing.

I don't have any more videos, or paper references, but I do have more anecdotal
stories.

* A coworker is applying RL to a toy navigation problem.
The episode terminates whenever the agent walks out of bounds.
Fairly standard stuff. Except, he forgot to add a large negative penalty if
it walks out of bounds.
And what ended up happening was that the agent learned to be suicidal if
it was ever close to getting negative reward. Reaching the positive reward
was too difficult, and a quick death ending in 0 reward is preferable to a
life of -1 reward.
* A friend is applying RL to another racing game in Universe.
The reward is defined by applying OCR to a segment of the game screen to
extract the score. In initial exploration, due to dumb luck, the OCR made a
rare mistake. It saw the starting line of the race, and interpreted it as
a leading 1, increasing the score by an order of magnitude. Upon seeing such a
large reward, the policy spent the rest of learning trying (and failing) to
reproduce that scenario.
* The same friend is training a simulated robot to reach towards a point
above a table. It turns out the point was defined *with respect to the table*.
The policy learned to pound the table really hard, making it fall down *just so*
to make the goal point move to the end of the arm.
* A researcher is training a simulated hand to pick up a hammer and push in a nail.
The reward is defined by how far the nail is pushed into the hole. Instead of
picking up the hammer, the policy learned to punch the nail into the hole. So
they added a reward term to encourage picking up the hammer. When trained again,
the policy learned to pick up the hammer...then throw it at the nail, instead
of hammering it in.

I haven't verified any of the above for myself, but none of it sounds
implausible to me. I've been burned by reinforcement learning too many times to
believe otherwise.

I know people like to tell stories about paperclip
optimizers, but there's no reason to speculate that far when
present-day examples show up all the time.


Even Given a Reward Oracle, Local Optima Can Be Hard To Escape
------------------------------------------------------------------------------

All the previous examples are commonly called "reward hacking", which to me
implies a clever, out-of-the-box solution that gives more reward than the
intended answer.

These are the exceptions. The much more common case is stupidity - poor
local optima that come from getting the exploration-exploitation tradeoff
wrong. Ones that lead to frustration and so, so much pain,

And oh do I have stories for this.

Here's one of my favorite videos. This is an implementation of NAF, learning
on the HalfCheetah environment.

VIDEO

Now, from an outside perspective, this is really, *really* dumb. But we can
only say it's dumb because we have a bunch of prebuilt knowledge that tells
us running on your feet is better. RL doesn't know this! It sends actions,
and gets back states and rewards. That's it. Here's my best guess for what
happened.

* Through random exploration, it learned to fall forward. This gives a bit of
reward - moving forward is better than not moving at all.
* It keeps doing that because it got rewarded for it, and the behavior got
"burned into" the policy. Now it's falling forward consistently.
* Through random exploration, from that state it learns a backflip maneuver.
This gives more reward - backflipping forward is better than lying down.
* It keeps doing that because it got rewarded for it, and backflipping got
burned in too. Now it's backflipping consistently.
* Once the policy flips to its back consistently, it's too hard to learn
how to flip right side up. So, the easiest path to high reward
is to figure out how to move forward while while lying on your back.

And thus, this.

SAME VIDEO

Realistically though, I have no idea if my theory above is right. All I know
is that it didn't learn.

Here's another failed run, this time on the Reacher environment.

VIDEO

In this run, the initial random weights tended to output highly positive or
highly negative action outputs. That made it easy to learn how to spin really
fast. It's easy to perpetuate this behavior - just make all your
neural network weights large in magnitude. It's hard to deviate from this policy
in a meaningful way - you have to take several exploration steps to stop spinning,
and until then your actions barely influence your state.
Meanwhile, this "spin really fast" policy is actually not that awful -
in every revolution, the endpoint is guaranteed to pass by the target location.

Combine all of this together, and it never learns to stop spinning really fast.

(May kill the Breakout example, not necessary)

This shows up in Atari games too. In Breakout, the first thing any policy learns
is to always output right. If you do so, you're guaranteed to reflect the
ball at least once and clear one brick. And hey - 5 points is better than 0
points.

VIDEO

Breakout is simple enough that most well-tuned learning algorithms will reliably
learn to superhuman performance. But for a poorly tuned algorithm, it's possible
that the policy will never do enough exploration to learn that it should be
moving to track the ball.

-------------------------

These are all examples of the classic exploration-exploitation problem that has dogged
reinforcement learning since time immemeorial.
Your data comes from your current policy. If your current policy explores too
much you get junk data and learn nothing. Exploit too much and you burn-in
behaviors that aren't optimal.

There are several intuitively pleasing ideas for addressing this, and many of
them have been attempted recently - intrinsic motivation, curiosity-driven
exploration, count-based exploration, and so forth. However, none of these have
worked consistently for all environments,
and I'm skeptical a similar solution will be figured out anytime soon. The
problem is really, really, really, really, really hard. To quote the
[Wikipedia article on multi-armed bandits](https://en.wikipedia.org/wiki/Multi-armed_bandit),

> Originally considered by Allied scientists in World War II, it proved so intractable that, according to Peter Whittle, the problem was proposed to be dropped over Germany so that German scientists could also waste their time on it.

I find it's almost productive to imagine your computer is a demon that's
deliberately misinterpreting your reward and actively searching for local optima.
Whenever your reward function gives small reward for stupid behavior, there's always a
chance learning just gets completely stuck at the wrong thing. It's just
how deep RL is right now.


Final Results are Incredibly Unstable and Hard to Reproduce
------------------------------------------------------------------------

Almost every ML algorithm has hyperparameters, which influence the behavior
of the learning system. Often, these are picked by hand, or by random search.

Supervised learning is stable. Fixed dataset, ground truth targets. If you
change the hyperparamters a little bit,
your performance won't change that much. Not all hyperparameters perform
well, but it's relatively easy to see signs of life. These signs of life are
super important, because they tell you that you're on the right track, you're
doing something reasonable, and it's worth investing more time.

Reinforcement learning, on the other hand, isn't stable at all. It is 
pathologically unstable.

When I started working at Brain, one of the first
things I worked on was reproducing the results of a continuous RL paper.
Specifically, I was trying to reproduce the Normalized Advantage Function
paper.

After about a month of failures, I finally solve my first task: the Pendulum
task from OpenAI Gym. If you're unfamiliar with this task, it's very simple.
There's a pendulum. There's gravity. The pendulum hangs from a point.
Each timstep, you can apply some acceleration around that point. You get
more reward the closer the pendulum is to the top, with maximum reward if
you can balance the pendulum.

GIF

This is the easiest continuous control task in OpenAI Gym - the action space
is 1-dimensional, the state space is 3-dimensional, and you have a shaped reward
that's giving you fine-grained feedback on how close you are to the top.

Why did this take me a month? Well, I was partly learning how TensorFlow worked.
I also had a bunch of bugs that took a while to pin down.
Now, when I finally get it to work, here are the plots of performance. Success
on this task is defined as hitting -100 to -150 reward. This is 10 independent
runs, with the exact same hyperparameters. The only difference is the random seed.

PLOT

10 seeds, and *3* of them work within 200k steps? If this happened in supervised
learning you'd be digging for bugs in data loading or training to figure out
why it's so unstable. In reinforcement learning? It's Tuesday.

<iframe width="560" height="315" src="https://www.youtube.com/embed/iVzAMmpMra8" frameborder="0" allowfullscreen></iframe>

To be fair, if you leave these running for longer, many of them do
eventually get hit the threshold for success, just in 800k steps instead of
200k steps. But this is still really frustrating. Taking 4 times longer to learn
by random chance is terrible!

Just to show this isn't a personal complaint: here is a plot from the VIME paper, trained with TRPO (which is billed as one of the more stable
RL algorithms.) The environment is HalfCheetah, modified to have sparse reward.

![Plot from VIME paper](/public/rl-hard/vime.png)

The dark line is the median performance over 10 random seeds, and the shaded
region is the 25th to 75th percentile. It's somewhat distributing that the 25%
line is so close to zero.

> If it makes you feel any better, I've been doing this for a while and it took me last ~6 weeks to get a from-scratch policy gradients implementation to work 50% of the time on a bunch of RL problems. And I also have a GPU cluster available to me, and a number of friends I get lunch with every day who've been in the area for the last few years.
>
> Also, what we know about good CNN design from supervised learning land doesn't seem to apply to reinforcement learning land, because you're mostly bottlenecked by credit assignment / supervision bitrate, not by a lack of a powerful representation. Your ResNets, batchnorms, or very deep networks have no power here.
>
> SL wants to work. Even if you screw something up you'll usually get something non-random back. RL must be forced to work. If you screw something up or don't tune something well enough you're exceedingly likely to get a policy that is even worse than random. And even if it's all well tuned you'll get a bad policy 30% of the time, just because.
>
> Long story short your failure is more due to the difficulty of deep RL, and much less due to the difficulty of "designing neural networks".

[Hacker News comment from Andrej Karpathy, back when he was at OpenAI](https://news.ycombinator.com/item?id=13519044)
{: .centered }

Instability to random seed is like a canary in a coal mine. If pure randomness
is enough to lead to these kinds of results, imagine how much an actual
difference in the code could make.

Oh that's right. You don't have to imagine.
The recent [Deep Reinforcement Learning That Matters](https://arxiv.org/abs/1709.06560)
paper has done this already. It makes a bunch of points that I think were well-known in the RL
community, but which wasn't packaged together until that paper. In particular,

* Changing the non-linearity can give significant differences in performance.
* Multiplying the rewards by a constant can give significant differences in performance.
* Different implementations of the same algorithm have different performance.

My theory is that RL is very sensitive to both your initialization and to the
dynamics of your training process, because your data is always collected online
and the only supervision you get is a single scalar for reward. A policy that
randomly stumbles onto good training examples will bootstrap itself much
faster than a policy that doesn't. A policy that fails to discover good training
examples in time will collapse towards learning nothing at all, as it becomes
more confident that any deviation it tries will fail.


Even When You Get Good Results, It's Often Just Incredible Overfitting to Weird Patterns In the Environment
------------------------------------------------------------------------------

In RL, your training set is your test set. The upside is that if you want to do
well in an environment, you're free to overfit like crazy. The downside is that
if you want to do well in any other environment, you're going to overfit like
crazy.

Consider DQN. It works well on Atari games, but importantly, the
benchmark for each Atari game has to be trained independently. There's no expectation
that an agent trained on one Atari game will be a good initialization for training
on another Atari game. In computer vision, Imagenet features transfer to lots of
image tasks. There doesn't seem to be an equivalent for reinforcement learning
yet.

This also shows up in multiagent environments. In this work (LINK), we applied
RL algorithms to a toy 2-player game, where there's a known analytic solution
for optimal behavior. We fixed player 1 to be optimal, then trained player 2
with reinforcement learning. In this view, player 1's actions are simply part
of the environment. Once the 2nd player is trained, we can test generalization
by changing player 1's behavior. And interestingly, even though player 2 can
perform well against an optimal player 1, it actually does *worse* if player 1
is changed to be suboptimal! The model hasn't generalized to beating any possible
player, it only knows what to do against the optimal one.

A DeepMind paper from NIPS showed something similar. Here, there are two agents
playing laser tag. The agents are trained with multiagent reinforcement learning,
and they learn to shoot each other very quickly.

VIDEO

They ran the same setup for
several experiments. Then, they took agent 1 from experiment 1, and played it against
agent 2 from experiment 2. They got behavior like this.

VIDEO

A kind of co-evolution happens. The agents get really good at beating each other,
but as soon as you introduce a different player, performance goes to shit. Note
that the only difference here is the random seed! It's the exact same learning
algorithm with the exact same hyperparameters.



But What About All The Great Things Deep RL Has Done For Us?
-------------------------------------------------------------------

Really? Has it really done great things? It's certainly done some very cool
things, but as far as I know there's very little evidence of it doing any
practical things. I tried to think of a real-world, productionized use of
deep RL, and the only thing I could think of was
the DeepMind data center power usage project.
There's rumors that some finance companies use reinforcement learning, but nothing definitive.
Of course, finance companies have good reasons to be cagey about how they
play the market, so maybe the evidence is never going to be strong. Jack
Clark (Strategy & Comms Director at OpenAI) [made a tweet asking about this](https://twitter.com/jackclarkSF/status/919584404472602624)
back in October 2017, and there just really isn't that much in the replies.

For all of it's promise, deep reinforcement learning is still decidedly a research
topic.


Okay, But What About All the Cool Things Deep RL Has Done?
-----------------------------------------------------------------------

I agree! Deep reinforcement learning has done some *awesome* stuff. 
Off the top of my head, here are the famous success stories.

* DeepMind created DQN, which learned how to play some Atari games to
superhuman performance, without hand-designed features, putting deep RL on the
map.
* DeepMind created AlphaGo, which beat a professional Go player, then AlphaGo Zero did even better. The
same learning algorithm was shown to have little issue with Shogi and Chess as
well.
* OpenAI made a Dota 2 1v1 Shadow Fiend bot, that beat top pro players in a
simplified duel setting.
* Google Brain showed that an agent trained with reinforcement learning could
learn novel neural net architectures that outperformed human-designed models.
They called this neural architecture search.
* Some students at MIT and NYU showed that deep RL could learn to beat
pro SSBM players at 1v1 Falcon dittos.

(Note: two machine learning systems, [Libratus](https://www.ijcai.org/proceedings/2017/0772.pdf) and [DeepStack](https://arxiv.org/abs/1701.01724),
recently beat pro players at no-limit heads
up Texas Hold'Em. These aren't listed, because from what I heard, neither
uses reinforcement learning. They both used counterfactual regret minimization
and clever solving of subgames.)

Taking each success above in turn, here would be my words of caution.

* *Atari*: As mentioned earlier, this required millions of frames of experience,
has score as a ground-truth reward function.
* *Go / Chess / Shogi*: All used millions of games, and all can employ
self-play, which has been shown to be a very useful property for learning.
* *Dota 2*: The Shadow Fiend bot was restricted to a small part of the Dota game
space. No bottle, few items, 1v1 laning only, and only Shadow Fiend.
The bot is likely using the [Dota 2 API](https://developer.valvesoftware.com/wiki/Dota_2_Workshop_Tools/Scripting/API)
to get the exact health of all enemies in sight. Gold and enemy health form
a rich, well shaped reward signal.
* *Power center PUE*: This built off of prior work from the Google data
center team, which showed neural nets could predict PUE with high accuracy.
Given a neural net with good predictions, it seems obvious that optimizing PUE
over the networks inputs is going to lead to good things.
* *Neural architecture search*: Validation accuracy of a network trained to
convergence is also a very rich reward signal - even if an action only increases
accuracy from 70% to 71%, RL will pick up on this. Empirically, hyperparameters
in deep learning tend to be close to linearly independent, and although NAS
isn't exactly tuning hyperparameters, it seems reasonable that there are clear
links between action choices and reward, which makes credit assignment
straightforward. Given all of this, working in "only" 2000 trained networks
doesn't seem so far-fetched.
* The SSBM bot shares many similarities to the Dota 2 setup - 1v1, Battlefield
only, infinite time match to avoid having to understand how stocks work,
Captain Falcon dittos only, gets to use self-play, and a rich reward based on
% dealt and received.

The broader point I want to get across is this: many things have to go right
for reinforcement learning to even be a plausible solution, and then it often
still requires a lot of careful work to make that solution happen. Perhaps
in the future, RL will be great at everything, but for now, there's a narrow
set of problems where RL is the right solution.

The same is true of reinforcement learning. It's not that reinforcement learning
is particularly good. It's more that the environments above are well-suited to
the strengths of reinforcement learning, and they tend to dodge its weaknesses.


Looking to The Future?
-------------------------------------------------------------------------------

There's an old saying - every researcher learns how to hate their area of
study. The trick is that researchers will press on despite this, because they
like the problems too much to quit them.

That's about where I'm at with deep reinforcement learning. Despite my reservations
and pessimism, I'm still interested in working in that area, keeping up with
what's going on, and wanting to see where it's all going. Notice that I said
"Deep Reinforcement Learning Doesn't Work *Yet*". I see no reason why it couldn't
work, given more time. To balance out the negativity, here are some ways I could
see things getting better.

*Local optima are good enough:* It would be very arrogant to claim humans are
globally optimal at anything. We're really just better at some things than
all the other species.
A reinforcement learning solution doesn't have to reach the global
optima either. It just has to do better than humans can.

*Hardware solves everything:* Personally, I'm a bit skeptical that hardware
will simply solve everything. Nevertheless, the faster you can run things, the
less you care about sample inefficiency. You could potentially brute-force
the exploration problem entirely. Hardware also lets you try existing
research ideas faster, and opens up new research avenues of its own.

*Add more learning signal:* Sparse rewards are hard to learn because you get
very little information about what thing help you. It's possible we can either hallucinate
positive rewards (HER), define auxiliary tasks (UNREAL), or do predicitive
learning to build a world model from transitions that give 0 reward.

*Learn learnable reward functions:* The human preferences paper suggests that
it's possible for neural nets to use human feedback to implicitly define a
reward signal that encourages partial progress for a task. In some cases, this
outperformed the existing reward definitions. It's possible we
could extend this with other parts of inverse RL / imitation learning to get
past many of the difficulties that come from explicitly defining reward.

*Metalearning saves the day:* RL algorithms are designed to apply to any environment that's an MDP. However,
we don't need to solve arbitrary environments, we just need to solve
environments in the real-world. In principle, we could use metalearning to learn
a real-world prior that lets us quickly learn new real-world tasks, at the cost
of general learning ability. (This is a point Pieter Abbeel likes to mention
in his talks.) For example, if you wanted to use RL to do navigating in
warehouses, there's probably a way to use metalearning to learn a good
navigation prior,

*Wild overfitting is actually okay:*

Note that this goes both ways: the DeepMind parkour paper learned several novel
behaviors by creating environments with difficult-to-traverse terrain, just by
using a reward of "go forward without falling".

work, given more time.
