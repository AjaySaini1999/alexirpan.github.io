---
layout: post
title:  "Deep Reinforcement Learning Doesn't Work Yet"
date:   2017-08-05 01:31:00 -0700
---

*Like all rants, this accentuates the negatives and downplays the positives,
so you should take it with some skepticism.*

*I mostly cite papers from Berkeley and DeepMind, because I know more of the
researchers there. Apologies if I missed a paper you think is relevant.*

I once mused that whenever people ask me if RL can solve their problem, I tell
them it can't, and about 70% of the time I'm right.

Every now and then, I'm at a party and someone wonders why reinforcement learning
hasn't been applied to many real-world problems.

Now, people don't actually directly ask me whether RL will work or not. It's
more about the heavy optimism some people have for reinforcement learning. The
naive, wide-eyed belief that RL works, and it's just a matter of time until
it changes the world, and it pains me that I end up being the one that has to
burst their aspirations and bring them down to Earth.

Reinforcement learning has mountains and mountains of hype behind it, somewhat
deservedly. It's an incredibly, incredibly flexible paradigm, and a robust,
performant reinforcement learning system should theoretically be great at
everything. It's the kind of dream that makes the money keep flowing.

Unfortunately, it doesn't really work. I believe it can work, but there are a
lot of problems in the way, many of which feel very fundamentally difficult.
Hidden behind the beautiful demos is mountains and mountains of pain, failed
experiments, statistically insignificant results, and pure, nonsensical
bullshit.

This is part of why all my AI timelines are fairly long term, by the way.

Okay. Why doesn't RL work?


It Can Be Horribly Sample Inefficient
-------------------------------------------------------------------------------

The most well-known benchmark for deep reinforcement learning is Atari. Throw
Q-Learning at it, and some (but not all!) of the games get solved to superhuman
performance.

Atari games run at 60 frames per second. Take a moment and guess how many frames
a state-of-the-art DQN takes to train.

The answer depends on the game, but let's take a look at a recent Deepmind
paper, [Rainbow DQN](https://arxiv.org/abs/1710.02298). Over the past 5 years,
there have been several extensions made to the original DQN network. Evidently,
some researchers got curious to see what would happen if you combined all the
advances together. The results come in this handy chart.

CHART

The y-axis is "median human-normalized score". There are 57 Atari games in the
benchmark. For each game, the score is normalized such that human performance
is 100%. The plotted curve is the median of those normalized scores.
Rainbow matches that within 7 million frames, which is about 1900 hours of
play.

Mind you, this is actually pretty great, because the old record (Distributional
DQN) didn't hit 100% median performance within 44 million frames. And the
original DQN (the one from the Nature paper) never even hits 100%. It gets to
79% median performance after 200 million frames.

Hundreds of thousands or millions of steps is roughly the scale for a number
of common benchmarks. Here's a plot from the PPO paper, https://arxiv.org/abs/1707.06347,
which is looking like one of the most performant RL algorithms so far.

PLOT

In pretty much all the RL benchmarks, you can solve the problem faster with some
other method. Cartpole is solvable with LQR or CEM. The MuJoCo environments are
solvable on-the-fly in real-time with model predictive control
(albeit with perfect models of environment dynamics). Atari can be solved
faster with search. So on one hand, it's not surprisingly that RL takes longer
to learn. On the other hand, it can be really, really disappointing to see
something requires thousands of times more samples. This is a problem in any
setting where it's hard to get a ridiculously large number of samples.


It (Usually) Requires a Reward Oracle
------------------------------------------------------------------------------

There are some papers on learning a reward function, often based on imitation
learning or expert feedback. However, outside of these exceptions, reinforcement
learning assumes that your reward function captures *exactly* what you want.
And I mean *exactly*. RL has an annoying tendency to overfit to your reward and
doing something you don't want it to.

Take Atari, for example. You want to maximize score. You
*always* want to maximize score. It's just how those games work, and it's why
Atari games are such a nice benchmark.

The majority of reinforcement learning results come from simulated environments,
and that's because you have perfect knowledge of the state and can define an
appropriate reward. Consider the MuJoCo tasks in OpenAI gym.
The Reacher task sants to reach a target location. You have exact locations of
everything, so reward is simply the distance. Easy. In HalfCheetah, you give
more reward if you make it run forward really fast. Also straightforward.

*Reward functions that don't have this property tend to work poorly, and sometimes
do weird things that give reward without solving the problem.* For example,
language models are often trained by maximizing the likelihood of a translation,
then evaluated by computing their BLEU score. Salesforce tried optimizing BLEU
score directly with RL, and found that although they got better BLEU score,
humans rated the translations as lower quality. If your reward function doesn't
reflect something you care about, there is no guarantee your model will care
about it either.
Reinforcement learning is a variant of black-box optimization, and
it's well-known that black-box optimization will learn whatever junk it
gets rewarded for.

A good example comes from this DeepMind paper, where the authors use DDPG to
learn to grasp and lift a block. The policy gets reward for increasing the
block's height, which is defined as the z-coordinate of the bottom face.

The policy ended up learning this.

GIF

This makes the bottom face have a higher z-coordinate. Now, clearly this isn't
what it's supposed to do...but RL doesn't care. From the perspective of the
learning algorithm, it did something, and got positive reward, so it's going to
keep doing that thing.

Here's an anecdote. A coworker talked about trying out RL on a toy environment. The
episode terminated whenever the agent walked out of bounds. Fairly standard
stuff. Except, he forgot to add a large negative penalty at the final timestep.
way. And what ended up happening was that the agent learned to be suicidal if
it was ever close to getting negative reward, because it was too difficult to
reach positive reward, and a quick death that ends in 0 reward is preferable
to a long life of -1 reward.

Here's another story. A friend is trying to learn a racing game in Universe.
The reward is defined by taking a segment of the game screen, and doing OCR on
the pixels to get a score. In initial exploration, it found that at a certain
point of the track, the start line was *really close* to the score, and the OCR
incorrectly interpreted it as a leading 1. This multiplied the reward by 10,
and so the policy spent all its time trying to manipulate the OCR to fail,
instead of on actually solving the game. (I have not verified this story in
any way.)

I know some people like to tell stories about paperclip
optimizers, and those stories have gotten new life with the paperclip game,
but there's no reason to speculate that far when there are present-day examples
all the time.

Note that this goes both ways: the DeepMind parkour paper learned several novel
behaviors by creating environments with difficult-to-traverse terrain, just by
using a reward of "go forward without falling".


Even Given a Reward Oracle, Local Optima Can Be Hard To Escape
------------------------------------------------------------------------------

All the previous examples are commonly called "reward hacking". To me, reward
hacking implies cleverness,
or out-of-the-box approaches that give more reward than the intended solution.

These are the exceptions. The much more common case is stupidity, poor solutions
to the exploration-exploitation tradeoff that lead to frustration and so, so
much pain. And oh do I have stories for this. So many, in fact, that I've had
to cut some to avoid rambling.

Here's one of my favorite videos. This is an implementation of NAF, learning
on the HalfCheetah environment.

VIDEO

Now, from an outside perspective, this is stupid, because *of course* running
on your feet is faster than falling on your back and flailing your legs. But
we can only say that because we have a bunch of prebuilt knowledge that tells
us running on your feet is a better idea. RL doesn't know this! It sends actions
and gets back states and rewards. That's it. I don't have videos from earlier
in the training run, but here is my guess for what happened.

* Through random exploration, it learned to fall forward. This gives a bit of
reward because it's moved forward a bit.
* It keeps doing that because it got rewarded for it, and the behavior got
"burned into" the policy. Now it's falling consistently.
* Through random exploration, from that state it learns a backflip maneuver.
This gives more reward, because it lunged forward even more.
* It keeps doing that because it got rewarded for it, and backflipping got
burned in too. Now it's backflipping consistently.

Once you're backflipping consistently, it's hard to flip yourself upright.
So the easiest path to high reward is to figure out how to move while lying
on your back. And thus, this.

SAME VIDEO

Here's another example, on the Reacher environment.

VIDEO

In this run, the initial policy was one that tended to output large accelerations
in either direction. It's easy to perpetuate this behavior - just make all your
neural network weights really big. And it's hard to deviate from this policy
in a meaningful way - you have to take several exploration steps to do anything
useful reward-wise. Meanwhile, this "spin really fast" policy isn't actually
that bad, because in every revolution around the center the endpoint gets *kind-of*
close to the target location.

Combine all of this together, and it never learns to stop spinning really fast.

This shows up in Atari games too. In Breakout, the first thing any policy learns
is to always output right. If you do so, you're guaranteed to reflect the
ball at least once and clear one brick. And hey - 5 points is better than 0
points.

VIDEO

Breakout is simple enough that most well-tuned learning algorithms will reliably
learn to superhuman performance. But for a poorly tuned algorithm, it's possible
that the policy will never do enough exploration to learn that it should be
moving to track the ball.

These are all examples of the classic exploration-exploitation problem that has dogged
reinforcement learning since time immemeorial.
Your data comes from your current policy. If your current policy explores too
much you get junk data and learn nothing. Exploit too much and you burn-in
behaviors that aren't optimal.

There are several intuitively pleasing ideas for addressing this, and many of
them have been attempted recently - intrinsic motivation, curiosity-driven
exploration, count-based exploration, to name a few. However, none of them
seem to be a silver bullet that works all the time.
I'm skeptical such a silver bullet will be figured out anytime soon. The
problem is really, really, really, really, really hard. To quote the
[Wikipedia article on multi-armed bandits](https://en.wikipedia.org/wiki/Multi-armed_bandit),

> Originally considered by Allied scientists in World War II, it proved so intractable that, according to Peter Whittle, the problem was proposed to be dropped over Germany so that German scientists could also waste their time on it.

I find it's almost productive to imagine your computer is a demon that's
deliberately misinterpreting your reward and actively searching for local optima.
Whenever your reward function gives small reward for stupid behavior, there's always a
chance learning just gets completely stuck at the wrong thing. It's just
how deep RL is right now.


Stability
------------------------------------------------------------------------

Almost every ML algorithm has hyperparameters, which influence the behavior
of the learning system. Often, these are picked by hand, or by random search.

Supervised learning is stable. Fixed dataset, ground truth targets. If you
change the hyperparamters a little bit,
your performance won't change that much. Not all hyperparameters perform
well, but it's relatively easy to see signs of life. These signs of life are
super important, because they tell you that you're on the right track, you're
doing something reasonable, and it's worth investing more time.

Reinforcement learning, on the other hand, isn't stable at all. It is aggressively,
almost pathologically unstable.

Let me give you an example. When I started working at Brain, one of the first
things I did was to reproduce the results of a continuous RL paper. Specifically,
I was trying to reproduce the Normalized Advantage Function paper.

After about a month of failures, I finally solve my first task: the Pendulum
task from OpenAI Gym. If you're unfamiliar with this task, it's very simple.
There's a pendulum. There's gravity. The pendulum hangs from a point.
Each timstep, you can apply some acceleration around that point. You get
more reward the closer the pendulum is to the top, with maximum reward if
you can balance the pendulum.

GIF

This is the easiest continuous control task in OpenAI Gym - the action space
is 1-dimensional, the state space is 3-dimensional, and you have a shaped reward
that's giving you fine-grained feedback on how close you are to the top.

Why did this take me a month? Well, I was partly learning how TensorFlow worked.
I also had a bunch of bugs that took a while to pin down.
Now, when I finally get it to work, here are the plots of performance. Success
on this task is defined as hitting -100 to -150 reward. This is 10 independent
runs, with the exact same hyperparameters. The only difference is the random seed.

PICTURE

10 seeds, and *3* of them work within 200k steps? If this happened in supervised
learning you'd be digging for bugs in data loading or training to figure out
why it's so unstable. In reinforcement learning? It's Tuesday.

<iframe width="560" height="315" src="https://www.youtube.com/embed/iVzAMmpMra8" frameborder="0" allowfullscreen></iframe>

To be fair, if you leave these running for longer, many of them do
eventually get hit the threshold for success, just in 800k steps instead of
200k steps. But this is still really frustrating. Taking 4 times longer to learn
by random chance is terrible!

Just to show this isn't a personal complaint: here is a plot from the VIME paper, trained with TRPO (which is billed as one of the more stable
RL algorithms.) The environment is HalfCheetah.

PLOT

The shaded region is the performance over 5 random seeds. The worst seed learned nothing and the
best seed got twice the reward.

And here's a [comment from Andrej Karpathy](https://news.ycombinator.com/item?id=13519044),
from earlier this year, when he was still at OpenAI.

> If it makes you feel any better, I've been doing this for a while and it took me last ~6 weeks to get a from-scratch policy gradients implementation to work 50% of the time on a bunch of RL problems. And I also have a GPU cluster available to me, and a number of friends I get lunch with every day who've been in the area for the last few years.
>
> Also, what we know about good CNN design from supervised learning land doesn't seem to apply to reinforcement learning land, because you're mostly bottlenecked by credit assignment / supervision bitrate, not by a lack of a powerful representation. Your ResNets, batchnorms, or very deep networks have no power here.
>
> SL wants to work. Even if you screw something up you'll usually get something non-random back. RL must be forced to work. If you screw something up or don't tune something well enough you're exceedingly likely to get a policy that is even worse than random. And even if it's all well tuned you'll get a bad policy 30% of the time, just because.
>
> Long story short your failure is more due to the difficulty of deep RL, and much less due to the difficulty of "designing neural networks".

Instability to random seed is like a canary in a coal mine. If only *randomness in
the initial conditions* is enough to lead to this much divergence, imagine how
much an *actual difference in the code* could make.

Oh that's right. You don't have to imagine.
The recent [Deep Reinforcement Learning That Matters](https://arxiv.org/abs/1709.06560)
paper has done this already. It makes a bunch of points that I think were well-known in the RL
community, but which wasn't packaged together until that paper. In particular,

* Changing the non-linearity can give significant differences in performance.
* Multiplying the rewards by a constant can give significant differences in performance.
* Different implementations of the same algorithm have different performance.

My theory is that RL is very sensitive to both your initialization and to the
dynamics of your training process, because your data is always collected online
and the only supervision you get is a single scalar for reward. A policy that
randomly stumbles onto good training examples will bootstrap itself much
faster than a policy that doesn't. A policy that fails to discover good training
examples in time will collapse towards learning nothing at all, as it becomes
more confident that any deviation it tries will fail.


What About All Reinforcement Learning Success Stories?
-------------------------------------------------------------------

To balance out the negative, there are certainly reinforcement learning
success stories. Here are some high-profile ones.

* AlphaGo and AlphaGo Zero reaching superhuman performance in Go.
* A Dota 2 1v1 Shadow Fiend bot that beat top pro players in laning.
* DeepMind annoucning that a learned agent achieved record low energey usage
in Google datacenters.
* Neural architecture search
* A learned Super Smash Brothers Melee bot that beat several pro players.
* DQN solving some Atari games to superhuman performance, without hand-designed
features.

Now, let me explain why I don't consider these as "RL working" yet.

Humans and computers are better at different things. This should be obvious:
we don't ask humans to add lots of numbers really fast, and we don't ask
computers to write literary analysis of *The Great Gatsby*.

A similar thing is true for reinforcement learning. Some environments are
easier for RL to solve than other environments. These are some properties that
generally help. Not all of them are required to be learnable, but each one
helps.

* Local optima in reward space are hard to find. This makes it harder for the
network learning to get stuck during learning.
* The reward is not a sparse 0-1 reward. The denser the reward signal, the easier
it is to give feedback on what actions are good and bad.
* It is easy to quickly generate a large amount of experience in the ground-truth
environment. Or, if this isn't possible, the simulation of the ground-truth
environment is accurate enough that you don't lose much from applying RL in
simulation instead of reality. This lets you sidestep sample inefficiencies.
* There are consistent short-term relationships between the action you
take and the rewards you receive. This makes credit assignment much easier.
* (A point about self-play)

Taking each success above in turn, they all have some of the attributes about.

* AlphaGo and AlphaGo Zero are both for playing Go, where a ridiculously large
amount of experience can be collected.
* The Dota 2 Shadow Fiend bot was restricted to a small part of the Dota game
space. No bottle, few items, 1v1 laning only, only Shadow Fiend out of over 100
heroes. The bot is most likely using the Dota 2 API to get exact health of all
enemies in sight. The reward signal is defined by gold and enemy health, both
of which are rich reward signals - if the agent was only given reward when it
got a kill, it would be much harder to learn.
* The Deepmind power center work built off prior work from the Google data
center team that showed PUE could be predicted with high accuracy from a fairly
small neural net, which evidently makes it good enough for learning. If a neural
net can do good prediction of (data center settings) --> PUE, then it seems
obvious that optimizing that network over its input could work.
* In the original neural architecture search paper, the policy outputs parameters
for building a neural net, and the reward is the validation accuracy of that network
when it's trained to convergence. Validation accuracy is actually a very rich
reward signal - an increase from 70% to 71% is going to be reflected in the final
reward. Across deep learning there's empirical evidence that hyperparameters
in deep learning are close-to linearly independent. Given both a rich reward and
seemingly simple relation between model choice and reward, working in "only"
2000 trained networks doesn't seem so far-fetched.
* The SSBM bot shares many similarities to the Dota 2 setup - 1v1, Battlefield
only, infinite time match to avoid having to learn strategies around stocks,
Captain Falcon dittos only, and a rich reward based on amount of % dealt and
taken.

The broader point I want to get across is this: many things have to go right
for reinforcement learning to even be a plausible solution, and then it still
requires a lot of careful work to get that solution.


Looking to The Future?
-------------------------------------------------------------------------------

There's an old saying - every researcher learns how to hate their area of
study. The trick is that researchers will press on despite this, because they
like the problems too much to quit them.

That's about where I'm at with deep reinforcement learning. Despite my reservations
and pessimism, I'm still interested in working in that area, keeping up with
what's going on, and wanting to see where it's all going. Notice that I said
"Deep Reinforcement Learning Doesn't Work *Yet*". I see no reason why it couldn't
work, given more time.

To play devil's advocate, let's take all the things I mentioned above that help
RL learn, and argue potential reasons why they might not matter.

* Local optima in reward space stop learning: with a good curriculum or good
exploration policy, this becomes less of an issue. And it's possible that if you
throw tons of agents with different exploration strategies, you can brute-force
your way past the problem. Or, a locally optimal solution could be good enough.
Humans certainly aren't the best possible lifeform that can come out of evolution,
but we're still good enough to get lots of things done.
* Sparse rewards are difficult to learn: this is true, but the flip side is that
sparse rewards can have fewer spurious minima than shaped rewards. The Learning
from Human Preferences paper found that human-feedback actually outperformed the
given shaped rewards, possibly because the hand-designed reward didn't give
enough reward for partial progress. It's possible we can either hallucinate
positive rewards (HER), define auxiliary tasks (UNREAL), or do predicitive
learning to build a world model from transitions that give 0 reward.
* Requires a large amount of experience:
