---
layout: post
title:  "Reinforcement Learning Doesn't Work Yet"
date:   2017-08-05 01:31:00 -0700
---

*Like all rants, this accentuates the negatives and downplays the positives,
so you should take it with some skepticism.*

*I mostly cite papers from Berkeley and DeepMind, because I know more of the
researchers there. Apologies if I missed a paper you think is relevant.*

I once mused that whenever people ask me if RL can solve their problem, I tell
them it can't, and about 70% of the time I'm right.

Every now and then, I'm at a party and someone wonders why reinforcement learning
hasn't been applied to many real-world problems.

Now, people don't actually directly ask me whether RL will work or not. It's
more about the heavy optimism some people have for reinforcement learning. The
naive, wide-eyed belief that RL works, and it's just a matter of time until
it changes the world, and it pains me that I end up being the one that has to
burst their aspirations and bring them down to Earth.

Reinforcement learning has mountains and mountains of hype behind it, somewhat
deservedly. It's an incredibly, incredibly flexible paradigm, and a robust,
performant reinforcement learning system should theoretically be great at
everything. It's the kind of dream that makes the money keep flowing.

Unfortunately, it doesn't really work. I believe it can work, but there are a
lot of problems in the way, many of which feel very fundamentally difficult.
Hidden behind the beautiful demos is mountains and mountains of pain, failed
experiments, statistically insignificant results, and pure, nonsensical
bullshit.

This is part of why all my AI timelines are fairly long term, by the way.

Okay. Why doesn't RL work?


It Can Be Horribly Sample Inefficient
-------------------------------------------------------------------------------

The most well-known benchmark for deep reinforcement learning is Atari. Throw
Q-Learning at it, and some (but not all!) of the games get solved to superhuman
performance.

Atari games run at 60 frames per second. Take a moment and guess how many frames
a state-of-the-art DQN takes to train.


The answer depends on the game, but let's take a look at a recent Deepmind
paper, [Rainbow DQN](https://arxiv.org/abs/1710.02298). Over the past 5 years,
there have been several extensions made to the original DQN network. Evidently,
some researchers got curious to see what would happen if you combined all the
advances together. The results come in this handy chart.

CHART

The y-axis is "median human-normalized score". There are 57 Atari games in the
benchmark. For each game, the score is normalized such that human performance
is 100%. The plotted curve is the median of those normalized scores.
Rainbow matches that within 7 million frames, which is about 1900 hours of
play.

Mind you, this is actually pretty great, because the old record (Distributional
DQN) didn't hit 100% median performance within 44 million frames. And the
original DQN (the one from the Nature paper) never even hits 100%. It gets to
79% median performance after 200 million frames.

Hundreds of thousands or millions of steps is roughly the scale for a number
of common benchmarks. Here's a plot from the PPO paper, https://arxiv.org/abs/1707.06347,
which is looking like one of the most performant RL algorithms so far.

PLOT

In pretty much all the RL benchmarks, you can solve the problem faster with some
other method. Cartpole is solvable with LQR or CEM. The MuJoCo environments are
solvable on-the-fly in real-time with model predictive control
(albeit with perfect models of environment dynamics). Atari can be solved
faster with search. So on one hand, it's not surprisingly that RL takes longer
to learn. On the other hand, it can be really, really disappointing to see
something requires thousands of times more samples. This is a problem in any
setting where it's hard to get a ridiculously large number of samples.


It (Usually) Requires a Reward Oracle
------------------------------------------------------------------------------

There are some papers on learning a reward function, often based on imitation
learning or expert feedback. However, outside of these exceptions, reinforcement
learning assumes that your reward function captures *exactly* what you want.
And I mean *exactly*. RL has an annoying tendency to overfit to your reward and
doing something you don't want it to.

Take Atari, for example. You want to maximize score. You
*always* want to maximize score. It's just how those games work, and it's why
Atari games are such a nice benchmark.

The majority of reinforcement learning results come from simulated environments,
and that's because you have perfect knowledge of the state and can define an
appropriate reward. Consider the MuJoCo tasks in OpenAI gym.
The Reacher task sants to reach a target location. You have exact locations of
everything, so reward is simply the distance. Easy. In HalfCheetah, you give
more reward if you make it run forward really fast. Also straightforward.

*Reward functions that don't have this property tend to work poorly, and sometimes
do weird things that give reward without solving the problem.* For example,
language models are often trained by maximizing the likelihood of a translation,
then evaluated by computing their BLEU score. Salesforce tried optimizing BLEU
score directly with RL, and found that although they got better BLEU score,
humans rated the translations as lower quality. If your reward function doesn't
reflect something you care about, there is no guarantee your model will care
about it either.
Reinforcement learning is a variant of black-box optimization, and
it's well-known that black-box optimization will learn whatever junk it
gets rewarded for.

A good example comes from this DeepMind paper, where the authors use DDPG to
learn to grasp and lift a block. The policy gets reward for increasing the
block's height, which is defined as the z-coordinate of the bottom face.

The policy ended up learning this.

GIF

This makes the bottom face have a higher z-coordinate. Now, clearly this isn't
what it's supposed to do...but RL doesn't care. From the perspective of the
learning algorithm, it did something, and got positive reward, so it's going to
keep doing that thing.

Anecdotally, a coworker talked about trying out RL on a toy environment. The
episode terminated whenever the agent walked out of bounds. Fairly standard
stuff. Except, he forgot to add a large negative penalty for terminating this
way. And what ended up happening was that the agent learned to be suicidal if
it was ever close to getting negative reward, because a quick death that ends in
0 reward is preferably to a life of -1 reward.

I know some people like to tell stories about paperclip
optimizers, but there's no reason to go speculating that far when there are
present-day examples. You know those MuJoCo environments I talked about? Most
of them have an extra reward term that penalizes actions of large magnitude.
Without this, you can get wildly unrealistic behavior.

Note that this goes both ways: the DeepMind parkour paper learned several novel
behaviors by creating environments with difficult-to-traverse terrain, just by
using a reward of "go forward without falling".


Even Given a Reward Oracle, Local Optima in Behavior Space Can Be Hard To Escape
------------------------------------------------------------------------------

A friend once told me a similar issue: they were using RL to train a robot
learn to move to a point in space, that was defined with respect to a table.
So the arm learned to move down at the table REALLY FAST, which made the table
fall down, such that the point *just happened* to move to the arm.

Not all of this is even reward hacking. Usually reward hacking implies cleverness
or out-of-the-box solutions that give more reward than the intended solution.
But these behaviors aren't clever, and they don't give more reward. They're
just stupid, a poor solution to the exploration-exploitation tradeoff. Whenever
your reward function gives small reward for stupid behavior, there's always a
chance learning just gets completely stuck at the wrong thing. It's just
how deep RL is, unfortunately.


Even Given a Reward Oracle, Local Optima in Behavior Space Can Be Hard To Escape
------------------------------------------------------------------------------

By behavior space, I mean "the set of possible behaviors an agent might learn
in this environment". It doesn't mean anything mathematical.

Here's one of my favorite videos. This is an implementation of NAF, learning
on the HalfCheetah environment.

VIDEO

Now, from an outside perspective, this is stupid, because *of course* you should
be running on your feet, not by flailing on your back. But that's only because
we have the outside context to know that running on the end of your feet
is what's "supposed" to happen. I don't have videos from earlier in training,
but this is my guess.

* The robot gets reward for moving forward.
* Through random exploration, it learns to backflip. This gives a bit of reward
since it succesfully lunged forward a bit.
* The policy learned to do this consistently.
* Once it learned to backflip, it's hard to reorient yourself, forcing the policy
to learn this back-flailing maneuver.

Here's another example, on the Reacher environment.

VIDEO

At some point, the policy learned to spin itself really, really fast. It's simple
to do so - just output the maximum torque allowed at each joint. No single
deviation is enough to change this behavior, so random exploration isn't enough
to escape this behavior, and it keeps spinning forever.

This shows up in Atari games too. In Breakout, the first thing any policy learns
is to always output right. That reflects the ball once and gives 5 points instead
of 0.

VIDEO

When tuned properly, a policy eventually learns to play the game.


Stability
------------------------------------------------------------------------

Almost every ML algorithm has hyperparameters, which influence the behavior
of the learning system. Often, these are picked by hand, or by random search.

Supervised learning is stable. If you change the hyperparamters a little bit,
your performance won't change that much. Not all hyperparameters perform that
well, but it's relatively easy to see signs of life. These signs of life are
super important, because they tell you that you're on the right track, and
that it's worth investing more time.

Reinforcement learning, on the other hand, isn't stable at all. It is aggressively,
almost pathologically unstable. It is so bad that it's almost productive to
imagine your computer is a demon that's trying to misinterpret what you're trying
to get it to optimize.

Let me give you an example. When I started working at Brain, one of the first
things I did was to reproduce the results of a continuous RL paper. Specifically,
I was trying to reproduce the paper that introduces Normalized Advantage Function.
Now, as luck would have it, the first author of that paper was interning at
Brain and sitting right next to me. He doesn't have his old code anymore but he
helpfully gives me a few pointers.

After about a month of failures, I finally solve my first task: the Pendulum
task from OpenAI Gym. If you're unfamiliar with this task, it's dead simple.
There's a pendulum. You send some amount of acceleration in either direction
around the circle. You get the most reward if you can balance the pendulum at
the top.

GIF

Now, when I finally get it to work, here are the plots of performance. Success
on this task is defined as hitting -100 to -150 reward. This is 10 independent
runs, with the exact same hyperparameters. The only difference is the random seed.

PICTURE

10 seeds, and *3* of them work within 200k steps? If this happened in any other
branch of ML, you'd be digging for bugs and would be trying to track down what's
going wrong. In RL, it's Tuesday.

Karpathy quote.

"I get lunch with people who have been in this field for a few years,
and after a few months I finally got RL to solve a task. Except then it
fails 30% of the time, just cause."

To be fair, if you leave these jobs running for longer, many of them do
eventually get past the success bar, after around 800k steps instead. But there
are other environments where that just doesn't happen. To wit: here is a plot
from the VIME paper, trained with TRPO (which is billed as one of the more stable
RL algorithms.) The environment is HalfCheetah.

PLOT

Hey, that reward curve looks reasonable. Except note the shaded region is the
performance over 5 random seeds, where the worst seed learned nothing and the
best seed got twice the reward.

Not all environment will have this kind of behavior, but many do.


Why?
---------------------------------------------------------------------------

The answer to "why?" isn't a big research question. In fact it is fairly obvious
once you do enough in the space, and it quickly becomes clear how difficult the
problem is.

Everything comes down to the exploration-exploitation tradeoff. The short version
is this: if you spend too much time on exploration, 

The core issue is that the data you collect depends on your current policy.
Let's consider just on-policy learning for now. In on-policy RL, you're trying
to learn a policy. You initially start by playing randomly. Over time, your policy
tries things, gets reward signal, and gets updated to try to make good rewards
happen more often. Importantly, all your data is getting generated online, and
the data you see at any point depends on your current policy.

Very often, there are behaviors that essentially act like local maxima in reward
space. As an example, consider the Hopper environment.

GIF

In this environment, you want the robot leg to hop forward. You get reward for
travelling forward, and the episode ends if the leg falls to the ground, which
happens when the leg falls too far forward.

If you visualize learned policies over time, you usually see the policy first
learn to jump forward.

This gives a small burst of reward, and then the episode stops.

The correct thing to do is to jump forward, but then start leaning back to
stabilize in time for another jump...

GIF

but it can be very difficult to learn to explore to this behavior, *because
if it leans back without learning to land stably, it gets less reward than
learning to lunge forward.*

If the policy is at a behavior that's far from a behavior that gives higher reward,
it may learn that any exploration just gives worse performance. Over time the
policy becomes more and more confident that it's doing the right thing, and
the policy never recovers learning.

And on the flip side, if the policy gets lucky and explores to a good behavior,
it can learn much more quickly than another one.
