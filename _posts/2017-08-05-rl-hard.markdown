---
layout: post
title:  "Reinforcement Learning Doesn't Work Yet"
date:   2017-08-05 01:31:00 -0700
---

*Like all rants, this should be taken with a grain of salt.*

*I mostly cite papers from Berkeley and DeepMind, because I know more about them.
Apologies if I missed a paper you think is relevant.*

I once mused that whenever people ask me if RL can solve their problem, I tell
them it can't, and about 70% of the time I'm right. Now, people don't actually
ask me that enough for me to figure out how accurate I am, but in general
people heavily overestimate what reinforcement learning can do.

Reinforcement learning has mountains and mountains of hype behind it. I think
RL certainly deserves a lot of hype, but not as much as it's currently getting,
given how little it can actually do. Hidden behind the beautiful demos is
mountains and mountains of pain, failed experiments, instability, and pure,
nonsensical bullshit.

This is part of why all my AI timelines are fairly long term, by the way.

Okay. Why doesn't RL work?


Stability
------------------------------------------------------------------------

Almost every ML algorithm has hyperparameters, which influence the behavior
of the learning system. Often, these are picked by hand, or by random search.

Supervised learning is stable. If you change the hyperparamters a little bit,
your performance won't change that much. Not all hyperparameters perform that
well, but it's relatively easy to see signs of life. These signs of life are
super important, because they tell you that you're on the right track, and
that it's worth investing more time.

Reinforcement learning, on the other hand, isn't stable at all. It is aggressively,
almost pathologically unstable. It is so bad that it's almost productive to
imagine your computer is a demon that's trying to misinterpret what you're trying
to get it to optimize.

Let me give you an example. When I started working at Brain, one of the first
things I did was to reproduce the results of a continuous RL paper. Specifically,
I was trying to reproduce the paper that introduces Normalized Advantage Function.
Now, as luck would have it, the first author of that paper was interning at
Brain and sitting right next to me. He doesn't have his old code anymore but he
helpfully gives me a few pointers.

After about a month of failures, I finally solve my first task: the Pendulum
task from OpenAI Gym. If you're unfamiliar with this task, it's dead simple.
There's a pendulum. You send some amount of acceleration in either direction
around the circle. You get the most reward if you can balance the pendulum at
the top.

GIF

Now, when I finally get it to work, here are the plots of performance. Success
on this task is defined as hitting -100 to -150 reward. This is 10 independent
runs, with the exact same hyperparameters. The only difference is the random seed.

PICTURE

10 seeds, and *3* of them work within 200k steps? If this happened in any other
branch of ML, you'd be digging for bugs and would be trying to track down what's
going wrong. In RL, it's Tuesday.

Karpathy quote.

"I get lunch with people who have been in this field for a few years,
and after a few months I finally got RL to solve a task. Except then it
fails 30% of the time, just cause."

To be fair, if you leave these jobs running for longer, many of them do
eventually get past the success bar, after around 800k steps instead. But there
are other environments where that just doesn't happen. To wit: here is a plot
from the VIME paper, trained with TRPO (which is billed as one of the more stable
RL algorithms.) The environment is HalfCheetah.

PLOT

Hey, that reward curve looks reasonable. Except note the shaded region is the
performance over 5 random seeds, where the worst seed learned nothing and the
best seed got twice the reward.

Not all environment will have this kind of behavior, but many do.


Why?
---------------------------------------------------------------------------

The answer to "why?" isn't a big research question. In fact it is fairly obvious
once you do enough in the space, and it quickly becomes clear how difficult the
problem is.

Everything comes down to the exploration-exploitation tradeoff. The short version
is this: if you spend too much time on exploration, 

The core issue is that the data you collect depends on your current policy.
Let's consider just on-policy learning for now. In on-policy RL, you're trying
to learn a policy. You initially start by playing randomly. Over time, your policy
tries things, gets reward signal, and gets updated to try to make good rewards
happen more often. Importantly, all your data is getting generated online, and
the data you see at any point depends on your current policy.

Very often, there are behaviors that essentially act like local maxima in reward
space. As an example, consider the Hopper environment.

GIF

In this environment, you want the robot leg to hop forward. You get reward for
travelling forward, and the episode ends if the leg falls to the ground, which
happens when the leg falls too far forward.

If you visualize learned policies over time, you usually see the policy first
learn to jump forward.

This gives a small burst of reward, and then the episode stops.

The correct thing to do is to jump forward, but then start leaning back to
stabilize in time for another jump...

GIF

but it can be very difficult to learn to explore to this behavior, *because
if it leans back without learning to land stably, it gets less reward than
learning to lunge forward.*

If the policy is at a behavior that's far from a behavior that gives higher reward,
it may learn that any exploration just gives worse performance. Over time the
policy becomes more and more confident that it's doing the right thing, and
the policy never recovers learning.

And on the flip side, if the policy gets lucky and explores to a good behavior,
it can learn much more quickly than another one.


It (Mostly) Requires a Reward Oracle
------------------------------------------------------------------------------

Every now and then, I'm at a party and someone wonders why reinforcement learning
hasn't been applied to many real-world problems.

There are several reasons why, but a big one is that *most RL depends on having
a reward function that captures exactly what you want.* The majority of
reinforcement learning results come from simulated environments, where you have
perfect knowledge of the entire state and can define a reward appropriately.
In Atari, you want to maximize score. You always want to maxmize score. That's
just how those games works. For the MuJoCo tasks, a similar thing is true. Reacher
wants to reach to an exact target location. HalfCheetah wants to run forward
really fast. Cartpole wants to not fall down.

*Reward functions that don't have this property tend to work poorly, and sometimes
do weird things that give  reward without solving the problem.* Here's an example
from a DeepMind paper on learning to grasp a block.
Reinforcement learning is best thought of as a variant of black-box optimization.

