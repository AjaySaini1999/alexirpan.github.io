---
layout: post
title:  "Deep Reinforcement Learning Doesn't Work Yet"
date:   2017-12-11 23:06:00 -0700
---

*Like all rants, this accentuates the negatives and downplays the positives.
Don't take it too seriously.*

*I mostly cite papers from Berkeley and DeepMind, because I'm more familiar with
their work. Apologies if I missed a paper you think is relevant.*

I once mused that hypothetically, if people asked me if RL can solve their
problem, I would just tell them it can't, because that would be correct about
70% of the time. I haven't gotten to test this empirically, but I stand by this.

Deep reinforcement learning has mountains and mountains of hype behind it, and
it deserves it. It's an incredibly, incredibly flexible paradigm, and in
principle, a robust and performant RL system should be great at everything - it
is almost certainly going to be part of a general AI. That's the kind of dream
that fuels millions of dollars of funding.

Unfortunately, it doesn't really work.

Now, I believe it can work, otherwise I wouldn't be interested in it.
But there are a lot of problems in the way, many of which feel fundamentally
difficult. Hidden behind the beautiful demos is a towering garbage pile of
pain, failed experiments, statistically insignificant results, and pure, nonsensical
bullshit.

My AGI timelines are long because everyone is looking towards RL
as the key, and to me it looks like a really, really shitty key.

So, to all the people who ask me why RL isn't used on more real-world problems,
or who believe that deep RL actually does reasonable things, or who have the
naive, wide-eyed belief that their new research idea will only take a few weeks to
test out: this post is for you. I take no joy in this. In fact, it pains me that
I have to burst these lofty aspirations and bring them down to Earth. I only
do this because people aren't talking about the dark sides of RL enough.


Reasons Deep Reinforcement Learning Sucks Right Now
------------------------------------------------------------------------------

List them


It Can Be Horribly Sample Inefficient
-------------------------------------------------------------------------------

The most well-known benchmark for deep reinforcement learning is Atari. Throw
DQN at it, and some (but not all!) of the games get solved to superhuman
performance.

Atari games run at 60 frames per second. On the top of your head, do you
know how many frames a state-of-the-art DQN needs to reach human performance?

The answer depends on the game, but let's take a look at a recent Deepmind
paper, [Rainbow DQN](https://arxiv.org/abs/1710.02298). This paper came out
about 2 months ago, and it's a combination
of the several incremental advances made in the past 5 years. Evidently,
somebody finally got curious to see what would happen if you combined all the
advances together. The results come in this handy chart.

![Figure from Rainbow DQN](/public/rl-hard/rainbow_dqn.png)
{: .centered }

The y-axis is "median human-normalized score". This is computed by training
57 DQNs, one for each Atari game, normalizing the score of each agent such that
human performance is 100%, then plotting the median performance across the
57 games. RainbowDQN matches original DQN performance in about 7 million
frames, and passes the 100% threshold at around 18 million frames. This
corresponds to about 83 hours of play experience, which doesn't include the
time needed to actually compute the outputs and gradients for the Q-network.

Mind you, this is actually a pretty great, because the old record (Distributional
DQN) didn't hit 100% median performance until abouut 70 million frames. The
original DQN (the one from the Nature paper) never even hits 100% normalized
performance. After 200 million frames it reaches 79% median performance.

Many common benchmarks require hundreds of thousands or millions of steps. And,
I mean, that's fine, but it feels like many of the benchmarks shouldn't require
this many steps. Here's a plot from another state-of-the-art algorithm,
[PPO](https://blog.openai.com/openai-baselines-ppo/). This was from about 5 months
ago, and was the algorithm behind the famous [parkour paper](https://arxiv.org/abs/1707.02286).

<iframe width="560" height="315" src="https://www.youtube.com/embed/hx_bgoTF7bs" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

These results are really impressive, but based on the paper it took about 100
hours of training with 64 workers to learn the policy for the Humanoid. That's
a lot of computation, considering that online trajectory optimization has done
similarly cool things since 2012.

<iframe width="560" height="315" src="https://www.youtube.com/embed/uRVAX_sFT24" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

This isn't a fair comparison, since model predictive control is doing planning
against a known dynamics model. In a similar vein, you can easily outperform
[DQN in Atari with Monte Carlo Tree Search instead](https://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning).
The point is more to show that RL uses orders of magnitude more computation
to arrive at the same result as other more domain-specific algorithms. This is
a common pattern - generality comes at the price of sample efficiency.


It (Usually) Requires a Reward Oracle
------------------------------------------------------------------------------

RL algorithms assume the existence of a reward function. Usually, this is either
given, or it is learned offline and fixed over the course of learning. Importantly,
for RL to do the right thing, your reward function must capture *exactly* what
you want.
And I mean *exactly*. RL has an annoying tendency to overfit to your reward and
do things you don't expect.

Take Atari, for example. You want to maximize score. You
*always* want to maximize score. It's just how those games work. That's why
Atari games are such a nice benchmark.

The majority of reinforcement learning results come from simulated environments,
and that's because you have perfect knowledge of the state and can define an
appropriate reward. Consider the MuJoCo tasks in OpenAI gym.
In the Reacher task, you control a two-segment arm, that's connected to a central
point, and the goal is to move the end of the arm to a target location.

GIF

You have the exact locations of everything, so reward is simply the distance to
the goal. Easy.

In the HalfCheetah environment, you have a two-legged robot, restricted to a
vertical plane, meaning it can only run forward or backward.

GIF

The goal is to learn
a running gait, so you give reward depending on its velocity. Also straightforward.

**Reward functions that don't have this property tend to work poorly.** There's
a semi-famous boat racing example from an [OpenAI blog post](https://blog.openai.com/faulty-reward-functions/),
where finishing the race gives less points than collecting powerups.

<iframe width="560" height="315" src="https://www.youtube.com/embed/tlOIHko8ySg" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

To be honest, at the time I really didn't understand why this merited a blog
post. It was just so *obvious*. Of course optimizing the wrong objective leads
to weird behavior! What, do people not see this happening in schools?
(Teaching to the test instead of teaching to learn.) In academia? (Publishing
more papers instead of publishing good papers because of publish or perish.)
How is it news that RL does the same thing? Do enough RL and you inevitably
run into an issue like this.

(Then I started writing this blog post, and found that the most compelling
video I could find was the boat racing video from the blog post. So, I now
begrudgingly accept that the post had a point.)

For example, [some researchers at Salesforce](https://www.salesforce.com/products/einstein/ai-research/tl-dr-reinforced-model-abstractive-summarization/)
trained a text summarization model.
This is usually trained with supervised learning. For evaluation, they use an
automated metric called ROUGE, as well as human ratings. ROUGE isn't differentiable,
so they tried using RL to optimize ROUGE. And yes, you can get much better
ROUGE scores if you train with RL, except there's one problem.

> While ROUGE scores have a good correlation with human judgment in general, the summaries with the highest ROUGE aren't necessarily the most readable or natural ones. This became an issue when we trained our model to maximize the ROUGE score with reinforcement learning alone. We observed that our models with the highest ROUGE scores also generated barely-readable summaries.

Again, if your reward function doesn't reflect something you care about,
there's no guarantee the learned model will care about it either.
Reinforcement learning is just a variant of black-box optimization, after all -
it's well-known that black-box optimization will learn whatever junk it
gets rewarded for.

Here's another example. In [this paper from DeepMind](https://arxiv.org/abs/1704.03073),
the authors use DDPG to learn a grasping policy.
The policy gets reward based on how high the block is, which is defined as
the z-coordinate of the bottom face.

The policy ended up learning this.

<iframe width="560" height="315" src="https://www.youtube.com/embed/8QnD8ZM0YCo?start=27" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>

Now, clearly this isn't the intended solution...but RL doesn't care.
From the perspective of the learning algorithm, it did something, and got
positive reward, so it's going to keep doing that thing.

I don't have any more videos, or paper references, but I do have more anecdotal
stories.

* A coworker is applying RL to a toy navigation problem.
The episode terminates whenever the agent walks out of bounds.
Fairly standard stuff. Except, he forgot to add a large negative penalty if
it walks out of bounds.
And what ended up happening was that the agent learned to be suicidal if
it was ever close to getting negative reward. Reaching the positive reward
was too difficult, and a quick death ending in 0 reward is preferable to a
life of -1 reward.
* A friend is applying RL to another racing game in Universe.
The reward is defined by applying OCR to a segment of the game screen to
extract the score. In initial exploration, due to dumb luck, the OCR made a
rare mistake. It saw the starting line of the race, and interpreted it as
a leading 1, increasing the score by an order of magnitude. Upon seeing such a
large reward, the policy spent the rest of learning trying (and failing) to
reproduce that scenario.
* The same friend is training a simulated robot to reach towards a point
above a table. It turns out the point was defined *with respect to the table*.
The policy learned to pound the table really hard, making it fall down *just so*
to make the goal point move to the end of the arm.
* A researcher is training a simulated hand to pick up a hammer and push in a nail.
The reward is defined by how far the nail is pushed into the hole. Instead of
picking up the hammer, the policy learned to punch the nail into the hole. So
they added a reward term to encourage picking up the hammer. When trained again,
the policy learned to pick up the hammer...then throw it at the nail, instead
of hammering it in.

I haven't verified any of the above for myself, but none of it sounds
implausible to me. I've been burned by reinforcement learning too many times to
believe otherwise.

I know people like to tell stories about paperclip
optimizers, but there's no reason to speculate that far when
present-day examples show up all the time.


Even Given a Reward Oracle, Local Optima Can Be Hard To Escape
------------------------------------------------------------------------------

All the previous examples are commonly called "reward hacking", which to me
implies a clever, out-of-the-box solution that gives more reward than the
intended answer.

These are the exceptions. The much more common case is stupidity - poor
local optima that come from getting the exploration-exploitation tradeoff
wrong. Ones that lead to frustration and so, so much pain,

And oh do I have stories for this.

Here's one of my favorite videos. This is an implementation of NAF, learning
on the HalfCheetah environment.

VIDEO

Now, from an outside perspective, this is really, *really* dumb. But we can
only say it's dumb because we have a bunch of prebuilt knowledge that tells
us running on your feet is better. RL doesn't know this! It sends actions,
and gets back states and rewards. That's it. Here's my best guess for what
happened.

* Through random exploration, it learned to fall forward. This gives a bit of
reward - moving forward is better than not moving at all.
* It keeps doing that because it got rewarded for it, and the behavior got
"burned into" the policy. Now it's falling forward consistently.
* Through random exploration, from that state it learns a backflip maneuver.
This gives more reward - backflipping forward is better than lying down.
* It keeps doing that because it got rewarded for it, and backflipping got
burned in too. Now it's backflipping consistently.
* Once the policy flips to its back consistently, it's too hard to learn
how to flip right side up. So, the easiest path to high reward
is to figure out how to move forward while while lying on your back.

And thus, this.

SAME VIDEO

Realistically though, I have no idea if my theory above is right. All I know
is that it didn't learn.

Here's another failed run, this time on the Reacher environment.

VIDEO

In this run, the initial random weights tended to output highly positive or
highly negative action outputs. That made it easy to learn how to spin really
fast. It's easy to perpetuate this behavior - just make all your
neural network weights large in magnitude. It's hard to deviate from this policy
in a meaningful way - you have to take several exploration steps to stop spinning,
and until then your actions barely influence your state.
Meanwhile, this "spin really fast" policy is actually not that awful -
in every revolution, the endpoint is guaranteed to pass by the target location.

Combine all of this together, and it never learns to stop spinning really fast.

(May kill the Breakout example, not necessary)

This shows up in Atari games too. In Breakout, the first thing any policy learns
is to always output right. If you do so, you're guaranteed to reflect the
ball at least once and clear one brick. And hey - 5 points is better than 0
points.

VIDEO

Breakout is simple enough that most well-tuned learning algorithms will reliably
learn to superhuman performance. But for a poorly tuned algorithm, it's possible
that the policy will never do enough exploration to learn that it should be
moving to track the ball.

-------------------------

These are all examples of the classic exploration-exploitation problem that has dogged
reinforcement learning since time immemeorial.
Your data comes from your current policy. If your current policy explores too
much you get junk data and learn nothing. Exploit too much and you burn-in
behaviors that aren't optimal.

There are several intuitively pleasing ideas for addressing this, and many of
them have been attempted recently - intrinsic motivation, curiosity-driven
exploration, count-based exploration, and so forth. However, none of these have
worked consistently for all environments,
and I'm skeptical a similar solution will be figured out anytime soon. The
problem is really, really, really, really, really hard. To quote the
[Wikipedia article on multi-armed bandits](https://en.wikipedia.org/wiki/Multi-armed_bandit),

> Originally considered by Allied scientists in World War II, it proved so intractable that, according to Peter Whittle, the problem was proposed to be dropped over Germany so that German scientists could also waste their time on it.

I find it's almost productive to imagine your computer is a demon that's
deliberately misinterpreting your reward and actively searching for local optima.
Whenever your reward function gives small reward for stupid behavior, there's always a
chance learning just gets completely stuck at the wrong thing. It's just
how deep RL is right now.


Final Results are Incredibly Unstable and Hard to Reproduce
------------------------------------------------------------------------

Almost every ML algorithm has hyperparameters, which influence the behavior
of the learning system. Often, these are picked by hand, or by random search.

Supervised learning is stable. Fixed dataset, ground truth targets. If you
change the hyperparamters a little bit,
your performance won't change that much. Not all hyperparameters perform
well, but it's relatively easy to see signs of life. These signs of life are
super important, because they tell you that you're on the right track, you're
doing something reasonable, and it's worth investing more time.

Reinforcement learning, on the other hand, isn't stable at all. It is 
pathologically unstable.

When I started working at Brain, one of the first
things I worked on was reproducing the results of a continuous RL paper.
Specifically, I was trying to reproduce the Normalized Advantage Function
paper.

After about a month of failures, I finally solve my first task: the Pendulum
task from OpenAI Gym. If you're unfamiliar with this task, it's very simple.
There's a pendulum. There's gravity. The pendulum hangs from a point.
Each timstep, you can apply some acceleration around that point. You get
more reward the closer the pendulum is to the top, with maximum reward if
you can balance the pendulum.

GIF

This is the easiest continuous control task in OpenAI Gym - the action space
is 1-dimensional, the state space is 3-dimensional, and you have a shaped reward
that's giving you fine-grained feedback on how close you are to the top.

Why did this take me a month? Well, I was partly learning how TensorFlow worked.
I also had a bunch of bugs that took a while to pin down.
Now, when I finally get it to work, here are the plots of performance. Success
on this task is defined as hitting -100 to -150 reward. This is 10 independent
runs, with the exact same hyperparameters. The only difference is the random seed.

PLOT

10 seeds, and *3* of them work within 200k steps? If this happened in supervised
learning you'd be digging for bugs in data loading or training to figure out
why it's so unstable. In reinforcement learning? It's Tuesday.

<iframe width="560" height="315" src="https://www.youtube.com/embed/iVzAMmpMra8" frameborder="0" allowfullscreen></iframe>

To be fair, if you leave these running for longer, many of them do
eventually get hit the threshold for success, just in 800k steps instead of
200k steps. But this is still really frustrating. Taking 4 times longer to learn
by random chance is terrible!

Just to show this isn't a personal complaint: here is a plot from the VIME paper, trained with TRPO (which is billed as one of the more stable
RL algorithms.) The environment is HalfCheetah, modified to have sparse reward.

![Plot from VIME paper](/public/rl-hard/vime.png)

The dark line is the median performance over 10 random seeds, and the shaded
region is the 25th to 75th percentile. It's somewhat distributing that the 25%
line is so close to zero.

> If it makes you feel any better, I've been doing this for a while and it took me last ~6 weeks to get a from-scratch policy gradients implementation to work 50% of the time on a bunch of RL problems. And I also have a GPU cluster available to me, and a number of friends I get lunch with every day who've been in the area for the last few years.
>
> Also, what we know about good CNN design from supervised learning land doesn't seem to apply to reinforcement learning land, because you're mostly bottlenecked by credit assignment / supervision bitrate, not by a lack of a powerful representation. Your ResNets, batchnorms, or very deep networks have no power here.
>
> SL wants to work. Even if you screw something up you'll usually get something non-random back. RL must be forced to work. If you screw something up or don't tune something well enough you're exceedingly likely to get a policy that is even worse than random. And even if it's all well tuned you'll get a bad policy 30% of the time, just because.
>
> Long story short your failure is more due to the difficulty of deep RL, and much less due to the difficulty of "designing neural networks".

[Hacker News comment from Andrej Karpathy, back when he was at OpenAI](https://news.ycombinator.com/item?id=13519044)
{: .centered }

Instability to random seed is like a canary in a coal mine. If pure randomness
is enough to lead to these kinds of results, imagine how much an actual
difference in the code could make.

Oh that's right. You don't have to imagine.
The recent [Deep Reinforcement Learning That Matters](https://arxiv.org/abs/1709.06560)
paper has done this already. It makes a bunch of points that I think were well-known in the RL
community, but which wasn't packaged together until that paper. In particular,

* Changing the non-linearity can give significant differences in performance.
* Multiplying the rewards by a constant can give significant differences in performance.
* Different implementations of the same algorithm have different performance.

My theory is that RL is very sensitive to both your initialization and to the
dynamics of your training process, because your data is always collected online
and the only supervision you get is a single scalar for reward. A policy that
randomly stumbles onto good training examples will bootstrap itself much
faster than a policy that doesn't. A policy that fails to discover good training
examples in time will collapse towards learning nothing at all, as it becomes
more confident that any deviation it tries will fail.


Everything Is Just Incredible Overfitting to Patterns That Don't Make Sense
------------------------------------------------------------------------------


What About All Reinforcement Learning Success Stories?
-------------------------------------------------------------------

To balance out the negative, there are certainly reinforcement learning
success stories. Here are some high-profile ones.

* AlphaGo and AlphaGo Zero reaching superhuman performance in Go.
* A Dota 2 1v1 Shadow Fiend bot that beat top pro players in laning.
* DeepMind annoucning that a learned agent achieved record low energey usage
in Google datacenters.
* Neural architecture search
* A learned Super Smash Brothers Melee bot that beat several pro players.
* DQN solving some Atari games to superhuman performance, without hand-designed
features.

Now, let me explain why I don't consider these as "RL working" yet.

Humans and computers are better at different things. This should be obvious:
we don't ask humans to add lots of numbers really fast, and we don't ask
computers to write literary analysis of *The Great Gatsby*.

A similar thing is true for reinforcement learning. Some environments are
easier for RL to solve than other environments. These are some properties that
generally help. Not all of them are required to be learnable, but each one
helps.

* Local optima in reward space are hard to find. This makes it harder for the
network learning to get stuck during learning.
* The reward is not a sparse 0-1 reward. The denser the reward signal, the easier
it is to give feedback on what actions are good and bad.
* It is easy to quickly generate a large amount of experience in the ground-truth
environment. Or, if this isn't possible, the simulation of the ground-truth
environment is accurate enough that you don't lose much from applying RL in
simulation instead of reality. This lets you sidestep sample inefficiencies.
* There are consistent short-term relationships between the action you
take and the rewards you receive. This makes credit assignment much easier.
* (A point about self-play)

Taking each success above in turn, they all have some of the attributes about.

* AlphaGo and AlphaGo Zero are both for playing Go, where a ridiculously large
amount of experience can be collected.
* The Dota 2 Shadow Fiend bot was restricted to a small part of the Dota game
space. No bottle, few items, 1v1 laning only, only Shadow Fiend out of over 100
heroes. The bot is most likely using the Dota 2 API to get exact health of all
enemies in sight. The reward signal is defined by gold and enemy health, both
of which are rich reward signals - if the agent was only given reward when it
got a kill, it would be much harder to learn.
* The Deepmind power center work built off prior work from the Google data
center team that showed PUE could be predicted with high accuracy from a fairly
small neural net, which evidently makes it good enough for learning. If a neural
net can do good prediction of (data center settings) --> PUE, then it seems
obvious that optimizing that network over its input could work.
* In the original neural architecture search paper, the policy outputs parameters
for building a neural net, and the reward is the validation accuracy of that network
when it's trained to convergence. Validation accuracy is actually a very rich
reward signal - an increase from 70% to 71% is going to be reflected in the final
reward. Across deep learning there's empirical evidence that hyperparameters
in deep learning are close-to linearly independent. Given both a rich reward and
seemingly simple relation between model choice and reward, working in "only"
2000 trained networks doesn't seem so far-fetched.
* The SSBM bot shares many similarities to the Dota 2 setup - 1v1, Battlefield
only, infinite time match to avoid having to learn strategies around stocks,
Captain Falcon dittos only, and a rich reward based on amount of % dealt and
taken.

The broader point I want to get across is this: many things have to go right
for reinforcement learning to even be a plausible solution, and then it often
still requires a lot of careful work to make that solution happen.


Looking to The Future?
-------------------------------------------------------------------------------

There's an old saying - every researcher learns how to hate their area of
study. The trick is that researchers will press on despite this, because they
like the problems too much to quit them.

That's about where I'm at with deep reinforcement learning. Despite my reservations
and pessimism, I'm still interested in working in that area, keeping up with
what's going on, and wanting to see where it's all going. Notice that I said
"Deep Reinforcement Learning Doesn't Work *Yet*". I see no reason why it couldn't

work, given more time. I think there are several avenues along which things can
get better.

*Good enough local optima:* If human experts are bad at a task, it doesn't
matter if an RL solution gets stuck at a local optima, as long as it's still
better than human performance.

*Just throw more hardware:* Given enough hardware, sample inefficiency may no
longer be the primary concern.

*Learned reward shaping:* The human preferences paper suggests that it's
possible for neural nets to implicitly define well-shaped reward functions
from human feedback. This, along with other parts of inverse RL, could let
us avoid the difficulties of explicit reward definition.

*Metalearning saves the day:* This is a point Pieter Abbeel likes to repeat.
RL algorithms are designed to apply to any environment that's an MDP. However,
we don't need to solve arbitrary environments, we just need to solve the real-world
environments we care about. In principle, metalearning could let use learn a
real-world prior that lets us learn new tasks faster. (i.e. if your goal is to
train an RL agent to navigate warehouses, you could presumably focus your
efforts on learning navigation priors.)

*Wild overfitting is actually okay:*

Note that this goes both ways: the DeepMind parkour paper learned several novel
behaviors by creating environments with difficult-to-traverse terrain, just by
using a reward of "go forward without falling".

work, given more time.

To play devil's advocate, let's take all the things I mentioned above that help
RL learn, and argue potential reasons why they might not matter.

* Local optima in reward space stop learning: with a good curriculum or good
exploration policy, this becomes less of an issue. And it's possible that if you
throw tons of agents with different exploration strategies, you can brute-force
your way past the problem. Or, a locally optimal solution could be good enough.
Humans certainly aren't the best possible lifeform that can come out of evolution,
but we're still good enough to get lots of things done.
* Sparse rewards are difficult to learn: this is true, but the flip side is that
sparse rewards can have fewer spurious minima than shaped rewards. The Learning
from Human Preferences paper found that human-feedback actually outperformed the
given shaped rewards, possibly because the hand-designed reward didn't give
enough reward for partial progress. It's possible we can either hallucinate
positive rewards (HER), define auxiliary tasks (UNREAL), or do predicitive
learning to build a world model from transitions that give 0 reward.
* Requires a large amount of experience:
