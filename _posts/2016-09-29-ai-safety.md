---
layout: post
title:  "\"What's Your Opinion on AI Risk?\""
date:   2016-08-13 18:52:00 -0700
---

*These are my personal opinions, and do not reflect the opinions of
anybody I've worked with. AI risk is a touchy subject, so I want to
be very clear about this.*

I went to Effective Altruism Global this year, which was pretty great.
I wasn't sure I would fit into the effective altruist culture, since I
didn't count myself as part of EA, but it turns out I do fit in, and
EA attracts a lot of interesting people.

Now, I'm bad at starting conversation, but starting conversation at
EA Global is easy. All you have to do is ask what they do. And that
led to the following exchange, which I probably repeated about 10 times
over the entire weekend.

> Them: "What do you do?"
>
> Me: "Well, I just finished my undergrad at Berkeley. I'm now working
> at Google Brain, a machine learning research group."
>
> Them: "That's really cool! So, just wondering...(what's your opinion
> on the existential risk on AI/what's your timeline for AGI)?"


DROP THE ABOVE.


In the mindscape, an AI safety supporter and AI safety detractor meet
around a wooden table. Detractor pulls his arms forwards, and in one
smooth motion sweeps the stacks of paper and books onto the ground.

"Must you be so dramatic?"

"Oh be quiet. You've wanted to do that for ages. After all, we're just
aspects of the same person."

Supporter shrugs. "Fair. Why are we even framing this as a conversation?
Why can't we just write an essay, like a normal person?"

Detractor raises a hand, pointing an index finger towards the metaphorical
sky as he counts off the ways. "One. This isn't something I've figured
out yet. It's something I'm in the middle of figuring out. So, I need to
explain my contradictory feelings, and that's easier if I anthropomorphize
the two sides.

"Two. It's a writing challenge. Restrictions breed creativity. Forcing the
argument into a dialogue should learn to interesting results, and we need
the writing practice.

"And three? Well, that's a secret. But we both know the reason."

Supporter pauses for a bit, then shrugs. "Alright. Sounds fair. Let's do this.
We'll take turns. I'll start by laying out the argument, you reply, and we
see where it goes from there. Sound good?"

"Yep."

"Alright. Let's do this."


Part 1: Laying Out the Ground Rules
===========================================================

Let's be clear here.

When I say "artificial intelligence", I'm not looking to hear anything about
consciousness or Chinese room experiments. To me, artificial intelligence, I
mean "a computer system that performs a task."

Astute readers will note that this might as well mean nothing, because literally
every computer program performs a task. But I don't have a better definition.

This view further breaks down into Weak AI and Strong AI. Weak AI is a computer
system that solves a specific task at a near-human level or above human level.
Every task has a different weak AI, which may or may not exist in the present.
Addition and multiplication have a weak AI. There is a weak AI for recognizing handwritten
digits, and there is almost a weak AI for recognizing pictures of digits.
There is a weak AI for recognizing whether a picture is a dog or a cat, and
in fact, there is a weak AI for identifying the breed of a specific dog.
There "is" a weak AI for something if somebody has made it, ever. AlphaGo
is a weak AI for Go, but only a few companies could make AlphaGo.
There is a weak AI for translating Spanish to English, but not for German to
English.

Strong AI is a computer system that solves every task at a near-human or above
human level. This is more commonly known as artificial general intelligence,
or AGI. I use these interchangeably, but I'll try to stick to AGI.

Currently, AGI does not exist.

I see no reason AGI is fundamentally impossible.

My current timeline is <FILL IN TIMELINE HERE>


Part 2: Lowering the Goal Post
===========================================================

Detractor speaks up. "Point of order."

"Yes?"

"I already know the arguments for value alignment, and find them valid. But,
maybe we should go over them anyways? Just in case."

"Sure, as long as we tell the reader that if they know what value alignment
is, they can probably skip this section without missing much. We're going
to retread a lot of old ground here."

\*\*\*
{: .centered }

The common counterargument against the dangers of AI is that it's very far away,
and will be held in a sandbox environment that prevents it from influencing the
world.

The core argument in favor of treating AI safety as important is that sandboxing
a superintelligent AI is very difficult, and the bar an AI needs to pass to be
dangerous is lower than people think. To be specific:


For a moment, let's assume we don't just have AGI. Instead, we have superhuman
Strong AI - something that's enormously better than human at every task.
Then, let's assume that AI has decided to deliberately kill all humans.
Given all these constraints, we're pretty screwed. Here's a plan off the top of
my head for superhuman AI.

* Break into insecure computer systems. This lets you: collect more computing
power, collect personal information, gain access to any physical production
systems that are online.

(Actually, I really don't want to write this section now. Let's skip it.)

\*\*\*

"Okay. So to recap:

* If you assume that intelligence explosion is possible, and a reasonably intelligent
system can self-improve itself with unbounded gains,
* and you believe a superintelligent system is hard to sandbox,
* and you believe there's no reason such a system will share the same values
that humans do, and that misalignment of values in a superhuman AI is an
existential risk,
* and you believe solving value alignment is a difficult problem that will take
decades to solve,

then research into AI safety in the present is a reasonable use of
time/money/talent."

Supporter nods. "Right. I'm sure there are more qualifiers you can throw into
there, but the main argument is that there is some marginal probability
an AI that's good at self-improvement is game, there is some probability such
an AI will not be friendly to humanity, there is some probability that AI safety
research makes that more likely, and together, these add to marginally decreasing
the odds humanity wipes itself out. And if you believe preventing existential
risk is the best use of your time, then AI safety is a reasonable place to work."

Detractor nods back. "Okay. That's the argument. So how do we **actually**
feel about it? It's one thing to summarize another person's argument. It's another
to actually believe it."

In my case, I'm somewhat skeptical intelligence explosion is possible. It feels
like there should be limits to how much a system can self-improve itself by.
But, these are all gut feelings, with no logic to back them up, so I'm wiling to
accept there's a chance intelligence explosion is possible. Say, on the order of
10%.

Next, the difficulty of sandboxing. This is going to depend on how intelligent
the AI is, which is a GIANT can of worms I don't want to open, but assuming
a superintelligent system this is probably around 95%.

Next, value sharing. I have firsthand experience of value misalignment in my
work.


THE RANT PARAGRAPHS

This is the problem I have with the AI safety debate.

People in the AI safety camp make arguments for why they believe research on
AI safety is one of the best ways to do good in the world. Their conclusion
is weird, but after considering their arguments, you understand how they came
to that conclusion, and can accept it as a valid position to have.
As part of that, you understand that people in MIRI have these points of view.

* AGI isn't coming soon. In fact, it's several decades away, and there's a
chance it doesn't come at all.
* Given this, work on value alignment is still worth doing, because it's a hard
problem.
* Current work on AI is removed enough from AGI that no AI researcher should
actively stop their research.
* Although there is potential work on value alignment in real world systems
to prevent unintended behavior (like fairness), there are more economic
incentives for companies to work on those problems, so it makes sense for
MIRI to focus on theory that deliberately aims for far-future systems.

And, although I have objections to some of the final conclusions, these arguments
all sound reasonable enough.

(By "people in MIRI have these points of view", I really mean, "people from MIRI
have posted these opinions online." I'm sure there's a diversity of opinions
even within MIRI about what to do in AI safety.)

The point, is that this argument is much more nuanced and reasonable than the
strawman people usually use. The strawman goes something like, "AGI is coming
soon, and it's going to be awful without value alignment, and MIRI is one of the few
groups of people who even know this problem *exists*."

I call this a strawman, but people like this exist. I know because they've said
so online. Once, I saw somebody collect a few recent results from NLP, a few
results from computer vision, and conclude that AGI was happening in 2017 +/- 1 year.
They said, "I would like to talk to people who can convince me that AGI is not
happening in a few years." They also said, "I find conversations with people
who haven't read Superintelligence boring."

I wonder if it has ever occured to this person that if you only talk to people
who've read Superintelligence, you might, just might, be getting a biased sample
of the state of machine learning?


