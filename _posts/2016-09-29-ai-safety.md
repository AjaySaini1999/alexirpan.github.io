---
layout: post
title:  "\"What's Your Opinion on AI Risk?\""
date:   2016-08-13 18:52:00 -0700
---

*These are my personal opinions, and do not reflect the opinions of
anybody I've worked with. AI risk is a touchy subject, so I want to
be very clear about this.*

I went to Effective Altruism Global this year, which was pretty great.
I wasn't sure I would fit into the effective altruist culture, since I
didn't count myself as part of EA, but it turns out I do fit in, and
EA attracts a lot of interesting people.

Now, I'm bad at starting conversation, but starting conversation at
EA Global is easy. All you have to do is ask what they do. And that
led to the following exchange, which I probably repeated about 10 times
over the entire weekend.

> Them: "What do you do?"
>
> Me: "Well, I just finished my undergrad at Berkeley. I'm now working
> at Google Brain, a machine learning research group."
>
> Them: "That's really cool! So, just wondering...(what's your opinion
> on the existential risk on AI/what's your timeline for AGI)?"


DROP THE ABOVE.


In the mindscape, an AI safety supporter and AI safety detractor meet
around a wooden table. Detractor pulls his arms forwards, and in one
smooth motion sweeps the stacks of paper and books onto the ground.

"Must you be so dramatic?"

"Oh be quiet. You've wanted to do that for ages. After all, we're just
aspects of the same person."

Supporter shrugs. "Fair. Why are we even framing this as a conversation?
Why can't we just write an essay, like a normal person?"

Detractor raises a hand, pointing an index finger towards the metaphorical
sky as he counts off the ways. "One. This isn't something I've figured
out yet. It's something I'm in the middle of figuring out. So, I need to
explain my contradictory feelings, and that's easier if I anthropomorphize
the two sides.

"Two. It's a writing challenge. Restrictions breed creativity. Forcing the
argument into a dialogue should learn to interesting results, and we need
the writing practice.

"And three? Well, that's a secret. But we both know the reason."

Supporter pauses for a bit, then shrugs. "Alright. Sounds fair. Let's do this.
We'll take turns. I'll start by laying out the argument, you reply, and we
see where it goes from there. Sound good?"

"Yep."

"Alright. Let's do this."


Part 1: Laying Out the Ground Rules
===========================================================

Let's be clear here.

When I say "artificial intelligence", I'm not looking to hear anything about
consciousness or Chinese room experiments. To me, artificial intelligence, I
mean "a computer system that performs a task."

Astute readers will note that this might as well mean nothing, because literally
every computer program performs a task. But I don't have a better definition.

This view further breaks down into Weak AI and Strong AI. Weak AI is a computer
system that solves a specific task at a near-human level or above human level.
Every task has a different weak AI, which may or may not exist in the present.
Addition and multiplication have a weak AI. There is a weak AI for recognizing handwritten
digits, and there is almost a weak AI for recognizing pictures of digits.
There is a weak AI for recognizing whether a picture is a dog or a cat, and
in fact, there is a weak AI for identifying the breed of a specific dog.
There "is" a weak AI for something if somebody has made it, ever. AlphaGo
is a weak AI for Go, but only a few companies could make AlphaGo.
There is a weak AI for translating Spanish to English, but not for German to
English.

Strong AI is a computer system that solves every task at a near-human or above
human level. This is more commonly known as artificial general intelligence,
or AGI. I use these interchangeably, but I'll try to stick to AGI.

Currently, AGI does not exist.

I see no reason AGI is fundamentally impossible.

My current timeline is <FILL IN TIMELINE HERE>


Part 2: Lowering the Goal Post
===========================================================

Detractor speaks up. "Point of order."

"Yes?"

"I already know the arguments for value alignment, and find them valid. But,
maybe we should go over them anyways? Just in case."

"Sure, as long as we tell the reader that if they know what value alignment
is, they can probably skip this section without missing much. We're going
to retread a lot of old ground here."

\*\*\*
{: .centered }

There are a LOT of counterarguments against the dangers of AGI. If I tried to
address all the concerns, we'd be here all day. However, I do want to bring up
the two common misconceptions. Namely,

* A superhuman AI can't be dangerous because we can sandbox it away from the
Internet, preventing it from accessing the real world.
* We would never intentionally create an AI that would try to kill humans, and
we would never give a superhuman AI access to any kind of weapon.

The counterarguments can be summarized as

* A superhuman AI that optimizes a seemingly innocent utility function may not
share the same social norms as humans, leading to sociopathic behavior.
* A superhuman AI would be very difficult to keep contained, and would either
hack its way out of the sandbox or convince a human to let it out.

Before going into slightly more detail, I want to detail why these
counterarguments matter.

A common belief about AGI is that very few people would deliberately create
a malicious AI that tries to kill humans. Most people don't want to murder other
people [citation needed]. However, if you think the first counterargument is
true, then you think that there doesn't have to be any malicious intent to
create unfriendly AGI, and that makes it more likely that an AGI gets created.

As for the second, if you believe AGI could be hard to contain, then it makes
it more likely that a created AGI can influence the world.

**These are reasons for higher likelihood of unfriendly AI. These arguments
are not guaranteed to be true.** They're thought experiments, intended to shift
weight onto thinking of AGI as dangerous.

Now, for the arguments themselves. Again, I'm not going to go in-depth here,
and I'm also saving my opinion on these arguments for later. What I'm trying to
do here is *explain the top-level principles in favor of AI safety, as I
understand them.*

For the first point: the classical thought experiment is the paperclip optimizer.
A superhuman AI that tries to maximize the number of paperclips it produces may
decide that the best way to do so is to ensure its own survival. In the long
term, making several paper clips will b
In doing so,
it may decide to kill all other life. (People like to call this "disabling the
off switch." I don't like this phrasing because it reinforces misconceptions
about AI, but it's certainly evocative.)

For the second point: there have been a few AI-box experiments, where people
roleplay as an AI and a human through a chat interface, with the winner paying
money to the loser. The AI won a few times, so it's not out of the question.
(CITE A SOURCE HERE.) Additionally, computer security has historically
been biased towards offense, just because the attack surface is so big. Outside
of provably correct programs (whose proofs are themselves verified correct),
it's possible there are holes in the safeguards.


\*\*\*

"Okay. So to recap:

* If you believe a superintelligent system is hard to sandbox,
* and you believe such a system isn't guaranteed to share human values,
* and you believe that on our current trajectory, we aren't going to address
these issues in time, because they're hard,
* and you believe that intelligence explosion is a thing (which requires
us to address these problems now rather than later.)

then research into AI safety in the present is a reasonable use of
time/money/talent."

Supporter nods. "Well, I'm sure there are more qualifiers you can throw into
that bucket, but sure, works for me."

Detractor nods back. "Okay. That's the argument. So how do we **actually**
feel about it?"

\*\*\*

THE RANT PARAGRAPHS

This is the problem I have with the AI safety debate.

When I talk to people who work at MIRI in real-life, they are reasonable
people with reasonable beliefs. I feel like they are informed about neural
nets. Not at the level of an active researcher, but they know how they
work, at a level better than "approximates the brain." (Which by the way,
is a really, really annoying simplification that's been turned into a meme.
Neural nets were inspired by neurons, but they don't approximate the brain
 - not even close.)

Anyways, point being, I feel like they have good basis for their AGI timelines.

Surrounding the gorup of active AI safety researchers is a group of people
who follow news in AI safety. They're 

People in the AI safety camp make arguments for why they believe research on
AI safety is one of the best ways to do good in the world. Their conclusion
is weird, but after considering their arguments, you understand how they came
to that conclusion, and can accept it as a valid position to have.
As part of that, you understand that people in MIRI have these points of view.

* AGI isn't coming soon. In fact, it's several decades away, and there's a
chance it doesn't come at all.
* Given this, work on value alignment is still worth doing, because it's a hard
problem.
* Current work on AI is removed enough from AGI that no AI researcher should
actively stop their research.
* Although there is potential work on value alignment in real world systems
to prevent unintended behavior (like fairness), there are more economic
incentives for companies to work on those problems, so it makes sense for
MIRI to focus on theory that deliberately aims for far-future systems.

And, although I have objections to some of the final conclusions, these arguments
all sound reasonable enough.

(By "people in MIRI have these points of view", I really mean, "people from MIRI
have posted these opinions online." I'm sure there's a diversity of opinions
even within MIRI about what to do in AI safety.)

The point, is that this argument is much more nuanced and reasonable than the
strawman people usually use. The strawman goes something like, "AGI is coming
soon, and it's going to be awful without value alignment, and MIRI is one of the few
groups of people who even know this problem *exists*."

I call this a strawman, but people like this exist. I know because they've said
so online. Once, I saw somebody collect a few recent results from NLP, a few
results from computer vision, and conclude that AGI was happening in 2017 +/- 1 year.
They said, "I would like to talk to people who can convince me that AGI is not
happening in a few years." They also said, "I'm not interested in talking to
people who haven't read Superintelligence. I find those conversations boring,
we end up starting at the same topics."

I wonder if it has ever occured to this person that if you only talk to people
who've read Superintelligence, you might, just might, be getting a biased sample
of AI opinions.

Don't get me wrong. As anyone who's ever tried to explain a technical topic
knows, it can be incredibly tiring to explain things you've explained several
times before. Why can't they just *know* what you're going to explain?
Why don't they just *understand* that we're not talking about Skynet, we're
talking about paperclip optimizers?

The problem is that unlike math, these arguments are just opinions. They
are well reasoned, but that doesn't make them right. They may only be well
reasoned from your point of view.

It's okay to put up behavioral filters that stop your from hearing opinions
from people you don't want to listen to. It's uncomfortable to admit it, but
people do this all the time. If you refuse to listen to skinheads, then you
aren't getting the whole picture on white supremecists.

The issue is when people don't acknowledge their biased opinions. That's when
there's a slide from weird-but-valid opinion to zealotry.

Now, before people start raising pitchforks at me, I'm going to apply a good
rule of thumb - apply the same criticism to your position, and see what happens.

If there's a strawman for the overly eager AI safety promoter, there's one
for the overly dour AI safety detractor. Some machine learning researchers
said, once, that AI safety research is dumb and the worries about AGI are
too alarmist. Therefore, all arguments for AI safety are coming from uninformed
people who are looking for a silicon god to magically solve all problems in the
world instead of trying to make the world better, now.

My guess is that there are a small number of people who match the strawman
promoter description, a small number of people who match the strawman detractor
description, and then a bigger group of people trying to ally with the position
closest to their own. Nick Bostrom, Andrew Ng, and Stuart Russell get namedropped
so much it might as well be a meme in AI safety discussions on the Internet.

It just...it makes me sad, that whenever I see AI safety debates on Hacker News,
I can predict the topics people bring up with annoying accuracy, and it crowds
out useful discussion, and it makes people view the other side as a caricature
instead of a person. Expecting people to have good arguments about anything on
the Internet is wishful thinking, but I was hoping we could actually get there
for AI safety.

"What, do you think you're Alai? What gives you the right to be the bridge, or
to even try to be one?"

Well, I've done research into neural nets and reinforcement learning. I've
worked with neural net researchers, and I've also talked with other AI safety
researchers. I'm a bit new to all this too, but that doesn't stop people from
massive theorizing.

(The above is a shitty paragraph. Kill it with fire!)


> I think people have this misconception about AI research. They think we're
> in a nice room, standing in front of a whiteboard, with some indoor plants,
> discussing the future of humanity. The vast majority of AI research is dirty
> work.

Let me outline the typical day for a researcher in deep reinforcement learning.

You're trying to reproduce the results from a recent paper. It's not going that
well. RL is a notoriously unstable domain. The greatest advance of
DeepMind's Nature paper about Atari games was all the tricks they used to make
training a DQN stable.

After two weeks of work, your code stops crashing. You spend two more weeks
trying to figure out why nothing trains, until you find all the stupid bugs
and finally get something working. You run the experiment with the exact same
settings 10 times, and get wildly different results each time, because the training
curve for RL is almost entirely decided by whether the policy explores the
right region of the state space, which is also entirely up to luck.
In one random run, the code fails to learn anything. In another run, the code
learns near perfectly. In any other ML domain, this would be insane, but not
reinforcement learning. A paper says "we did 50 runs, and graphed the top 5 runs",
and it wins Best Paper Award, because sometimes that's just what you have to do.


Anyways, ranting aside, after two months of work, you try to teach a simulated
toy robot to run forward, and it does this instead.

GIF

That's actually an accoplishment, compared to what it did a month ago.

GIF

Imagine throwing millions and millions of timesteps at the problem, still failing
to do anything at all, and then coming home to read *yet another* Hacker News
discussion where people talk about AGI. Or another discussion where people
think AGI is just five years away. And you just want to yell at them.
"WE'RE NOT THERE YET! SAMPLE EFFICIENCY IS HORRIBLE! Simple MDPs need exponentially
many samples without good exploration, and people have yet to develop generalizable
exploration policies. Stability of RL is horrible!
Factorized representation of the world is both necessary and completely nascent!
Go and Atari only work so well *because* they're incredibly easy to simulate.
Long term dependencies are a struggle, curiosity-driven exploration still isn't
there yet..." All the bottled up frustrations, about just how *hard* some of these
problems are, they roil inside, and soon you build up a disdain for AI hype
of any kind.

I feel there's a common trap when it comes to debates. A person reads one
very good argument for why X is true, written by somebody they trust. Over time,
they tend to believe things their friend group believes, and then immediately
discounts arguments that don't line up with those beliefs. As if intelligence
is a zero-sum game. As if other communities aren't allowed to be smart, if your
community is smart.

It's obviously false. The beautiful, beautiful thing about intelligence is that
it isn't zero-sum. If two smart people walk into a room and have a good
conversation, two smarter people walk out. Of course that's true! Then I catch
myself calling an argument stupid and uninformed, without stopping to think
why I think that way.

At a rational level, I'm sure people understand that two smart people can think
deeply about a problem and come to radically different conclusions. But I think
we don't internalize that enough, and it's easy to assume that a claim that
goes against What Your Hero Says **can't** be well founded.

Yes, we. We as in me, and you, and world leaders and professors and people
shouting in the streets. I'd be very hesistant to believe the words of anyone
who's said they've transcended this flaw. Luckily, I don't know anyone who
claims this about themselves, in a way that doesn't make them sound incredibly
pretentious.
