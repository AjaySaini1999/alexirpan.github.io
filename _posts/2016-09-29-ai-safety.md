---
layout: post
title:  "\"What's Your Opinion on AI Risk?\""
date:   2016-08-13 18:52:00 -0700
---

*These are my personal opinions, and do not reflect the opinions of
anybody I've worked with. AI risk is a touchy subject, so I want to
be very clear about this.*

I went to Effective Altruism Global this year, which was pretty great.
I wasn't sure I would fit into the effective altruist culture, since I
didn't count myself as part of EA, but it turns out I do fit in, and
EA attracts a lot of interesting people.

Now, I'm bad at starting conversation, but starting conversation at
EA Global is easy. All you have to do is ask what they do. And that
led to the following exchange, which I probably repeated about 10 times
over the entire weekend.

> Them: "What do you do?"
>
> Me: "Well, I just finished my undergrad at Berkeley. I'm now working
> at Google Brain, a machine learning research group."
>
> Them: "That's really cool! So, just wondering...(what's your opinion
> on the existential risk on AI/what's your timeline for AGI)?"


DROP THE ABOVE.


In the mindscape, an AI safety supporter and AI safety detractor meet
around a wooden table. Detractor pulls his arms forwards, and in one
smooth motion sweeps the stacks of paper and books onto the ground.

"Must you be so dramatic?"

"Oh be quiet. You've wanted to do that for ages. After all, we're just
aspects of the same person."

Supporter shrugs. "Fair. Why are we even framing this as a conversation?
Why can't we just write an essay, like a normal person?"

Detractor raises a hand, pointing an index finger towards the metaphorical
sky as he counts off the ways. "One. This isn't something I've figured
out yet. It's something I'm in the middle of figuring out. So, I need to
explain my contradictory feelings, and that's easier if I anthropomorphize
the two sides.

"Two. It's a writing challenge. Restrictions breed creativity. Forcing the
argument into a dialogue should learn to interesting results, and we need
the writing practice.

"And three? Well, that's a secret. But we both know the reason."

Supporter pauses for a bit, then shrugs. "Alright. Sounds fair. Let's do this.
We'll take turns. I'll start by laying out the argument, you reply, and we
see where it goes from there. Sound good?"

"Yep."

"Alright. Let's do this."


Part 1: Laying Out the Ground Rules
===========================================================

Let's be clear here.

When I say "artificial intelligence", I'm not looking to hear anything about
consciousness or Chinese room experiments. To me, artificial intelligence, I
mean "a computer system that performs a task."

Astute readers will note that this might as well mean nothing, because literally
every computer program performs a task. But I don't have a better definition.

This view further breaks down into Weak AI and Strong AI. Weak AI is a computer
system that solves a specific task at a near-human level or above human level.
Every task has a different weak AI, which may or may not exist in the present.
Addition and multiplication have a weak AI. There is a weak AI for recognizing handwritten
digits, and there is almost a weak AI for recognizing pictures of digits.
There is a weak AI for recognizing whether a picture is a dog or a cat, and
in fact, there is a weak AI for identifying the breed of a specific dog.
There "is" a weak AI for something if somebody has made it, ever. AlphaGo
is a weak AI for Go, but only a few companies could make AlphaGo.
There is a weak AI for translating Spanish to English, but not for German to
English.

Strong AI is a computer system that solves every task at a near-human or above
human level. This is more commonly known as artificial general intelligence,
or AGI. I use these interchangeably, but I'll try to stick to AGI.

Currently, AGI does not exist.

I see no reason AGI is fundamentally impossible.

My current timeline is <FILL IN TIMELINE HERE>


Part 2: Lowering the Goal Post
===========================================================

Detractor speaks up. "Point of order."

"Yes?"

"I already know the arguments for value alignment, and find them valid. But,
maybe we should go over them anyways? Just in case."

"Sure, as long as we tell the reader that if they know what value alignment
means, they can probably skip this section without missing much. We're going
to retread a lot of old ground here."

\*\*\*
{: .centered }
