---
layout: post
title:  "\"What's Your Opinion on AI Risk?\""
date:   2016-08-13 18:52:00 -0700
---

*These are my personal opinions, and do not reflect the opinions of
anybody I've worked with. AI risk is a touchy subject, so I want to
be very clear about this.*

I went to Effective Altruism Global this year, which was pretty great.
I wasn't sure I would fit into the effective altruist culture, since I
didn't count myself as part of EA, but it turns out I do fit in, and
EA attracts a lot of interesting people.

Now, I'm bad at starting conversation, but starting conversation at
EA Global is easy. All you have to do is ask what they do. And that
led to the following exchange, which I probably repeated about 10 times
over the entire weekend.

> Them: "What do you do?"
>
> Me: "Well, I just finished my undergrad at Berkeley. I'm now working
> at Google Brain, a machine learning research group."
>
> Them: "That's really cool! So, just wondering...(what's your opinion
> on the existential risk on AI/what's your timeline for AGI)?"


DROP THE ABOVE.


In the mindscape, an AI safety supporter and AI safety detractor meet
around a wooden table. Detractor pulls his arms forwards, and in one
smooth motion sweeps the stacks of paper and books onto the ground.

"Must you be so dramatic?"

"Oh be quiet. You've wanted to do that for ages. After all, we're just
aspects of the same person."

Supporter shrugs. "Fair. Why are we even framing this as a conversation?
Why can't we just write an essay, like a normal person?"

Detractor raises a hand, pointing an index finger towards the metaphorical
sky as he counts off the ways. "One. This isn't something I've figured
out yet. It's something I'm in the middle of figuring out. So, I need to
explain my contradictory feelings, and that's easier if I anthropomorphize
the two sides.

"Two. It's a writing challenge. Restrictions breed creativity. Forcing the
argument into a dialogue should learn to interesting results, and we need
the writing practice.

"And three? Well, that's a secret. But we both know the reason."

Supporter pauses for a bit, then shrugs. "Alright. Sounds fair. Let's do this.
We'll take turns. I'll start by laying out the argument, you reply, and we
see where it goes from there. Sound good?"

"Yep."

"Alright. Let's do this."


Part 1: Laying Out the Ground Rules
===========================================================

Let's be clear here.

When I say "artificial intelligence", I'm not looking to hear anything about
consciousness or Chinese room experiments. To me, artificial intelligence, I
mean "a computer system that performs a task."

Astute readers will note that this might as well mean nothing, because literally
every computer program performs a task. But I don't have a better definition.

This view further breaks down into Weak AI and Strong AI. Weak AI is a computer
system that solves a specific task at a near-human level or above human level.
Every task has a different weak AI, which may or may not exist in the present.
Addition and multiplication have a weak AI. There is a weak AI for recognizing handwritten
digits, and there is almost a weak AI for recognizing pictures of digits.
There is a weak AI for recognizing whether a picture is a dog or a cat, and
in fact, there is a weak AI for identifying the breed of a specific dog.
There "is" a weak AI for something if somebody has made it, ever. AlphaGo
is a weak AI for Go, but only a few companies could make AlphaGo.
There is a weak AI for translating Spanish to English, but not for German to
English.

Strong AI is a computer system that solves every task at a near-human or above
human level. This is more commonly known as artificial general intelligence,
or AGI. I use these interchangeably, but I'll try to stick to AGI.

Currently, AGI does not exist.

I see no reason AGI is fundamentally impossible.

My current timeline is <FILL IN TIMELINE HERE>


Part 2: Lowering the Goal Post
===========================================================

Detractor speaks up. "Point of order."

"Yes?"

"I already know the arguments for value alignment, and find them valid. But,
maybe we should go over them anyways? Just in case."

"Sure, as long as we tell the reader that if they know what value alignment
is, they can probably skip this section without missing much. We're going
to retread a lot of old ground here."

\*\*\*
{: .centered }

There are a LOT of counterarguments against the dangers of AGI. If I tried to
address all the concerns, we'd be here all day. However, I do want to bring up
the two common misconceptions. Namely,

* A superhuman AI can't be dangerous because we can "sandbox" it away from the
Internet, preventing it from accessing the real world.
* We would never intentionally create an AI that would try to kill humans, and
we would never give a superhuman AI access to any kind of weapon.

The counterarguments can be summarized as

* A superhuman AI that optimizes a seemingly innocent utility function may kill humans
*incidentally*.
* It's very difficult to sandbox something smarter than you, and we shouldn't
expect any guarantees that we could sandbox superhuman AI.

Before talking about the rebuttal to these claims, I want to point


The core argument in favor of treating AI safety as important is that sandboxing
a superintelligent AI is very difficult, and the bar an AI needs to pass to be
dangerous is lower than people think. To be specific:

\*\*\*

"Okay. So to recap:

* If you assume that intelligence explosion is possible, and a reasonably intelligent
system can self-improve itself with unbounded gains,
* and you believe a superintelligent system is hard to sandbox,
* and you believe there's no reason such a system will share the same values
that humans do, and that misalignment of values in a superhuman AI is an
existential risk,
* and you believe solving value alignment is a difficult problem that will take
decades to solve,

then research into AI safety in the present is a reasonable use of
time/money/talent."

Supporter nods. "Right. I'm sure there are more qualifiers you can throw into
there, but the main argument is that there is some marginal probability
an AI that's good at self-improvement is game, there is some probability such
an AI will not be friendly to humanity, there is some probability that AI safety
research makes that more likely, and together, these add to marginally decreasing
the odds humanity wipes itself out. And if you believe preventing existential
risk is the best use of your time, then AI safety is a reasonable place to work."

Detractor nods back. "Okay. That's the argument. So how do we **actually**
feel about it? It's one thing to summarize another person's argument. It's another
to actually believe it."

In my case, I'm somewhat skeptical intelligence explosion is possible. It feels
like there should be limits to how much a system can self-improve itself by.
But, these are all gut feelings, with no logic to back them up, so I'm wiling to
accept there's a chance intelligence explosion is possible. Say, on the order of
10%.

Next, the difficulty of sandboxing. This is going to depend on how intelligent
the AI is, which is a GIANT can of worms I don't want to open, but assuming
a superintelligent system this is probably around 95%.

Next, value sharing. I have firsthand experience of value misalignment in my
work.


THE RANT PARAGRAPHS

This is the problem I have with the AI safety debate.

People in the AI safety camp make arguments for why they believe research on
AI safety is one of the best ways to do good in the world. Their conclusion
is weird, but after considering their arguments, you understand how they came
to that conclusion, and can accept it as a valid position to have.
As part of that, you understand that people in MIRI have these points of view.

* AGI isn't coming soon. In fact, it's several decades away, and there's a
chance it doesn't come at all.
* Given this, work on value alignment is still worth doing, because it's a hard
problem.
* Current work on AI is removed enough from AGI that no AI researcher should
actively stop their research.
* Although there is potential work on value alignment in real world systems
to prevent unintended behavior (like fairness), there are more economic
incentives for companies to work on those problems, so it makes sense for
MIRI to focus on theory that deliberately aims for far-future systems.

And, although I have objections to some of the final conclusions, these arguments
all sound reasonable enough.

(By "people in MIRI have these points of view", I really mean, "people from MIRI
have posted these opinions online." I'm sure there's a diversity of opinions
even within MIRI about what to do in AI safety.)

The point, is that this argument is much more nuanced and reasonable than the
strawman people usually use. The strawman goes something like, "AGI is coming
soon, and it's going to be awful without value alignment, and MIRI is one of the few
groups of people who even know this problem *exists*."

I call this a strawman, but people like this exist. I know because they've said
so online. Once, I saw somebody collect a few recent results from NLP, a few
results from computer vision, and conclude that AGI was happening in 2017 +/- 1 year.
They said, "I would like to talk to people who can convince me that AGI is not
happening in a few years." They also said, "I'm not interested in talking to
people who haven't read Superintelligence. I find those conversations boring,
we end up starting at the same topics."

I wonder if it has ever occured to this person that if you only talk to people
who've read Superintelligence, you might, just might, be getting a biased sample
of AI opinions.

Don't get me wrong. As anyone who's ever tried to explain a technical topic
knows, it can be incredibly tiring to explain things you've explained several
times before. Why can't they just *know* what you're going to explain?
Why don't they just *understand* that we're not talking about Skynet, we're
talking about paperclip optimizers?

The problem is that unlike math, these arguments are just opinions. They
are well reasoned, but that doesn't make them right. They may only be well
reasoned from your point of view.

It's okay to put up behavioral filters that stop your from hearing opinions
from people you don't want to listen to. It's uncomfortable to admit it, but
people do this all the time. If you refuse to listen to skinheads, then you
aren't getting the whole picture on white supremecists.

The issue is when people don't acknowledge their biased opinions. That's when
there's a slide from weird-but-valid opinion to zealotry.

Now, before people start raising pitchforks at me, I'm going to apply a good
rule of thumb - apply the same criticism to your position, and see what happens.

If there's a strawman for the overly eager AI safety promoter, there's one
for the overly dour AI safety detractor. Some machine learning researchers
said, once, that AI safety research is dumb and the worries about AGI are
too alarmist. Therefore, all arguments for AI safety are coming from uninformed
people who are looking for a silicon god to magically solve all problems in the
world instead of trying to make the world better, now.

My guess is that there are a small number of people who match the strawman
promoter description, a small number of people who match the strawman detractor
description, and then a bigger group of people trying to ally with the position
closest to their own. Nick Bostrom, Andrew Ng, and Stuart Russell get namedropped
so much it might as well be a meme in AI safety discussions on the Internet.

It just...it makes me sad, that whenever I see AI safety debates on Hacker News,
I can predict the topics people bring up with annoying accuracy, and it crowds
out useful discussion, and it makes people view the other side as a caricature
instead of a person. Expecting people to have good arguments about anything on
the Internet is wishful thinking, but I was hoping we could actually get there
for AI safety.

"What, do you think you're Alai? What gives you the right to be the bridge, or
to even try to be one?"

Well, I've done research into neural nets and reinforcement learning. I've
worked with neural net researchers, and I've also talked with other AI safety
researchers. I'm a bit new to all this too, but that doesn't stop people from
massive theorizing.

(The above is a shitty paragraph. Kill it with fire!)


> I think people have this misconception about AI research. They think we're
> in a nice room, standing in front of a whiteboard, with some indoor plants,
> discussing the future of humanity. The vast majority of AI research is dirty
> work.

Let me outline the typical day for a researcher in deep reinforcement learning.

You're trying to reproduce the results from a recent paper. It's not going that
well. RL is a notoriously unstable domain. The greatest advance of
DeepMind's Nature paper about Atari games was all the tricks they used to make
training a DQN stable.

After two weeks of work, your code stops crashing. You spend two more weeks
trying to figure out why nothing trains, until you find all the stupid bugs
and finally get something working. You run the experiment with the exact same
settings 10 times, and get wildly different results each time, because the training
curve for RL is almost entirely decided by whether the policy explores the
right region of the state space, which is also entirely up to luck.
In one random run, the code fails to learn anything. In another run, the code
learns near perfectly. In any other ML domain, this would be insane, but not
reinforcement learning. A paper says "we did 50 runs, and graphed the top 5 runs",
and it wins Best Paper Award, because sometimes that's just what you have to do.


Anyways, ranting aside, after two months of work, you try to teach a simulated
toy robot to run forward, and it does this instead.

GIF

That's actually an accoplishment, compared to what it did a month ago.

GIF

Imagine throwing millions and millions of timesteps at the problem, still failing
to do anything at all, and then coming home to read *yet another* Hacker News
discussion where people talk about AGI. Or another discussion where people
think AGI is just five years away. And you just want to yell at them.
"WE'RE NOT THERE YET! SAMPLE EFFICIENCY IS HORRIBLE! Stability of RL is horrible!
Factorized representation of the world is both necessary and completely nascent!
Go and Atari only work so well *because* they're incredibly easy to simulate.
Long term dependencies are a struggle, curiosity-driven exploration still isn't
there yet..." All the bottled up frustrations, about just how *hard* some of these
problems are, they roil inside, and soon you build up a disdain for AI hype
of any kind.

