---
layout: post
title:  "Read-through: Hyperparameter Optimization: A Spectral Approach"
date:   2017-06-18 00:52:00 -0700
---

Similar to Wasserstein GAN, this is another theory-motivated paper with neat
applications to deep learning. Once again, the paper has a lot of theory, but
if you allow for some looseness in rigor, the core idea isn't too difficult
to implement.

This, by the way, is the general trend with many theory papers - the
algorithm isn't that bad, but the runtime proof is complicated. If you're looking
to do theory, you should read the paper. If, however, you're just looking to
apply the method, keep reading.

Why Is This Paper Important?
----------------------------------------------------------------------

I really, really don't like hyperparam optimization. Nobody does. It's grunt
work that's required to get the best results, and it has to be done, but that
doesn't mean I like doing it.

Because of this, I try to keep an eye on the hyperparam optimization space.
For one, anything that lets me spend less time on hyperparam optimization makes
me happy, because it means I can get to the parts of research I actually like.
But also, I think


Follow-Up Questions
------------------------------------------------------------------------------
