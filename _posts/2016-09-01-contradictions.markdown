---
layout: post
title:  "Prioritized Contradiction Resolution"
date:   2016-09-01 00:10:00 -0700
---

I know I said my next post was going to be my position on the existential risk
of AI, but I love metaposts too much. I'm going to explain why I'm writing that
post, through the lens of how I view self-consistency. Said lens is probably
obvious to anyone in my friend group who'll read this, but oh well.

At a party several months ago, I got into a discussion with Alice and Bob
about effective altruism. (Fake names, for privacy reasons.)
Alice self-identified as EA, and Bob didn't. At one point, Bob said he approved
of EA, and agreed with the arguments for EA, but had yet to donate. He didn't
see how to resolve the arguments in a way other than "donate all disposable
income and live a minimal life, because your marginal utility for money is
smaller than the majority of the world", so he didn't donate at all.

Alice replied like so.

> Alice: I agree that's the conclusion you end at, but that doesn't mean you
> have to follow it. I think aiming for perfect self-consistency is a pipe dream.
> For EA in particular, you can try to live up to that conclusion by donating
> some money, which is better than none. That being said, you should only be
> donating if it's something you really want to do. It's more sustainable that
> way.

In short, [the perfect is the enemy of the good](https://en.wikipedia.org/wiki/Perfect_is_the_enemy_of_good).
It's okay, and even laudable, to try towards a good enough solution, instead of
hoping for a perfect solution.

I think acting in a 
If you're like me, you want to act self-consistently.
By doing so, you sometimes arrive to very strange
conclusions. Sometimes, that's okay. Strange beliefs aren't necessarily bad.
However, it's often the case that you don't want to act according to your
conclusions. Assuming you believe in diminishing returns of money, it's true
that giving away the majority of your wealth is better for the world, but there
are things like beds and nice food and laptops that you'd like to have in your
life.

If you believe very strongly that you should follow valid chains of reasoning,
and then produce a chain of reasoning that leads to something you **don't** want
to do, it can feel pretty awful. Because if you do it, you're doing something
you don't want to do, and if you don't, you're violating a personal creed.

I think it's entirely okay to have both these beliefs.

Like, okay, let's be real. Brains are garbage. They're incredible at a lot of
things, but they're also garbage. I've heard enough stories about distorted
perception and cognitive biases to make my faith in my brain extremely low.
Someone recently tried to convince me that my brain could be lying to me without
my knowledge, because my knowledge is dependent on my brain, and it didn't
really faze me. We're already getting lied to when the chemical reactions
for eyes and touch sync themselves up despite operating on different timescales.
At that point, all bets are off.

A few days ago, someone pointed

From this viewpoint, having contradictions in your brain is entirely expected.
Interestingly, this isn't a hopeless problem. If you put enough effort in, you
can take the implicit thoughts and make them explicit, working at contradictions
until they go away.

