---
layout: post
title:  "Prioritized Contradiction Resolution"
date:   2016-09-01 00:10:00 -0700
---

I know I said my next post was going to be my position on the existential risk
of AI, but I love metaposts too much. I'm going to explain why I'm writing that
post, through the lens of how I view self-consistency. Said lens is probably
obvious to anyone in my friend group who'll read this, but oh well.

At a party several months ago, I got into a discussion with Alice and Bob
about effective altruism. (Fake names, for privacy reasons.)
Alice self-identified as EA, and Bob didn't. At one point, Bob said he approved
of EA, and agreed with the arguments for EA, but had yet to donate. He didn't
see how to resolve the arguments in a way other than "donate all disposable
income and live a minimal life, because your marginal utility for money is
smaller than the majority of the world", so he didn't donate at all.

Alice replied like so.

> Alice: I agree that's the conclusion you end at, but that doesn't mean you
> have to follow it. I think aiming for perfect self-consistency is a pipe dream.
> For EA in particular, you can try to live up to that conclusion by donating
> some money, which is better than none. That being said, you should only be
> donating if it's something you really want to do. It's more sustainable that
> way.

In short, [the perfect is the enemy of the good](https://en.wikipedia.org/wiki/Perfect_is_the_enemy_of_good).
It's okay, and even laudable, to try towards a good enough solution, instead of
hoping for a perfect solution.

If you're like me, you want to have consistent beliefs that lead to consistent
actions. If you're like me, you don't actually have consistent beliefs. Instead
you have inconsistent beliefs and inconsistent actions.

This is totally okay and entirely expected.

Let me be frank. Brains are astoundingly useful, but they are also complete
garbage. Our intuitive understanding of probability is good at making quick
decisions, but awful at [solving even simple probability questions](https://en.wikipedia.org/wiki/Monty_Hall_problem).
We double check information that challenges our ideas, but [don't double
check information that confirms them](https://en.wikipedia.org/wiki/Confirmation_bias).
And if someone else does try to challenges our world view, we
[can double down on our beliefs instead of moving away from them](https://youarenotsosmart.com/2011/06/10/the-backfire-effect/),
flying in the face of any reasonble model of intelligent thought.

![Dinosaur Comics](/public/contradictions/dinosaur.png)
{: .centered }

([Dinosaur Comics](http://www.qwantz.com/index.php?comic=1806))
{: .centered }

We're getting lied to on a subconscious level. Lying to ourselves on a
conscious level isn't much different.

Recently, someone tried to convince me I could theoretically be schizophrenic
and not even know it, and that in fact many schizophrenics do not believe they
are schizophrenic. I think the most surprising thing about it was that I wasn't
phased at all. Someone, I've read about cognitive biases and
perceptual biases for so long that I've gained a deep-seated mistrust of my
own ability to know the truth.

That doesn't mean I don't try to seek the truth. All I've got is my brain. It's
not like I have a choice.
All it means is that I've accepted that using an imperfect reasoning
engine to reason about the world is an amusing comedy, and we just have to
deal with it.

\*\*\*

Well. Actually, it's not hopeless. Brains can be self-consistent too.

The trick is that you find a nice, quiet place, and sit down to think. You let
the thoughts come, sometimes consciously, sometimes subconsciously, and wait
until you understand the implicit thoughts well enough to say them out loud.

Writing helps with this. When you write, you're forced to try words, again and
again, until you hit upon the one with perfect weight. Rinse, lather, repeat.

The only problem is that this takes *forever*, especially on complicated topics.
It doesn't matter how many contradictions you resolve, more will pop up.

Hence, prioritized contradiction resolution. Here's how my reasoning goes.

* I have contradictory thoughts.
* I have finite time and motivation.
* It costs time and motivation to resolve contradictions.
* Therefore, I should focus on fixing the contradictions most important to me.

It makes sense for me to think deeply about the AI safety debate, because I do
research in machine learning. Not having a well thought out opinion would feel
irresponsible.

In contrast, I don't need to have well thought out reasons to like My Little Pony.
I'd like to take the time to tease apart why I like the fandom so much, but
the value I'd gain from figuring that out doesn't seem as large. Thus,
it's okay for me to put those contradictions on my internal todo pile.
