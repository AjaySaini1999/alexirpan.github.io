---
layout: post
title:  "Deep Reinforcement Learning Doesn't Work Yet"
date:   2017-12-11 23:06:00 -0700
---


*This mostly cites papers from Berkeley, Google Brain, DeepMind, and OpenAI
from the past few years, because I'm most familiar with that work.
I'm almost certainly missing stuff from older literature and less famous
institutions, and for that I apologize - I'm just one guy, after all.*

Once, on Facebook, I made the following claim.

> Whenever someone asks me if reinforcement learning can solve their problem, I tell them it can't. I think this is right at least 70% of the time.

![Futurama Bender meme](/public/rl-hard/bender-70.jpg)
{: .centered }

Deep reinforcement learning is driven by mountains and mountains of hype. And
for good reason!
Reinforcement learning is an incredibly general paradigm,
and in principle, a robust and performant RL system should be great at
everything. Merging this paradigm with the empirical power of deep learning
is an obvious fit. Deep RL is one of the closest things that looks anything like
AGI, and that's the kind of dream that fuels billions
of dollars of funding.

Unfortunately, it doesn't really work yet.

Now, I believe it *can* work. If I didn't believe in reinforcement learning,
I wouldn't be interested in it.
But there are a lot of problems in the way, many of which feel fundamentally
difficult. Hidden behind the beautiful demos of learned agents is a
towering garbage pile of pain, failed experiments, statistically
insignificant results, and incredibly nonsensical behaviors.

Several times, I've seen people try deep reinforcement learning for the first
time. Without fail, something goes wrong. Without fail, the "toy problem" is
not as easy as it looks. And without fail, they get destroyed a few times,
until they learn how to set realistic research expectations.

My goal isn't to claim that RL is useless, or to discourage people from joining
the field. I just want people to know what they're getting into, so that when
they fail to get positive results for a month, they know it's not necessarily
their fault.

(A final note: in the rest of this post, I use RL and deep RL interchangeably.
I'd like to clarify that I am criticizing just deep RL in particular. These
observations are based on papers and personal observations from the last 5 years,
almost all of which use deep neural nets. The criticisms may apply to linear
RL or tabular RL, but I'm not confident enough to claim the criticisms
generalize.
The hype of RL is driven by the promise of applying RL to large, complex,
high-dimensional environments where good function approximation is necessary.
It is that hype in particular that needs to be addressed.)


Reinforcement Learning Can Be Horribly Sample Inefficient
==============================================================================

The most well-known benchmark for deep reinforcement learning is Atari. This is
the environment that put DeepMind on the map. Combine Q-Learning with a
reasonably sized neural networks, and several (but not all) of the games get
solved to human or superhuman performance.

Atari games run at 60 frames per second. On the top of your head, do you
know how many frames a state of the art DQN needs to reach human performance?

The answer depends on the game, but let's take a look at a recent Deepmind
paper, [Rainbow DQN (Hessel et al, 2017)](https://arxiv.org/abs/1710.02298).
This paper is the combination
of the several incremental advances made to DQN in the past 5 years. It exceeds
human-level performance on over 40 of the 57 Atari games attempted.
The results come in this handy chart.

![Figure from Rainbow DQN](/public/rl-hard/rainbow_dqn.png)
{: .centered }

The y-axis is "median human-normalized score". This is computed by training
57 DQNs, one for each Atari game, normalizing the score of each agent such that
human performance is 100%, then plotting the median performance across the
57 games. RainbowDQN passes the 100% threshold at about 18 million frames.
This corresponds to about 83 hours of play experience, plus however long it takes
to train the model.

This is actually pretty good, when you consider that the previous record
([Distributional DQN (Bellemare et al, 2017)](https://arxiv.org/pdf/1707.06887.pdf)) needed
*70 million* frames to hit 100% median performance. As for the
[DQN from the Nature paper (Mnih et al, 2015)](https://www.nature.com/articles/nature14236),
it never hits 100% median performance, even after 200 million frames of training.

You know the planning fallacy? The one that says finishing something usually
takes longer than you think it will? I'm starting to believe reinforcement
learning has a similar rule of thumb - it usually takes more samples than you
think it will. Many of the MuJoCo benchmarks take between $$10^5$$ to $$10^7$$ steps,
depending on the task.
The [DeepMind parkour paper (Heess et al, 2017)](https://arxiv.org/abs/1707.02286), demoed below,
used 64 workers over 100 hours. (The paper does not clarify what "worker"
means, but I assume it means CPUs.)

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/hx_bgoTF7bs" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

These results are *super cool*, and when it first came out I was really excited
that deep RL could learn running behaviors like this...but it still feels like
these behaviors shouldn't need this much time to learn, nor this much
computation.

There's an obvious counterpoint here: what if we just ignore sample efficiency?
There are several settings where it's easy to generate experience. Games are
a big example. But, for any setting where this *isn't* true, RL faces an uphill
battle, and unfortunately, most real-world settings fall under this category.


If You Just Care About Final Performance, Many Problems are Better Solved by Other Methods
==============================================================================

This section is directed towards people who apply deep RL, rather than people
who directly do research on it.

When searching for solutions to any research problem, there are usually
trade-offs between different objectives. You can optimize for getting a really
good solution for that research problem, or you can optimize for making a good
research contribution. The best problems are ones where getting a good solution
*requires* making good research contributions, but it can be hard to find
approachable problems of that kind.

From the perspective of purely getting good performance, deep RL's track record
isn't that great.
Here is a video of similar simulated robots, controlled
with online trajectory optimization.
The correct actions are computed in near real-time, online, with no offline
training. Oh, and it's from 2012. ([Tassa et al, IROS 2012](https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf)), so the hardware is 5 years old.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/uRVAX_sFT24" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

I think these behaviors compare well to the previous video from the parkour
paper. How is this possible with so much less compute?

The trick is that the approach shown (model predictive control) gets to do
planning against a known world model (the physics simulator). This is
a very unfair comparison to model-free RL.
On the other hand, if you have a model, and a model can help by this much,
why spend a bunch of time training an RL policy if it doesn't perform any
better?

In a similar vein, you can easily outperform
DQN in Atari with off-the-shelf Monte Carlo Tree Search. Here are baseline
numbers from [Guo et al, NIPS 2014](https://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning). UCT is the standard version of
MCTS used today. They report the score achieved by the learned agent.

![DQN results](/public/rl-hard/dqn_atari.png)
{: .centered }

![MCTS results](/public/rl-hard/uct_atari.png)
{: .centered }

Again, not a fair comparison, because DQN does no search, and MCTS gets to
perform search against the Atari emulaor.
However, it does show that if you just want a good Atari bot, implementing a
tree search algorithm is probably way easier than training a DQN.

Reinforcement learning can theoretically work for anything, including
environments where a model of the world isn't known. But this generality
comes at a price: it's hard to exploit any problem-specific information that
could help with learning, which forces you to use tons of samples to learn
things that could have been hardcoded.

The rule-of-thumb is that except in rare cases, domain-specific algorithms
work faster and better than reinforcement learning. This isn't a problem if
you're doing reinforcement learning for reinforcement learning's sake, but it
can make it harder to get motivated, and it's harder to explain to laypeople
why the problems you're working on are cool.

Consider the company most people think of when you mention robotics:
[Boston Dynamics](https://www.youtube.com/channel/UC7vVhkEfw4nOGp8TyDk7RcQ).
At NIPS 2016, a leading machine learning conference, they gave [a great demo](https://www.youtube.com/watch?v=SvEs_hhb0sI) - and then said they didn't do machine learning. (At least that's
what I heard, I wasn't there in person.)
If you look up research papers from the group, you find papers mentioning
[time-varying LQR, QP solvers, and convex optimization](https://dspace.mit.edu/openaccess-disseminate/1721.1/110533). In other words, classical robotics stuff.
(Personally, I'd bundle those techniques as machine learning, but that
might be an unorthodox view.)

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/fRj34o4hN4I" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

No one at Boston Dynamics has to explain why teaching a robot to do that is cool.
I have to explain my problems all the time, because often they're hard for
subtle reasons that aren't as visually impressive.


It Usually Requires a Reward Oracle
==============================================================================

Reinforcement learning assumes the existence of a reward function. Usually, this is either
given, or it is learned offline and fixed over the course of learning.

Importantly,
for RL to do the right thing, your reward function must capture *exactly* what
you want.
And I mean *exactly*. RL has an annoying tendency to overfit to your reward and
do things you don't expect.

Do you know why Atari games are such a nice benchmark? There are a few
reasons, but a big one is that the goal in every game is to maximize score, so
you never have to worry about specifying reward.

The majority of reinforcement learning results come from simulated environments,
in part because perfect knowledge of the state makes reward function design
waaaaaay easier. Consider the example of the MuJoCo tasks in OpenAI gym.
(MuJoCo is a physics simulator.)

In the Reacher task, you control a two-segment arm, that's connected to a central
point, and the goal is to move the end of the arm to a target location. Below
is a video of a successfully learned policy.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/BkhSKqc8vSA" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

Reward is easy to define. It's the distance from the end of the arm to the target,
plus a small control cost.

In the HalfCheetah environment, you have a two-legged robot, restricted to a
vertical plane, meaning it can only run forward or backward.

<div class="centered">
<video controls="controls">
  <source type="video/mp4" src="/public/rl-hard/upright_half_cheetah.mp4"></source>
  <p>Your browser does not support the video element.</p>
</video>
</div>

The goal is to learn a running gait. Again, reward is easy to define: it's
just the velocity of the HalfCheetah.

Let's consider a harder example. There's
a semi-famous boat racing example from an [OpenAI blog post](https://blog.openai.com/faulty-reward-functions/),
where finishing the race gives less points than collecting powerups.
**When reward functions don't reflect what you care about, the final behavior
doesn't always match what you wanted.**

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/tlOIHko8ySg" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

To be honest, at the time I wasn't a fan of this blog post. Not because I
thought the point was bad. It was more because I thought the point it was
making was blindingly obvious, and the post was making a bigger deal out of
it than necessary. How is it news that reinforcement learning does weird
things when your reward doesn't match what you care about?

Then I started writing this blog post, and realized the most compelling video
was the boat racing video from that blog post. So, okay, I now begrudgingly
accept that this blog post was useful.

Anyways, my grumpiness aside, there are other examples of this too. Here
is [a blog post about text summarization, from Salesforce](https://www.salesforce.com/products/einstein/ai-research/tl-dr-reinforced-model-abstractive-summarization/). Normally, the model is
trained with supervised learning, then evaluated with
an automated metric called ROUGE. ROUGE isn't differentiable, but RL can
deal with non-differentiable rewards, so it's natural to apply RL to optimize
ROUGE directly. This works, but high ROUGE doesn't guarantee high-quality
summaries.

> While ROUGE scores have a good correlation with human judgment in general, the summaries with the highest ROUGE aren't necessarily the most readable or natural ones. This became an issue when we trained our model to maximize the ROUGE score with reinforcement learning alone. We observed that our models with the highest ROUGE scores also generated barely-readable summaries.

![Salesforce ROUGE performance](/public/rl-hard/salesforce_rouge.png)
{: .centered }

Here is an example summary from a ROUGE-maximizing model.

> Button was denied his 100th race for McLaren after an ERS prevented him from making it to the start-line. It capped a miserable weekend for the Briton.  Button has out-qualified.  Finished ahead of Nico Rosberg at Bahrain. Lewis Hamilton has. In 11 races. . The race. To lead 2,000 laps. . In. . . And.

[Paulus et al, 2017](https://arxiv.org/abs/1705.04304)
{: .centered }

Again, if your reward function doesn't reflect something you care about,
there's no guarantee the learned model will care about it either.

Here's another fun example. This is [Popov et al, 2017](https://arxiv.org/abs/1704.03073),
sometimes known as "the Lego stacking paper".
The authors use a distributed version of DDPG to learn a grasping policy. The
goal is to grasp the red block, and stack it on top of the blue block.

Secondly, it still has some failure cases.
For the initial lifting motion, reward is given based on how high the red block
is. This is defined by the z-coordinate of the
bottom face of the block. One of the failure modes was that the policy learned
to tip the red block over, instead of picking it up.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/8QnD8ZM0YCo?start=27" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

Now, clearly this isn't the intended solution. But RL doesn't care.
From the perspective of reinforcement learning, it did something, and got
rewarded for it. So, it's going to keep doing that thing.

In theory, this all comes from improper reward shaping. When a policy isn't
learning, you can add reward terms to try to make the behaviors you want
more learnable.
However, whether those shaping terms actually work or not is very up in
the air. Learnability is very difficult to predict ahead of time, and while it's
possible to fight RL on this front, it's an unfulfilling fight that takes
a lot of time without teaching you very much.

For reference, here is one of the reward functions from the Lego stacking
paper.

![Lego grasp reward function](/public/rl-hard/lego_reward.png)
{: .centered }

I don't know how much time was spent designing this reward, but I'm going to
guess "a lot".

Reinforcement learning can be seen as a variant of black-box optimization. In
these black-box methods, you are given nothing about the environment. You are
simply told that this gives +1 reward, or it doesn't, and have to learn on your
own. And the problem is that anything that gives +1 reward is good, even if
the +1 reward isn't coming for the right reasons.

A classic non-RL example is the time someone applied genetic algorithms to
circuit design, and
[got a circuit where an unconnected logic gate was necessary to the final
design](https://en.wikipedia.org/wiki/Evolvable_hardware#Introduction).

![Circuit with crazy gates](/public/rl-hard/circuit.png)
{: .centered }

The gray cells are required to get correct behavior, including the one in the top-left corner,
even though it's connected to nothing.
[From "An Envolved Circuit, Intrinsic in Silicon, Entwined with Physics"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.50.9691&rep=rep1&type=pdf)
{: .centered }

However, I've heard several anecdotes to this effect from other RL researchers.

* A coworker is teaching an
agent to learn how to navigate. The episode terminates whenever the agent
walks out of bounds. He didn't add any penalty if the episode terminates this
way. The final policy learned to be suicidal, because negative reward was
plentiful, positive reward was too hard to achieve, and a quick death ending
in 0 reward was preferable to a long life that risked negative reward.

* A friend is training a simulated robot arm to reach towards a point
above a table. It turns out the point was defined *with respect to the table*,
and the table wasn't anchored to anything.
The policy learned to slam the table really hard, making the table fall
over, which moved the target point too. The target point *just so happened*
to fall next to the end of the arm.

* A researcher gives a talk about using RL to train a simulated robot hand to
pick up a hammer and hammer in a nail. Initially, the reward was defined just
by how far the nail is pushed into the hole. Instead of
picking up the hammer, the robot used its own limbs to punch the nail in.
So, they added a reward term to encourage picking up the hammer, and retrained
the policy.
They got the policy to pick up the hammer...but then it threw the hammer at the
nail instead of actually using it.

Admittedly, these are all secondhand accounts, and I haven't seen videos of
any of these behaviors.
However, none of it sounds implausible to me.
I've been burned by RL too many times to believe otherwise.

I know people who like to tell stories about [paperclip optimizers](https://en.wikipedia.org/wiki/Instrumental_convergence#Paperclip_maximizer). I get it,
I really do. But honestly, I'm sick of hearing those stories, because they
always speculate up some superhuman misaligned AGI to create a just-so story.
There's no reason to speculate that far when present-day examples happen
all the time.

(If you're interested in further reading on what makes a good reward,
a good search term is ["proper scoring rule"](https://en.wikipedia.org/wiki/Scoring_rule#Proper_scoring_rules).
See [this Terrence Tao blog post](https://terrytao.wordpress.com/2016/06/01/how-to-assign-partial-credit-on-an-exam-of-true-false-questions/) for an approachable example.)



Even Given a Reward Oracle, Local Optima Can Be Hard To Escape
==============================================================================

All the previous examples are commonly called "reward hacking", which to me
implies a clever, out-of-the-box solution that gives more reward than the
intended answer.

These are the exceptions. The much more common case is stupidity - poor
local optima that come from getting the exploration-exploitation tradeoff
wrong. Ones that lead to frustration and so, so much pain,

I have far too many stories for this, so here are just a few.

Here's one of my favorite videos. This is an implementation of
[Normalized Advantage Function](https://arxiv.org/abs/1603.00748), learning
on the HalfCheetah environment.

<div class="centered">
<video controls="controls">
  <source type="video/mp4" src="/public/rl-hard/upsidedown_half_cheetah.mp4"></source>
  <p>Your browser does not support the video element.</p>
</video>
</div>

From an outside perspective, this is really, *really* dumb. But we can
only say it's dumb because we can see the 3rd person view, and have a bunch of
prebuilt knowledge that tells us running on your feet is better.
RL doesn't know this! It sees a state vector, it sends action vectors, and it
knows it's getting some positive reward. That's it.

Here's my best guess for what happened during learning.

* Falling forward is better than standing still. The policy explored its way
to learning how to fall.
* It did so enough to "burn in" that behavior, so now it's falling forward
consistently.
* Doing a backflip is better than staying still after falling over.
The policy explored its way to a backflip maneuver.
* It did that enough too, and now backflipping is burned into the policy.
* Once the policy is backflipping consistently, which is easier for the
policy: learning to do a front-flip and then learning to run from that,
or figuring out how to move forward while lying on its back? I would
guess the latter.

And thus, backflipping and traveling upside-down.

Realistically though, I have no idea if my theory above is right. All I know
is that it didn't learn.

Here's another failed run, this time on the Reacher environment.

<div class="centered">
<video controls="controls">
  <source type="video/mp4" src="/public/rl-hard/failed_reacher.mp4"></source>
  <p>Your browser does not support the video element.</p>
</video>
</div>

In this run, the initial random weights tended to output highly positive or
highly negative action outputs. This makes most of the actions output the
maximum or minimum acceleration possible. It's really easy to spin super fast:
just output high magnitude actions. Once the robot gets going, it's hard
to deviate from this policy in a meaningful way - to deviate, you have to take
several exploration steps to stop the rampant spinning. It's certainly
possible, but in this run, it didn't happen.

These are both cases of the classic exploration-exploitation problem that has dogged
reinforcement learning since time immemorial.
Your data comes from your current policy. If your current policy explores too
much you get junk data and learn nothing. Exploit too much and you burn-in
behaviors that aren't optimal.

There are several intuitively pleasing ideas for addressing this - intrinsic
motivation, curiosity-driven exploration, count-based exploration, and so forth.
Many of these approaches date back to the 1980s or earlier, and several of them
have been attempted recently with deep learning models.
However, as far as I know, none of the recent results have worked consistently
for all environments.
I'm skeptical such a silver bullet will be figured out anytime soon. The
problem is really, really, really, really, really hard. Exploration-exploitation
was first studied in multi-armed bandits, and to quote the
[Wikipedia article](https://en.wikipedia.org/wiki/Multi-armed_bandit),

> Originally considered by Allied scientists in World War II, it proved so intractable that, according to Peter Whittle, the problem was proposed to be dropped over Germany so that German scientists could also waste their time on it.

(Reference: [Q-Learning for Bandit Problems, Duff 1995](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.1916&rep=rep1&type=pdf))
{: .centered }

I've taken to imagining reinforcement learning as a demon that's
deliberately misinterpreting your reward and actively searching for local optima.
Whenever your reward function gives small reward for stupid behavior, there's always a
chance learning just gets completely stuck at the wrong thing. It's just
how deep RL is right now.


Even When Deep RL Works, It May Just Be Overfitting to Weird Patterns In the Environment
==============================================================================

> Deep RL is popular because it's the only area in ML where it's socially
> acceptable to train on the test set.

[Source](https://twitter.com/jacobandreas/status/924356906344267776)
{: .centered }

The upside of reinforcement learning is that if you want to do
well in an environment, you're free to overfit like crazy. The downside is that
if you want to generalize to any other environment, you're probably going to
do poorly, because you overfit like crazy.

(To forestall some obvious counterexamples: yes, sometimes you can train on a
distribution of environments, to get around this issue. I'll get to this soon.)

DQN can solve a lot of the Atari games, but it does so by focusing all of
learning on a single goal - getting really good at one game. The final model
won't generalize to other games, because it hasn't been trained that way.
Finetuning a learned DQN to a new Atari game sort of works
(see [Progressive Neural Networks (Rusu et al, 2016)](https://arxiv.org/abs/1606.04671)), but it's not the wild success people empirically get from
pretrained ImageNet features.

To forestall some obvious counterexamples: yes, in some cases, there is
a natural distribution of environments that you can train on. An example
is navigation, where you can sample goal locations randomly, and use the
universal value function approach to generalized.
(See [Universal Value Function Approximators, Schaul et al, ICML 2015](http://proceedings.mlr.press/v37/schaul15.pdf).) I think there are promising signs here, and
I give more examples in a future section. However, in practice, I think the
generalization capabilities aren't strong enough yet. RL has yet to have
its "ImageNet for control" moment. (OpenAI Universe tried to solve
this, but from what I heard, it's just too hard to solve anything reliably
in it, due to the asynchronous implementation.)

This is most obvious in multiagent environments. An example is
[Raghu et al, 2017](https://arxiv.org/abs/1711.02301). (Disclaimer: I worked
on this paper.) We studied a toy 2-player
game where there's a closed-form analytic solution for optimal play.
In one of our first experiments, we freezed player 1's behavior, then trained
player 2 with RL. By doing this, you can treat player 1's actions as part
of the environment. We trained player 2 against the optimal player 1, and
although RL can learn good strategies, we found it failed to generalize
to non-optimal players. Our theory was that the data distribution wasn't
rich enough.

[Lanctot et al, NIPS 2017](https://arxiv.org/abs/1711.00832) showed a
similar result. Here, there are two agents
playing laser tag. The agents are trained with multiagent reinforcement
learning. To test generalization, they run the training with 5 random
seeds. Here's a video of agents that have been trained against one
another.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/8vXpdHuoQH8" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

As you can see, they learn to move towards and shoot each other. Then, they
took player 1 from one experiment, and pitted it against player 2 from a
*different* experiment. If the learned policies generalize, we should see
similar behavior.

Spoiler alert: you don't.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/jOjwOkCM_i8" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

This seems to be a running theme in multiagent RL. When agents are trained
against one another, a kind of co-evolution happens. The agents get really good
at beating each other, but as soon as they get deployed against a player they
haven't seen before, performance goes to shit. Note
that the only difference between these videos is the random seed. Same learning
algorithm, same hyperparameters, and the divergence is purely from randomness
in initial conditions.

That being said, there are some neat recents from competitive self-play environments
that seem to contradict this. [OpenAI has a nice blog post of some of their work in this space](https://blog.openai.com/competitive-self-play/). Self-play is also an important part of both
AlphaGo and AlphaZero. My intuition is that if your agents are learning at
the same pace, they can continually challenge each other and speed up each other's
learning, but if one of them learns much faster, it exploits the weaker player
too much and overfits. This is easiest to get in symmetric self-play, and gets
harder when you try to relax to general multiagent settings.


Even Ignoring Generalization Issues, The Final Results Can be Unstable and Hard to Reproduce
========================================================================

Almost every ML algorithm has hyperparameters, which influence the behavior
of the learning system. Often, these are picked by hand, or by random search.

Supervised learning is stable. Fixed dataset, ground truth targets. If you
change the hyperparamters a little bit,
your performance won't change that much. Not all hyperparameters perform
well, but with all the empirical tricks discovered over the years,
many hyperparams will show signs of life at train time. These signs of life are
super important, because they tell you that you're on the right track, you're
doing something reasonable, and it's worth investing more time.

Reinforcement learning, on the other hand, isn't stable at all. Sometimes it
is, but most of the time it's pathologically awful.

When I started working at Google Brain, one of the first
things I did was implement the algorithm from the Normalized Advantage Function
paper. I figured it would only take me about 2-3 weeks. I had several things
going for me: some familiarity with Theano (which transferred to TensorFlow
well), some deep RL experience, and the first author was literally sitting
next to me because he was interning at Brain.

It ended up taking me 6 weeks to reproduce results, thanks to
a bunch of subtle software bugs that took forever to track down.
The question is, why did this take so long?

To answer this, let's consider the simplest continuous control task in
OpenAI Gym: the Pendulum task. In this task, there's a pendulum, anchored
at a point, with gravity acting on the pendulum. The input state is
3-dimensional. The action space is 1-dimensional, the amount of torque to apply.
The goal is to balance the pendulum perfectly straight up.

This is a tiny problem, and it's made even easier by a well shaped reward.
Reward is defined by the angle of the pendulum. Actions bringing the pendulum
closer to the vertical not only give reward, they give *increasing* reward.
The reward landscape is basically concave.

Below is a video of a policy that *mostly* works. Although the policy doesn't
balance straight up, it outputs the exact torque needed to counteract
gravity.

<div class="centered">
<video controls="controls">
  <source type="video/mp4" src="/public/rl-hard/pendulum_example.mp4"></source>
  <p>Your browser does not support the video element.</p>
</video>
</div>

Here is a plot of performance, after I fixed all the bugs. Each line is the
reward curve from one of 10 independent runs. Same hyperparameters, the only
difference is the random seed.

![Graph of Pendulum results](/public/rl-hard/pendulum_results.png)

30% of the time, it's not doing anything at all! *But this counts as working.*
Here's another plot from some published work,
["Variational Information Maximizing Exploration" (Houthooft et al, NIPS 2016)](https://arxiv.org/abs/1605.09674).
The environment is HalfCheetah. The reward is modified to be sparser, but the
details aren't too important.
The y-axis is episode reward, the x-axis is number of timesteps, and the
algorithm used is TRPO.

![Plot from VIME paper](/public/rl-hard/vime.png)
{: .centered }

The dark line is the median performance over 10 random seeds, and the shaded
region is the 25th to 75th percentile. Don't get me wrong, this plot is a good
argument in favor of VIME. But on the other hand, the 25th percentile line
is really close to 0 reward. That means about 25% of runs are failing for no
reason.

Look, there's variance in supervised learning too, but it's rarely this bad.
If my supervised learning code failed by random chance 30% of the time, I'd
have super high confidence there was a bug in data loading or training. But
if that happens in reinforcement learning, I have no idea if there's a bug,
or if reinforcement learning is just messing with me. Maybe it's just Tuesday.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/GlhOUyy4wbs" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

When your training algorithm is both sample inefficient and unstable, it heavily
slows down your rate of productive research, because you have to spend much more
compute to get good evidence for your hypotheses. Maybe it only takes 1 million
steps, but when you multiply it by 5 random seeds, and then multiply that with
hyperparam tuning, it all becomes a bit depressing.

> If it makes you feel any better, I've been doing this for a while and it took me last ~6 weeks to get a from-scratch policy gradients implementation to work 50% of the time on a bunch of RL problems. And I also have a GPU cluster available to me, and a number of friends I get lunch with every day who've been in the area for the last few years.
>
> Also, what we know about good CNN design from supervised learning land doesn't seem to apply to reinforcement learning land, because you're mostly bottlenecked by credit assignment / supervision bitrate, not by a lack of a powerful representation. Your ResNets, batchnorms, or very deep networks have no power here.
>
> [Supervised learning] wants to work. Even if you screw something up you'll usually get something non-random back. RL must be forced to work. If you screw something up or don't tune something well enough you're exceedingly likely to get a policy that is even worse than random. And even if it's all well tuned you'll get a bad policy 30% of the time, just because.
>
> Long story short your failure is more due to the difficulty of deep RL, and much less due to the difficulty of "designing neural networks".

[Hacker News comment from Andrej Karpathy, back when he was at OpenAI](https://news.ycombinator.com/item?id=13519044)
{: .centered }

Instability to random seed is like a canary in a coal mine. If pure randomness
is enough to lead to this much variance between runs, imagine how much an actual
difference in the code could make.

Luckily, we don't have to imagine, because this was inspected by
the paper ["Deep Reinforcement Learning That Matters" (Henderson et al, AAAI 2018)](https://arxiv.org/abs/1709.06560).
It makes several points that were well-known in the RL community, but which
weren't packaged together until recently. Among its conclusions
are:

* Multiplying the reward by a constant can give significant differences in performance.
* Five random seeds (a common reporting metric) may not be enough to argue
significant results.
* Different implementations of the same algorithm have different performance on
the same task.

My theory is that RL is very sensitive to both your initialization and to the
dynamics of your training process, because your data is always collected online
and the only supervision you get is a single scalar for reward. A policy that
randomly stumbles onto good training examples will bootstrap itself much
faster than a policy that doesn't. A policy that fails to discover good training
examples in time will collapse towards learning nothing at all, as it becomes
more confident that any deviation it tries will fail.


But What About All The Great Things Deep RL Has Done For Us?
==================================================================

Deep reinforcement learning has certainly done some very cool things. DQN is
old news now, but was absolutely *nuts* at the time. A single model was able to
learn directly from raw pixels, without tuning for each game individually.

AlphaGo and AlphaZero
continue to be very impressive achievements.
However, outside of these game playing successes,
deep RL hasn't done much that has practical real world value.

(Link to Andrej post?)

I tried to think of real-world, productionized uses of deep RL, and I could
only think of two things: [the project to reduce the power used by
Google's data centers](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/),
and [the project to automatically design neural net architectures](https://research.googleblog.com/2017/11/automl-for-large-scale-image.html). A friend reminded me of a 3rd project: [optimizing
device placement for large Tensorflow graphs (Mirhoseini et al, ICML 2017)](https://arxiv.org/abs/1706.04972).
Jack Clark from OpenAI
[tweeted a request for similar examples](https://twitter.com/jackclarkSF/status/919584404472602624),
and I just didn't find that much in the replies.

I assume finance companies have tried using RL, but so far there's no definitive
proof. Of course, finance companies have reasons to be cagey
about how they play the market, so perhaps the evidence there is never going to
be strong.

**Overall, for all of its hype and promise, deep RL is still decidedly a research
topic.** If you came to me with an image classification, I'd point you to
pretrained ImageNet models, and I'd have high confidence that the learning
would work.
If [the people behind *Silicon Valley* can make Not Hotdog](https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3),
then you can too. Right now, I can't say the same about reinforcement learning.
There's too much that could go wrong.


Alright. In What Settings Could Deep RL Do Something Great For Me?
=====================================================================

A priori, it's really hard to say. The problem with trying to solve everything
with RL is that you're trying to solve several very different environments
with the same approach. Until you try things out, it can be hard to judge
whether a given RL algorithm will work.

That being said, we can draw conclusions from the current list of deep
reinforcement learning successes. Here's my list so far.

* Things mentioned in the previous sections: DQN, AlphaGo, AlphaZero,
the parkour bot, reducing power center usage, Neural Architecture Search.
* OpenAI's Dota 2 1v1 Shadow Fiend bot, which beat top pro players in a
simplified duel setting.
* A Super Smash Brothers Melee bot from students at MIT and NYU, that can beat
pro players at 1v1 Falcon dittos.

Two machine learning systems,
[Libratus (Brown et al, IJCAI 2017)](https://www.ijcai.org/proceedings/2017/0772.pdf) and [DeepStack (Moravčík et al, 2017)](https://arxiv.org/abs/1701.01724),
recently beat pro players at no-limit heads up Texas Hold'Em.
These are both very cool, but by my understanding, neither uses reinforcement
learning. They instead use counterfactual regret minimization and clever
iterative solving of subgames.

From this list, we can identify common properties that make learning more
feasible. None of the properties below are required, but every one of them
helps.

* **It is easy to generate near unbounded amounts of experience.**
It should be clear why this helps. The more data you have, the easier the learning
problem is. This applies to
Atari, Go, Chess, Shogi, and the simulated environments for the parkour bot.
It likely applies to the power center project too, because
[in prior work (Gao, 2014)](https://googleblog.blogspot.com/2014/05/better-data-centers-through-machine.html),
it was shown that neural nets can predict energy efficiency with high
accuracy. That's exactly the kind of simulated model you'd want for training an
RL system.

    It might apply to the Dota 2 and SSBM work, but it depends on the throughput
of how quickly the games can be run, and how many machines were available to
run them.

* **The problem is simplified into an easier form.** One of the common errors
I've seen in deep RL is to dream too big. Reinforcement learning can do
anything! That doesn't mean you have to do everything at once.

    The OpenAI Dota 2 bot only played the early game, only played Shadow Fiend against Shadow
Fiend in a 1v1 laning setting, used hardcoded item builds, and presumably
called the [Dota 2 API](https://developer.valvesoftware.com/wiki/Dota_2_Workshop_Tools/Scripting/API)
to avoid having to solve perception. The SSBM bot started with 1v1 games,
Captain Falcon only, Battlefield only, infinite time matches only.

    This isn't a dig at either bot. Why work on a hard problem when you don't
even know the easier one is solvable? The
broad trend of all research is to demonstrate the smallest proof-of-concept
first and generalize it later. OpenAI is extending their Dota 2 work, and
the SSBM bot has been extended to play Marth and Sheik. (LINK).

* **There is a way to introduce self-play into learning.** This is a component
of AlphaGo, AlphaZero, the Dota 2 Shadow Fiend bot, and the SSBM Falcon bot. I
should note that by self-play, I mean exactly the setting where the game is
competitive, and both players can be controlled by the same agent. So far,
that setting seems to have the most stable and well-performing behavior.

* **The reward signal is rich, and actions have quick consequences.**
In Dota 2, reward can come from last hits (triggers after every monster kill
by either player), and health (triggers after every attack or skill that
hits a target.) These reward signals
come quick and often. For the SSBM bot, reward can be given for damage dealt
and taken, which gives signal for every attack that successfully lands. The shorter
the delay between action and consequence, the easier it is for
reinforcement learning to figure out a path to high reward.

    Consider the [original neural architecture search paper (Zoph et al, ICLR 2017)](https://openreview.net/forum?id=r1Ue8Hcxg). The reward is validation accuracy, which is a
very rich reward signal - if a neural net design decision only increases
accuracy from 70% to 71%, RL will still pick up on this. Additionally, there's
evidence that hyperparameters in deep learning are close to
linearly independent. (This was empirically shown in [Hyperparameter
Optimization: A Spectral Approach (Hazan et al, 2017)](https://arxiv.org/abs/1706.00764) - a summary by me is
[here]({% post_url 2017-06-27-hyperparam-spectral %}) if interested.)
NAS isn't exactly tuning hyperparameters, but it seems
reasonable that neural net design decisions are also similar. This is good
news for learning because the correlations between decision and performance
will be strong.

    The combination of these two helps me understand why it "only" takes about
12800 trained networks to learn a better one, compared to the millions of examples
needed in other environments.

The broader point I want to get across is this: many things have to go right
for reinforcement learning to be a plausible solution, and even then, it's
not a free ride to make that solution happen. Perhaps in the future, RL will
be robust enough to be plug-and-play. But for now, it's not there.


Looking to The Future
===============================================================================

There's an old saying - every researcher learns how to hate their area of
study. The trick is that researchers will press on despite this, because they
like the problems too much.

That's roughly how I feel about deep reinforcement learning. Despite my
reservations, I think people absolutely should be throwing RL at different
problems, including ones where it probably shouldn't work. How else are we
supposed to make RL better?

I see no reason why deep RL couldn't work, given more time. The question is
how it'll get there. Here are some futures I find plausible. For the futures
based on further research, I've provided citations to relevant papers in those
research areas.

**Local optima are good enough:** It would be very arrogant to claim humans are
globally optimal at anything. I would guess we're juuuuust good enough to get
to civilization stage, compared to any other species. In the same vein, an
RL solution doesn't have to achieve a global optima, as long as its local optima
is better than the human baseline.

**Hardware solves everything:** I know some people who believe that the most
influential thing that can be done for AI is simply scaling up hardware. Personally,
I'm skeptical that hardware will fix everything, but it's certainly going to
be important. The faster you can run things, the less you care about sample
inefficiency, and the easier it is to brute-force your way past exploration
problems.

**Add more learning signal:** Sparse rewards are hard to learn because you get
very little information about what thing help you. It's possible we can either hallucinate
positive rewards ([Hindsight Experience Replay, Andrychowicz et al, NIPS 2017](https://arxiv.org/abs/1707.01495)), define auxiliary tasks ([UNREAL, Jaderberg et al, NIPS 2016](https://arxiv.org/abs/1611.05397)),
or bootstrap with self-supervised learning to build good world model. Adding
more cherries to the cake, so to speak.

**Model-based learning unlocks sample efficiency:** Here's how I describe
model-based RL: "EVeryone wants to do it, not many people know how." In principle,
a good model fixes a bunch of problems. Good world models transfer well to new tasks,
and rollouts of the world model let you imagine new experience. From what I've
seen, model-based approaches use fewer samples as well.

The problem is that learning good models is hard. My impression is that
low-dimensional state models work sometimes, and image
models are usually too hard. But, if this gets easier, some interesting things
could happen.

[Dyna (Sutton, 1991)](http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=711FEF6BA26BBF98C28BC111B26F8761?doi=10.1.1.48.6005&rep=rep1&type=pdf) and
[Dyna-2 (Silver et al., ICML 2008)](http://www0.cs.ucl.ac.uk/staff/D.Silver/web/Applications_files/dyna2.pdf) are
classical papers in this space.
For papers combining model-based learning with deep nets, I would recommend a few recent papers from the Berkeley robotics labs:
[Neural Network Dynamics for Model-Based Deep RL with Model-Free Fine-Tuning (Nagabandi et al, 2017](http://bair.berkeley.edu/blog/2017/11/30/model-based-rl/),
[Self-Supervised Visual Planning with Temporal Skip Connections (Ebert et al, CoRL 2017)](https://arxiv.org/abs/1710.05268),
[Combining Model-Based and Model-Free Updates for Trajectory-Centric Reinforcement Learning (Chebotar et al, ICML 2017)](https://arxiv.org/abs/1703.03078).
[Deep Spatial Autoencoders for Visuomotor Learning (Finn et al, ICRA 2016)](http://rll.berkeley.edu/dsae/dsae.pdf),
and [Guided Policy Search (Levine et al, ICML 2013)](https://graphics.stanford.edu/projects/gpspaper/gps_full.pdf).

**Reward functions could be learnable:** The promise of ML is that we can use
data to learn things that are better than human design. If reward function design
is so hard, Why not apply this to learn better reward functions? Imitation
learning and inverse reinforcement learning are both rich fields that have
shown reward functions can be implicitly
defined by human demonstrations or human ratings.

For famous papers in inverse RL and imitatin learning, see
[Algorithms for Inverse Reinforcement Learning (Ng and Russell, ICML 2000)](http://ai.stanford.edu/~ang/papers/icml00-irl.pdf),
[Apprenticeship Learning via Inverse Reinforcement Learning (Abbeel and Ng, ICML 2004)](http://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf),
and [DAgger (Ross, Gordon, and Bagnell, AISTATS 2011)](https://www.cs.cmu.edu/~sross1/publications/Ross-AIStats11-NoRegret.pdf).

For recent work scaling these ideas to deep learning, see [Guided Cost Learning (Finn et al, ICML 2016)](https://arxiv.org/abs/1603.00448), [Time-Constrastive Networks (Sermanet et al, 2017)](https://arxiv.org/abs/1704.06888),
and [Learning From Human Preferences (Christiano et al, NIPS 2017)](https://blog.openai.com/deep-reinforcement-learning-from-human-preferences/). (The Human Preferences paper in particular showed
that a reward learned from human ratings was actually better-shaped for learning
than the original hardcoded reward, which is a neat practical result.)

For longer term work that doesn't use deep learning, I liked
[Inverse Reward Design (Hadfield-Menell et al, NIPS 2017)](https://arxiv.org/abs/1711.02827)
and [Learning Robot Objectives from Physical Human Interaction (Bajcsy et al, CoRL 2017)](http://proceedings.mlr.press/v78/bajcsy17a/bajcsy17a.pdf).


**Transfer learning saves the day:** The promise of transfer learning is that
you can leverage knowledge from previous tasks to speed up learning of new ones.
I think this is absolutely the future, when task learning is robust enough to
solve several disparate tasks. It's hard to do transfer learning if you can't
learn at all, and given task A and task B, it can be very hard to predict
whether A transfers to B. In my experience, it's either super obvious, or super
unclear, and even the super obvious cases aren't trivial to get working.

[Universal Value Function Approximators (Schaul et al, ICML 2015)](http://proceedings.mlr.press/v37/schaul15.pdf),
[Distral (Whye Teh et al, NIPS 2017)](https://arxiv.org/abs/1707.04175),
and [Overcoming catastrophic forgetting (Kirkpatrick et al, PNAS 2017)](https://deepmind.com/blog/enabling-continual-learning-in-neural-networks/) are recent works in this direction.
For older work, consider reading [Horde (Sutton et al, AAMAS 2011)](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.6455&rep=rep1&type=pdf).

Robotics
in particular has had lots of progress in sim-to-real transfer (transfer learning
between a simulated version of a task and the real task). See [Domain Randomization (Tobin et al, IROS 2017)](https://blog.openai.com/spam-detection-in-the-physical-world/), [Sim-to-Real Robot
Learning with Progressive Nets (Rusu et al, CoRL 2017)](https://arxiv.org/abs/1610.04286),
and
[GraspGAN (Bousmalis et al, 2017)](https://research.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html). (Disclaimer: I worked on GraspGAN.)


**Priors from metalearning could speed up learning elsewhere:**
RL algorithms are designed to apply to any environment that's an MDP. However,
we don't need to solve arbitrary environments, we just need to solve
environments in the real-world. In principle, we could use metalearning to learn
a real-world prior that lets us quickly learn new real-world tasks, at the cost
of general learning ability.

I credit Pieter Abbeel for first introducing me to this argument. He mentions
it in a lot of his talks, and I agree it makes sense.
For example, if I wanted to use RL to do navigating in warehouses, and wanted to
arbitrary warehouses, I'd look into using metalearning to learn a good
navigation prior, and then finetune it for the specific warehouse the robot is
deployed in,

A summary of recent work can be found in [this post](http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/)
from BAIR (Berkeley AI Research).


**Harder environments could paradoxically be easier:** One of the big lessons
from the DeepMind parkour paper is that if you make your task very difficult
by adding several task variations, you can actually make the learning
easier, because the policy cannot overfit to any one setting without losing
performance on all the other settings. We've seen a similar thing in the
domain randomization papers, and even back to ImageNet: models trained on
ImageNet will generalize way better than ones trained on CIFAR-100. An
analogous "ImageNet for control" could make RL start working.
(OpenAI tried to spur this with Universe, but it didn't get adopted. From what I
heard, the environments were just too hard to solve. However, the core premise
is sound and I'd like to see another attempt when RL is more performant.)

Environment wise, there are a lot of options. [OpenAI Gym](https://github.com/openai/gym)
easily has the most traction, but there's also the [Arcade Learning Environment](https://github.com/mgbellemare/Arcade-Learning-Environment), [Roboschool](https://github.com/openai/roboschool),
[DeepMind Lab](https://github.com/deepmind/lab), the [DeepMind Control Suite](https://github.com/deepmind/dm_control), and [ELF](https://github.com/facebookresearch/ELF).


Finally, although it's unsatisfying from a research
perspective, the issues of RL may not matter for practical purposes. As a
hypothetical example, suppose a finance company is using RL. They train a
trading agent based on past data from the US stock market, using 3 random seeds.
In live A/B testing, one gives 2% less revenue, one performs the
same, and one gives 2% more revenue. In that hypothetical, reproducibility
doesn't matter - you deploy the model with 2% more revenue and celebrate.
Similarly, it doesn't matter that the trading agent may only perform well
in the United States - if it generalizes poorly to the worldwide market,
just don't deploy it there. There is a large gap between doing something
extraordinary and making that extraordinary success reproducible, and maybe it's
worth focusing on the former first.


Where We Are Now
===============================================================================

Overall, although I'm pessimistic about where reinforcement learning *is*, I
still believe in where reinforcement learning *could be*. That being said,
the next time someone asks me whether RL can solve their problem, I'm still
going to tell them that no, it can't. But I'll also tell them to ask me again in
a few years. By then, maybe it can.
