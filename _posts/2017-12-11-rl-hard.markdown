---
layout: post
title:  "Deep Reinforcement Learning Doesn't Work Yet"
date:   2017-12-11 23:06:00 -0700
---

*This mostly cites papers from Berkeley, Google Brain, DeepMind, and OpenAI,
because I'm most familiar with that work.
I'm almost certainly missing stuff from less famous institutions, so if I missed
a paper that's an important counterexample, I apologize - I'm just one guy,
after all.*

Once, on Facebook, I made the following claim.

> Whenever someone asks me if reinforcement learning can solve their problem, I tell them it can't. I think this is right at least 70% of the time.

![Futurama Bender meme](/public/rl-hard/bender-70.jpg)
{: .centered }

Deep reinforcement learning is driven by mountains and mountains of hype. And
for good reason!
Reinforcement learning is an incredibly general paradigm,
and in principle, a robust and performant RL system should be great at
everything. Merging this paradigm with the empirical power of deep learning
is an obvious fit. Deep RL is one of the closest things that looks anything like
AGI, and that's the kind of dream that fuels billions
of dollars of funding.

Unfortunately, it doesn't really work.

Now, I believe it can work, otherwise I wouldn't be interested in it.
But there are a lot of problems in the way, many of which feel fundamentally
difficult. Hidden behind the beautiful demos is a towering garbage pile of
pain, failed experiments, statistically insignificant results, and pure, nonsensical
bullshit.

My AGI timelines are long because everyone is looking towards RL
as the key, and to me it looks like a really, really shitty key.

So, to all the people who ask me why RL isn't used on more real-world problems,
or who believe that deep RL actually does reasonable things, or who have the
naive, wide-eyed belief that their new research idea will only take a few weeks to
test out: let me tell you many of the reasons why deep reinforcement learning
is terrible.


It Can Be Horribly Sample Inefficient
==============================================================================

The most well-known benchmark for deep reinforcement learning is Atari. This is
the environment that put DeepMind on the map. Combine Q-Learning with a
reasonably sized neural networks, and some (but not all) of the games get
solved to human or superhuman performance.

Atari games run at 60 frames per second. On the top of your head, do you
know how many frames a state of the art DQN needs to reach human performance?

The answer depends on the game, but let's take a look at a recent Deepmind
paper, [Rainbow DQN](https://arxiv.org/abs/1710.02298). This paper came out
about 2 months ago, and it's a combination
of the several incremental advances made in the past 5 years.
The results come in this handy chart.

![Figure from Rainbow DQN](/public/rl-hard/rainbow_dqn.png)
{: .centered }

The y-axis is "median human-normalized score". This is computed by training
57 DQNs, one for each Atari game, normalizing the score of each agent such that
human performance is 100%, then plotting the median performance across the
57 games. RainbowDQN passes the 100% threshold at about 18 million frames.
This corresponds to about 83 hours of play experience, plus however long it takes
to train the model.

This is actually pretty good, when you consider that the previous record
([Distributional DQN](https://arxiv.org/pdf/1707.06887.pdf)) needed
*70 million* frames to hit 100% median performance. As for the
[DQN from the Nature paper](https://www.nature.com/articles/nature14236),
it never hits 100% median performance, even after 200 million frames of training.

RL has a nasty habit of taking longer than you think it should.
Many of the MuJoCo benchmarks require 10^5 to 10^7 steps. Honestly, that's
fine, but it *really* feels like it shouldn't require that many steps. These
tasks don't look that hard.

Below is a video from the [DeepMind parkour paper](https://arxiv.org/abs/1707.02286),
trained using PPO.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/hx_bgoTF7bs" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

These results are *incredible*. When I saw this video I was shocked that RL
could even do this. Turns out it can...if you use 64 workers, and train for
100 hours. Not that unreasonable, but it's still a lot of compute.


Many of the Benchmarks are Better Solved by Other Methods
==============================================================================

Now, I love the parkour paper. But it's worth putting it in context of other
work.

Below is a video of similar simulated robots, controlled
with online trajectory optimization.
The correct actions are computed in near real-time, online, with no offline
training. Oh, and it's [from 2012](https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf),
so it's using old hardware too.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/uRVAX_sFT24" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

Personally, I find these behaviors equally cool to watch. So how can this
approach work so well, while using way less compute.

The trick is that the approach shown (model predictive control) gets to do
planning against a known world model (the physics simulator). So on one hand,
it's not a fair comparison to model-free RL. On the other hand, it's somewhat
disheartening how much time RL takes to get results that look *worse* than
this.

In a similar vein, you can easily outperform
[DQN in Atari with Monte Carlo Tree Search instead](https://papers.nips.cc/paper/5421-deep-learning-for-real-time-atari-game-play-using-offline-monte-carlo-tree-search-planning).

Reinforcement learning can theoretically work for anything, including
environments where a model of the world isn't known. However, this generality
comes at the price of sample efficiency, and the rule-of-thumb is that if any
domain-specific algorithm works, it'll work faster and better. In my experience,
if you want to solve something, you're better off trying anything besides
reinforcement learning first, because it'll be easier to utilize
problem-specific information.

Consider the company most people think of when you mention robots:
[Boston Dynamics](https://www.youtube.com/channel/UC7vVhkEfw4nOGp8TyDk7RcQ).
At NIPS 2016, a leading machine learning conference, they gave a legendary
performance where they said they din't do machine learning.
If you look up research papers from the group, you can find
[that they use time-varying LQR, QP solvers, convex optimization](https://dspace.mit.edu/openaccess-disseminate/1721.1/110533). In other words, classical robotics stuff.
(Personally, I'd bundle those techniques as close enough to machine learning,
but that's probably an unorthodox classification.)

https://agile.seas.harvard.edu/files/agile/files/explicit-zmp.pdf

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/fRj34o4hN4I" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>



It (Usually) Requires a Reward Oracle
==============================================================================

RL algorithms assume the existence of a reward function. Usually, this is either
given, or it is learned offline and fixed over the course of learning. Importantly,
for RL to do the right thing, your reward function must capture *exactly* what
you want.
And I mean *exactly*. RL has an annoying tendency to overfit to your reward and
do things you don't expect.

Take Atari, for example. You want to maximize score. You
*always* want to maximize score. It's just how those games work. That's why
Atari games are such a nice benchmark.

The majority of reinforcement learning results come from simulated environments,
and that's because you have perfect knowledge of the state and can define an
appropriate reward. Consider the MuJoCo tasks in OpenAI gym.
In the Reacher task, you control a two-segment arm, that's connected to a central
point, and the goal is to move the end of the arm to a target location.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/BkhSKqc8vSA" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

You have the exact locations of everything, so reward is simply the distance to
the goal. Easy.

In the HalfCheetah environment, you have a two-legged robot, restricted to a
vertical plane, meaning it can only run forward or backward.

<div class="centered">
<video controls="controls">
  <source type="video/mp4" src="/public/rl-hard/upright_half_cheetah.mp4"></source>
  <p>Your browser does not support the video element.</p>
</video>
</div>

The goal is to learn
a running gait, so you give reward depending on its velocity. Also straightforward.

**Reward functions that don't have this property tend to work poorly.** There's
a semi-famous boat racing example from an [OpenAI blog post](https://blog.openai.com/faulty-reward-functions/),
where finishing the race gives less points than collecting powerups.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/tlOIHko8ySg" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

To be honest, at the time I really didn't understand why this merited a blog
post. It was just so *obvious*. Of course optimizing the wrong objective leads
to weird behavior! What, do people not see this happening in schools?
(Teaching to the test instead of teaching to learn.) In academia? (Publishing
more papers instead of publishing good papers because of publish or perish.)
How is it news that RL does the same thing? Do enough RL and you inevitably
run into an issue like this.

(Then I started writing this blog post, and found that the most compelling
video I could find was the boat racing video from the blog post. So, I now
begrudgingly accept that the post had a point.)

For example, [some researchers at Salesforce](https://www.salesforce.com/products/einstein/ai-research/tl-dr-reinforced-model-abstractive-summarization/)
trained a text summarization model.
This is usually trained with supervised learning. For evaluation, they use an
automated metric called ROUGE, as well as human ratings. ROUGE isn't differentiable,
so they tried using RL to optimize ROUGE. And yes, you can get much better
ROUGE scores if you train with RL, except there's one problem.

> While ROUGE scores have a good correlation with human judgment in general, the summaries with the highest ROUGE aren't necessarily the most readable or natural ones. This became an issue when we trained our model to maximize the ROUGE score with reinforcement learning alone. We observed that our models with the highest ROUGE scores also generated barely-readable summaries.

![Salesforce ROUGE performance](/public/rl-hard/salesforce_rouge.png)
{: .centered }

Although the RL model performed best on ROUGE, the model actually used is the
one trained with both supervised and reinforcement learning.
Again, if your reward function doesn't reflect something you care about,
there's no guarantee the learned model will care about it either.

Here's another example. In [this paper from DeepMind](https://arxiv.org/abs/1704.03073),
the authors use DDPG to learn a grasping policy.
The policy gets reward based on how high the block is, which is defined as
the z-coordinate of the bottom face.

The policy ended up learning this.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/8QnD8ZM0YCo?start=27" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

Now, clearly this isn't the intended solution...but RL doesn't care.
From the perspective of the learning algorithm, it did something, and got
positive reward, so it's going to keep doing that thing.

Anecdotally, I've heard several more horror stories. It's been known for a while
that black-box optimization has a tendency to lead to weird solutions. A classic
example is the time someone applied genetic algorithms to circuit design, and
[got a circuit where an unconnected logic gate was necessary to the final
design](https://en.wikipedia.org/wiki/Evolvable_hardware#Introduction).

![Circuit with crazy gates](/public/rl-hard/circuit.png)
{: .centered }

[From "An Envolved Circuit, Intrinsic in Silicon, Entwined with Physics"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.50.9691&rep=rep1&type=pdf)
{: .centered }

Reinforcement learning can be seen as a variant of black-box optimization,
and it has similar pitfalls. Anything that gives +1 reward is good, even if
the +1 reward isn't coming for the right reasons.

A coworker is teaching an
agent to learn how to navigate. The episode terminates whenever the agent
walks out of bounds. He didn't add any penalty if the episode terminates this
way. The final policy learned to be suicidal, because negative reward was
plentiful, positive reward was rare, and a quick death ending in 0 reward
was preferable to a long life with negative reward.

A friend is training a simulated robot to reach towards a point
above a table. It turns out the point was defined *with respect to the table*.
The policy learned to pound the table really hard, making it fall down *just so*
to make the goal point move to the end of the arm.

A researcher gives a talk about using RL to train a simulated robot hand to
pick up a hammer and hammer in a nail. Initially, the reward was defined just
by how far the nail is pushed into the hole. Instead of
picking up the hammer, the robot used its own limbs to punch the nail in.
So, they added a reward term to encourage picking up the hammer, and retrained.
They got the policy to pick up the hammer...but then it threw the hammer at the
nail instead of actually using it.

I admit these are all anecdotes, and I haven't seen the videos of the behavior.
However, none of it sounds implausible to me.
I've been burned by RL too many times to believe otherwise.

I know people who like to tell stories about paperclip optimizers. I get it,
I really do. But honestly, I'm sick of hearing those stories, because they
always speculate up some superhuman misaligned AGI to create a just-so story.
Doing that really shouldn't be necessary, when people have to deal with
present-day examples all the time.


Even Given a Reward Oracle, Local Optima Can Be Hard To Escape
==============================================================================

All the previous examples are commonly called "reward hacking", which to me
implies a clever, out-of-the-box solution that gives more reward than the
intended answer.

These are the exceptions. The much more common case is stupidity - poor
local optima that come from getting the exploration-exploitation tradeoff
wrong. Ones that lead to frustration and so, so much pain,

And oh do I have stories for this.

Here's one of my favorite videos. This is an implementation of NAF, learning
on the HalfCheetah environment.

<div class="centered">
<video controls="controls">
  <source type="video/mp4" src="/public/rl-hard/upsidedown_half_cheetah.mp4"></source>
  <p>Your browser does not support the video element.</p>
</video>
</div>

Now, from an outside perspective, this is really, *really* dumb. But we can
only say it's dumb because we have a bunch of prebuilt knowledge that tells
us running on your feet is better. RL doesn't know this! It sends actions,
and gets back states and rewards. That's it. Here's my best guess for what
happened.

* Through random exploration, it learned to fall forward. This gives a bit of
reward - moving forward is better than not moving at all.
* It keeps doing that because it got rewarded for it, and the behavior got
"burned into" the policy. Now it's falling forward consistently.
* Through random exploration, from that state it learns a backflip maneuver.
This gives more reward - backflipping forward is better than lying down.
* It keeps doing that because it got rewarded for it, and backflipping got
burned in too. Now it's backflipping consistently.
* Once the policy flips to its back consistently, it's too hard to learn
how to flip right side up. So, the easiest path to high reward
is to figure out how to move forward while while lying on your back.

And thus, this.

<div class="centered">
<video controls="controls">
  <source type="video/mp4" src="/public/rl-hard/upsidedown_half_cheetah.mp4"></source>
  <p>Your browser does not support the video element.</p>
</video>
</div>

Realistically though, I have no idea if my theory above is right. All I know
is that it didn't learn.

Here's another failed run, this time on the Reacher environment.

<div class="centered">
<video controls="controls">
  <source type="video/mp4" src="/public/rl-hard/failed_reacher.mp4"></source>
  <p>Your browser does not support the video element.</p>
</video>
</div>

In this run, the initial random weights tended to output highly positive or
highly negative action outputs. That made it easy to learn how to spin really
fast. It's easy to perpetuate this behavior - just make all your
neural network weights large in magnitude. It's hard to deviate from this policy
in a meaningful way - you have to take several exploration steps to stop spinning,
and until then your actions barely influence your state.
Meanwhile, this "spin really fast" policy is actually not that awful -
in every revolution, the endpoint is guaranteed to pass by the target location.

CHANGE ABOVE

Combine all of this together, and it never learns to stop spinning really fast.

These are all examples of the classic exploration-exploitation problem that has dogged
reinforcement learning since time immemeorial.
Your data comes from your current policy. If your current policy explores too
much you get junk data and learn nothing. Exploit too much and you burn-in
behaviors that aren't optimal.

There are several intuitively pleasing ideas for addressing this, and many of
them have been attempted recently - intrinsic motivation, curiosity-driven
exploration, count-based exploration, and so forth. However, none of these have
worked consistently for all environments,
and I'm skeptical a similar solution will be figured out anytime soon. The
problem is really, really, really, really, really hard. To quote the
[Wikipedia article on multi-armed bandits](https://en.wikipedia.org/wiki/Multi-armed_bandit),

> Originally considered by Allied scientists in World War II, it proved so intractable that, according to Peter Whittle, the problem was proposed to be dropped over Germany so that German scientists could also waste their time on it.

http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.57.1916&rep=rep1&type=pdf

I've taken to imagining reinforcement learning as a demon that's
deliberately misinterpreting your reward and actively searching for local optima.
Whenever your reward function gives small reward for stupid behavior, there's always a
chance learning just gets completely stuck at the wrong thing. It's just
how deep RL is right now.


Even When Deep RL Works, It May Just Be Overfitting to Weird Patterns In the Environment
==============================================================================

In reinforcement learning, your train set is your test set. The upside is that if you want to do
well in an environment, you're free to overfit like crazy. The downside is that
if you want to generalize to any other environment, you're going to overfit like
crazy.

DQN can solve a lot of the Atari games, but there's little expectation
that an agent trained on one Atari game will be a good initialization for
training on another Atari game. In contrast, if you're doing image classification,
pretrained features from a model trained on ImageNet should be one of the
first things you try, because they transfer really well. RL doesn't have
an equivalent "ImageNet for control" yet. (OpenAI Universe tried to solve
this, but from what I heard, it's just too hard to solve anything reliably
in it, due to the asychronous implementation.)

This is most obvious in multiagent environments.
In [this work](https://arxiv.org/abs/1711.02301), we studied a toy 2-player
game where there's a closed-form analytic solution for optimal play.
In one of our first experiments, we freeze player 1's behavior, then trained
player 2 with RL. By doing this, you can treat player 1's actions as part
of the environment. We trained player 2 against the optimal player 1, and
although RL can learn good strategies, we found it failed to generalize
to non-optimal players.

A [DeepMind paper](https://arxiv.org/abs/1711.00832) from NIPS showed a
similar result. Here, there are two agents
playing laser tag. The agents are trained with multiagent reinforcement
learning. To test generalization, they run the training with 5 random
seeds. Here's a video of agents that have been trained against one
another.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/8vXpdHuoQH8" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

As you can see, they learn to move towards and shoot each other. Then, they
took player 1 from one experiment, and pitted it against player 2 from a
*different* experiment. If the learned policies generalize, we should see
similar behavior.

Spoiler alert: you don't.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/jOjwOkCM_i8" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
</div>

This seems to be a running theme in multiagent RL. When agents are trained
against one another, a kind of co-evolution happens. The agents get really good
at beating each other, but as soon as they get deployed against a player they
haven't seen before, performance goes to shit. Note
that the only difference between these videos is the random seed. Same learning
algorithm, same hyperparameters, and the divergence is purely from randomness
in initial conditions.

Self-play doesn't seem to have this problem. Somehow, controlling both players
with the same neural net gets around this problem.


Even Ignoring Generalization Issues, The Final Results Can be Unstable and Hard to Reproduce
========================================================================

Almost every ML algorithm has hyperparameters, which influence the behavior
of the learning system. Often, these are picked by hand, or by random search.

Supervised learning is stable. Fixed dataset, ground truth targets. If you
change the hyperparamters a little bit,
your performance won't change that much. Not all hyperparameters perform
well, but it's relatively easy to see signs of life. These signs of life are
super important, because they tell you that you're on the right track, you're
doing something reasonable, and it's worth investing more time.

Reinforcement learning, on the other hand, isn't stable at all. It is 
pathologically unstable.

When I started working at Brain, one of the first
things I worked on was reproducing the results of a continuous RL paper.
Specifically, I was trying to reproduce the Normalized Advantage Function
paper.

After about a month of failures, I finally solve my first task: the Pendulum
task from OpenAI Gym. If you're unfamiliar with this task, it's very simple.
There's a pendulum. There's gravity. The pendulum hangs from a point.
Each timstep, you can apply some acceleration around that point. You get
more reward the closer the pendulum is to the top, with maximum reward if
you can balance the pendulum.

Below is a video of a run that *mostly* works. Although the policy doesn't
balance straight up, it learns to output the exact torque needed to counteract
gravity.

<div class="centered">
<video controls="controls">
  <source type="video/mp4" src="/public/rl-hard/pendulum_example.mp4"></source>
  <p>Your browser does not support the video element.</p>
</video>
</div>

This is the easiest continuous control task in OpenAI Gym - the action space
is 1-dimensional, the state space is 3-dimensional, and you have a shaped reward
that's giving you fine-grained feedback on how close you are to the top.

Why did this take me a month? Well, I was partly learning how TensorFlow worked.
I also had a bunch of bugs that took a while to pin down.
Now, when I finally get it to work, here are the plots of performance.
This is 10 independent
runs, with the exact same hyperparameters. The only difference is the random seed.

![Graph of Pendulum results](/public/rl-hard/pendulum_results.png)

Now, I did solve the environment, but 30% of the time, it didn't learn at all.
This is just from different random seeds! Look, there's variance in supervised
learning too, but if models failed to learn anything at all 30% of the time, it
would be a clear signal of an implementation bug in data loading or training.
In reinforcement learning? It's Tuesday.

<div class="centered">
<iframe width="560" height="315" src="https://www.youtube.com/embed/iVzAMmpMra8" frameborder="0" allowfullscreen></iframe>
</div>

Here's a plot from the [Variational Information Maximizing Exploration paper](https://arxiv.org/abs/1605.09674).
The environment is HalfCheetah. The reward is modified to be sparser, but the
details aren't too important.
The y-axis is episode reward, the x-axis is number of timesteps, and the
algorithm used is TRPO.

![Plot from VIME paper](/public/rl-hard/vime.png)
{: .centered }

The dark line is the median performance over 10 random seeds, and the shaded
region is the 25th to 75th percentile. Don't get me wrong, this plot is a good
argument in favor of VIME. But on the other hand, the 25th percentile line
is really close to 0 reward, meaning about 25% of runs are failing for no
reason.

> If it makes you feel any better, I've been doing this for a while and it took me last ~6 weeks to get a from-scratch policy gradients implementation to work 50% of the time on a bunch of RL problems. And I also have a GPU cluster available to me, and a number of friends I get lunch with every day who've been in the area for the last few years.
>
> Also, what we know about good CNN design from supervised learning land doesn't seem to apply to reinforcement learning land, because you're mostly bottlenecked by credit assignment / supervision bitrate, not by a lack of a powerful representation. Your ResNets, batchnorms, or very deep networks have no power here.
>
> [Supervised learning] wants to work. Even if you screw something up you'll usually get something non-random back. RL must be forced to work. If you screw something up or don't tune something well enough you're exceedingly likely to get a policy that is even worse than random. And even if it's all well tuned you'll get a bad policy 30% of the time, just because.
>
> Long story short your failure is more due to the difficulty of deep RL, and much less due to the difficulty of "designing neural networks".

[Hacker News comment from Andrej Karpathy, back when he was at OpenAI](https://news.ycombinator.com/item?id=13519044)
{: .centered }

Instability to random seed is like a canary in a coal mine. If pure randomness
is enough to lead to these kinds of results, imagine how much an actual
difference in the code could make.

Luckily, we don't have to imagine, because this was inspected by
the recent [Deep Reinforcement Learning That Matters](https://arxiv.org/abs/1709.06560)
paper. It makes a bunch of points that I think were well-known in the RL
community, but which weren't packaged together until now. Among its conclusions
are:

* Multiplying the reward by a constant can give significant differences in performance.
* Five random seeds (a common reporting metric) may not be enough to argue
significant results.
* Different implementations of the same algorithm have different performance on
the same task.

My theory is that RL is very sensitive to both your initialization and to the
dynamics of your training process, because your data is always collected online
and the only supervision you get is a single scalar for reward. A policy that
randomly stumbles onto good training examples will bootstrap itself much
faster than a policy that doesn't. A policy that fails to discover good training
examples in time will collapse towards learning nothing at all, as it becomes
more confident that any deviation it tries will fail.


But What About All The Great Things Deep RL Has Done For Us?
==================================================================

Deep reinforcement learning has certainly done some very cool things. DQN is
old news now, but was absolutely *nuts* at the time. AlphaGo and AlphaZero
continue to be very impressive achievements.
However, outside of these game playing successes,
deep RL hasn't done much that has practical real world value.

I tried to think of a real-world, productionized use of deep RL, and I could
only think of two things: [the project to reduce the power used by
Google's data centers](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/),
and [the project to automatically design neural net architectures](https://research.googleblog.com/2017/11/automl-for-large-scale-image.html).
Jack Clark (Strategy & Comms Director at OpenAI)
[tweeted a request for more examples](https://twitter.com/jackclarkSF/status/919584404472602624),
and I just didn't find that much in the replies.

I assume finance companies have tried using RL, but so far there's nothing
definitive about it. Of course, finance companies have good reasons to be cagey
about how they play the market, so perhaps the evidence there is never going to
be strong.

**Overall, for all of its hype and promise, deep RL is still decidedly a research
topic.** If you came to me with an image classification, I'd point you to
pretrained ImageNet models, and I'd have high confidence that the learning
would work.
If [the people behind *Silicon Valley* can make Not Hotdog](https://medium.com/@timanglade/how-hbos-silicon-valley-built-not-hotdog-with-mobile-tensorflow-keras-react-native-ef03260747f3),
then you can too. Right now, I can't say the same about reinforcement learning.
There's too much that could go wrong.


Alright. In What Settings Could Deep RL Do Something Great For Me?
=====================================================================

A priori, it's really hard to say. The problem with trying to solve everything
with RL is that you're trying to solve several very different environments
with the same approach. Until you try things out, it can be hard to judge
whether a given RL algorithm will work.

That being said, we can draw conclusions from the current list of deep
reinforcement learning successes.

Ignoring real-world value, here's my list of cool things deep RL has done.

* Ones from the previous section: DQN, AlphaGo, AlphaZero, the parkour bot,
reducing power center usage, Neural Architecture Search.
* OpenAI's Dota 2 1v1 Shadow Fiend bot, which beat top pro players in a
simplified duel setting.
* A Super Smash Brothers Melee bot from students at MIT and NYU, that can beat
pro players at 1v1 Falcon dittos.

Two machine learning systems,
[Libratus](https://www.ijcai.org/proceedings/2017/0772.pdf) and [DeepStack](https://arxiv.org/abs/1701.01724),
recently beat pro players at no-limit heads up Texas Hold'Em.
These are both very cool, but by my understanding, neither uses reinforcement
learning. They instead use counterfactual regret minimization and clever
iterative solving of subgames.



Taking each success above in turn, here would be my words of caution.

* **Atari**: As mentioned earlier, this required millions of frames of experience,
has score as a ground-truth reward function.
* **Go / Chess / Shogi**: All used millions of games, and all can employ
self-play, which has been shown to be a very useful property for learning.
* **Dota 2**: The Shadow Fiend bot was restricted to a small part of the Dota game
space. No bottle, few items, 1v1 laning only, and only Shadow Fiend.
The bot is likely using the [Dota 2 API](https://developer.valvesoftware.com/wiki/Dota_2_Workshop_Tools/Scripting/API)
to get the exact health of all enemies in sight. Gold and enemy health form
a rich, well shaped reward signal.
* **Power center PUE**: This built off of prior work from the Google data
center team, which showed neural nets could predict PUE with high accuracy.
Given a neural net with good predictions, it seems obvious that optimizing PUE
over the networks inputs is going to lead to good things.
* **Neural architecture search**: Validation accuracy of a network trained to
convergence is also a very rich reward signal - even if an action only increases
accuracy from 70% to 71%, RL will pick up on this. Empirically, hyperparameters
in deep learning tend to be close to linearly independent, and although NAS
isn't exactly tuning hyperparameters, it seems reasonable that there are clear
links between action choices and reward, which makes credit assignment
straightforward. Given all of this, working in "only" 2000 trained networks
doesn't seem so far-fetched.
* The SSBM bot shares many similarities to the Dota 2 setup - 1v1, Battlefield
only, infinite time match to avoid having to understand how stocks work,
Captain Falcon dittos only, gets to use self-play, and a rich reward based on
% dealt and received.

The broader point I want to get across is this: many things have to go right
for reinforcement learning to even be a plausible solution, and then it often
still requires a lot of careful work to make that solution happen. Perhaps
in the future, RL will be great at everything, but for now, there's a narrow
set of problems where RL is the right solution.

The same is true of reinforcement learning. It's not that reinforcement learning
is particularly good. It's more that the environments above are well-suited to
the strengths of reinforcement learning, and they tend to dodge its weaknesses.


Looking to The Future?
===============================================================================

There's an old saying - every researcher learns how to hate their area of
study. The trick is that researchers will press on despite this, because they
like the problems too much to quit them.

That's about where I'm at with deep reinforcement learning. Despite my reservations
and pessimism, I'm still interested in working in that area, keeping up with
what's going on, and wanting to see where it's all going. Notice that I said
"Deep Reinforcement Learning Doesn't Work *Yet*". I see no reason why it couldn't
work, given more time. To balance out the negativity, here are some ways I could
see things getting better.

**Local optima are good enough:** It would be very arrogant to claim humans are
globally optimal at anything. I would guess we're juuuuust good enough to get
to civilization stage, compared to any other species. In the same vein, an
RL solution doesn't have to achieve a global optima, as long as its local optima
is better than the human baseline.

**Hardware solves everything:** I know some people who believe that the most
influential thing that can be done for AI is simply scaling up hardware. Personally,
I'm skeptical that hardware will fix everything, but it's certainly going to
be important. The faster you can run things, the less you care about sample
inefficiency, and the easier it is to brute-force your way past exploration
problems.

**Add more learning signal:** Sparse rewards are hard to learn because you get
very little information about what thing help you. It's possible we can either hallucinate
positive rewards (HER), define auxiliary tasks (UNREAL), or do predicitive
learning to build a world model from transitions that give 0 reward.

**Reward functions could be learnable:** The human preferences paper suggests that
it's possible for neural nets to use human feedback to implicitly define a
reward signal that encourages partial progress for a task. In some cases, this
outperformed the existing reward definitions. It's possible we
could extend this with other parts of inverse RL / imitation learning to get
past many of the difficulties that come from explicitly defining reward.

**Priors from metalearning could speed up learning elsewhere:**
RL algorithms are designed to apply to any environment that's an MDP. However,
we don't need to solve arbitrary environments, we just need to solve
environments in the real-world. In principle, we could use metalearning to learn
a real-world prior that lets us quickly learn new real-world tasks, at the cost
of general learning ability. (This is a point Pieter Abbeel likes to mention
in his talks.) For example, if you wanted to use RL to do navigating in
warehouses, there's probably a way to use metalearning to learn a good
navigation prior,

**Reproducability and wild overfitting don't matter for the problem:**
From a research perspective, this is disappointing, but there are several
settings where you might not care how easy it is to reproduce the same results,
as long as it works. Suppose you train 3 ad-serving models, only changing the
random seed. In live A/B testing, one gives 2% less revenue, one performs the
same, and one gives 2% more revenue. In that hypothetical, you don't throw out
the results - you keep the model with 2% more revenue and celebrate your new
earnings. Similarly, suppose those ad-serving models were trained primarily
on clicks logged in the United States. It might not generalize to worldwide
markets, but that doesn't matter - just don't deploy it to other countries.

**Harder environments could paradoxably be easier:** One of the big lessons
from the DeepMind parkour paper is that if you make your task very difficult
by adding several variations, you can sometimes actually make the learning
easier, because the policy cannot overfit to any one setting without losing
performance on all the other settings. We've seen a similar thing in the
domain randomization papers, and even back to ImageNet: models trained on
ImageNet will generalize way better than ones trained on CIFAR-100. An
analogous "ImageNet for control" could fix this. (OpenAI tried to spur this
with Universe, but it didn't get adopted. From what I heard, the environments
were just too hard to solve. However, the core premise is sound and I'd like
to see another attempt when RL is more performant.)

Overall, although I spent a lot of this post being pessimistic about where
reinforcement learning *is*, I still believe in where reinforcement learning
*could be*.
