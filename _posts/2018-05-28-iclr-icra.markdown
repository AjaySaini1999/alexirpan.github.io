---
layout: post
title:  "Thoughts On ICLR 2018 and ICRA 2018"
date:   2018-05-27 15:38:00 -0700
---

In the span of just under a month, I attended two conferences,
[ICLR 2018](https://iclr.cc/) and
[ICRA 2018](https://icra2018.org/). The first is a deep learning conference,
and the second is a robotics conference. They were pretty different, and I
figured it would be neat to compare and contrast the two.

My memory for conferences is shockingly short, so this is all reconstructed
from notes and pictures I took of research posters. The problem with this
strategy is that the reconstruction quality depends on how sleepy I was,
and whether I remembered to take notes for something or not.


ICLR 2018
=======================================================================

From the research side, the TL;DR of ICLR was that adversarial learning
continues to be a big thing.

The elephant in the room is GANs. Generative adversarial networks have been
popular for a few years, but I'm casting a wide umbrella here, one that
includes adversarial examples, multiagent environments where agents compete,
and general minimax optimization problems of the form $$\min_x \max_y f(x, y)$$.

There's a good chance this is selection bias.
I like adversarial learning a lot.
One way to describe a GAN is that the discriminator is learning an implicit
cost function to help train the generator.
In an abstract sense, this adversarial approach lets you change from a
fixed, human defined cost function, to a learned cost function that can adapt
to the capabilities of whatever you're trying to learn. In the GAN case,
you have a dynamic cost over image generation. But in a learning-by-self-play
case, you get dynamic costs over learning good agent behavior.

This does make your problem more complicated. One advantage
is that if there are advances in optimization or modeling ability, it can
improve both your ability to learn cost functions and your ability to minimize
those learned costs. The hope is that you eventually hit a tipping point where
it's *worth* spending a bunch of time doing adversarial learning.

From an even more abstract view, this touches on
the power of expressive, optimizable function families, like neural nets.
Minimax optimization is not a new idea, it's been around for ages. The new thing
is that deep learning enables learning cost functions on high-dimensional data.

There are other parts of the learning process that could be replaced with
learned nets instead of human-defined behavior. Does it make sense to do this.
Well, *maybe*. We'll see.

There was a recent [Quanta article](https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/),
where Judea Pearl expressed his disappointment that deep learning was just
learning correlations and curve fitting, that this doesn't cover all of
intelligence, and that the next step for AI is to learn cause and effect and
do causal inference. I agree with this, but it's possible that if you
throw enough super-big neural nets into a big enough vat of optimization soup,
you could learn something that looks like causal inference, or whatever else you want to
count as intelligence. But now we're in somewhat-unfounded philosophy land,
so I'll stop here.

PARAGRAPH BELOW CUT

I do worry a bit about this research trend for robustness reasons.
My intuition is that adversarial methods have more variance in performance,
because the alternating minimization makes your learning dynamics a lot less
predictable. Now, this isn't a deal breaker. It just depends
on the mean performance of adversarial methods. If they work a lot better on average,
then they'll outperform other methods on average, even if there's variance.

PARAGRAPH ABOVE CUT

From an attendee perspective, I liked having a lot of poster sessions. This is
the first time I've gone to ICLR. My previous ML conference was NIPS, and NIPS
just feels ridiculously large. Checking every poster at NIPS doesn't feel doable.
Checking every poster at ICLR felt *possible*, although whether you'd actually
want to do so is questionable.

I also appreciated that corporate recuriting didn't feel as ridiculous as NIPS.
At NIPS, companies were giving out fidget spinners and slinkies, which was *unique*,
but the fact that companies needed to *come up with unique swag to stand out*
felt...strange. At ICLR, the weirdest thing I got was a pair of socks.

Papers I noted to follow-up on later:

* Intrinsic Motivation Self Play
* Robust Adversarial Inverse RL
* Policy Optimization by Genetic Distillation
* Bayesian Optimization and HYperband
* The Intrinsic Dimension of Objective Landscapes
* Eigenoption Discovery Eigenpurposes as Intrinc Rewward
* Self-Ensembling for Visual Domain Adaptation
* To TD or not to TD
* Hypergradient optimization
* Action Selection Biasing, Knowledge GAin Estimates, exmploration MDP


ICRA 2018
================================================================================

ICRA 2018 was my first robotics conference. I wasn't sure what to expect. I
started research as an ML person, and then sort of fell into robotics on the
side, so my interests are closer to learning-for-control instead of
making-new-robots. My ideal setup is one where I can run models on a real-world
robot, but the details of the real-world hardware / low-level software stack
are an abstraction. (Somewhere, a roboticist weeps.)

This meant I was unfamiliar with a lot of the topics, so less of the conference
appealed to me. Still, there were plenty of learning papers, and I'm glad I
went.

From the research side, it was interesting to see how many RL papers there
were. It was also interesting to see that almost none of those papers used
purely model-free RL. One thing about ICRA is that your paper has a much, much
better chance of getting accepted if it runs on a real-world robot. That
forces you to care about data efficiency. I kept seeing "We combine model-free
reinforcement learning with X", where X was model-based RL, or learning from
human demonstrations, or learning from existing motion planning solutions,
and so on.

At a broader level, it felt like there was a sense of practicality about a lot
of the research. This being research, plenty of it was still very speculative,
but it felt like another consequence of having to work with real hardware. You
can't ignore inference time if you need to run your model in real time. You
can't ignore data efficiency if you need to collect it from a real robot.
It has to work, and the real world doesn't care about a lot of things.

This surprises a lot of ML people I talk to, but robotics hasn't fully embraced
ML the way that people at NIPS / ICLR / ICML have. For a lot of people,
machine learning is just a solution that may make sense. The impression I got
was that only a few people actively wanted ML to fail, and everyone else wanted
it to prove itself first. Every perception paper I saw used CNNs in one way
or another, but significantly fewer people were using deep learning for control,
because that's the part that's uncertain. It was good to hear comments from
people who see deep learning as just a fad (even if I don't agree!).

It was also interesting to see a bit more emphasis on the consumer. Rodney
Brooks had a neat talk about the design decisions behind turning Roomba into
a product, pointing out that a couple hundreed dollars gives you very little
leeway for fancy sensors and hardware. (He also had a tangent rant about
HRI research, which felt a bit out of place, but hey, sometimes you have to
rant.)

Papers I noted to follow-up on later:

* Applying Async Deep Classifcation Networks and Gaming RL BASED MOtion Planners
* OptLayer - Practice Constrain Optimization
* Synthetically Trained NN for Learning HUman-REadable PLans
* Semantic Robo Programming for Goal-Directed Manipulaion
* Interactive Perception

