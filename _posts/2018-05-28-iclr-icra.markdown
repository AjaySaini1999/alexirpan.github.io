---
layout: post
title:  "Thoughts On ICLR 2018 and ICRA 2018"
date:   2018-05-27 15:38:00 -0700
---

In the span of just under a month, I attended two conferences,
[ICLR 2018](https://iclr.cc/) and
[ICRA 2018](https://icra2018.org/). The first is a deep learning conference,
and the second is a robotics conference. They were pretty different, and I
figured it would be neat to compare the two.


ICLR 2018
=======================================================================

From the research side, the TL;DR of ICLR was that adversarial learning
continues to be a big thing.

The most popular thing in that sphere would be generative adversarial
networks. However, I'm casting a wide umbrella here, one that
includes adversarial examples and environments with competing agents.
Really, any minimax optimization problems of the form
$$\min_x \max_y f(x, y)$$ counts as adversarial learning to me.

I don't know if it was actually popular, or if my memory has selective
bias, because I have a soft spot for these approaches. They feel powerful.
One way to view a GAN is that you are learning a generator by using a learned
implicit cost instead of a human defined one.
This lets you adapt to the capabilities of your generator and lets you define
costs that could be cumbersome to explain by hand.

Sure, this makes your problem more complicated. But if you have strong
enough optimization and modeling ability, the implicitly learned cost gives you
sharper images than other approaches. And one advantage of replacing parts
of your system with learned components is that advances in optimization and
modeling ability apply to more aspects of your problem.
You are improving both your ability to learn cost functions and your ability to
minimize those learned costs. Eventually, there's a tipping point where it's
worth adding all this machinery.

From a more abstract viewpoint, this touches on
the power of expressive, optimizable function families, like neural nets.
Minimax optimization is not a new idea. It's been around for ages. The new
thing that deep learning lets you model and learn complicated cost functions
on high-dimensional data. To me, the interesting thing about GANs isn't the
image generation, it's the proof-of-concept they show on complicated data
*like* images. Nothing about the framework requires you to use image data.

There are other parts of the learning process that could be replaced with
learned methods instead of human-defined one, and deep learning may be how
we get there. Does it make sense to do this? Well, *maybe*. The problem is that
the more you do this, the harder it becomes to actually make everything
learnable.

There was a recent [Quanta article](https://www.quantamagazine.org/to-build-truly-intelligent-machines-teach-them-cause-and-effect-20180515/),
where Judea Pearl expressed his disappointment that deep learning was just
learning correlations and curve fitting, that this doesn't cover all of
intelligence, and that the next step for AI is to learn cause and effect and
do causal inference. I agree with this, but there is a chance that if you
throw enough super-big neural nets into a big enough vat of optimization soup,
you would learn something that looks a lot like causal inference, or whatever
else you want to count as intelligence.
But now we're rapidly approaching philosophy land, so I'll stop here and move
on.

From an attendee perspective, I liked having lots of poster sessions. This is
the first time I've gone to ICLR. My previous ML conference was NIPS, and NIPS
just feels ridiculously large. Checking every poster at NIPS doesn't feel doable.
Checking every poster at ICLR felt *possible*, although whether you'd actually
want to do so is questionable.

I also appreciated that corporate recuriting didn't feel as ridiculous as NIPS.
At NIPS, companies were giving out fidget spinners and slinkies, which was *unique*,
but the fact that companies needed to *come up with unique swag to stand out*
felt...strange. At ICLR, the weirdest thing I got was a pair of socks. It was
certainly odd, but not too outlandish.

Papers I noted to follow-up on later:

* [Intrinsic Motivation and Automatic Curricula via Asymmetric Self-Play](https://openreview.net/forum?id=SkT5Yg-RZ)
* [Learning Robust Rewards with Adverserial Inverse Reinforcement Learning](https://openreview.net/forum?id=rkHywl-A-)
* [Policy Optimization by Genetic Distillation](https://openreview.net/forum?id=ByOnmlWC-)
* [Measuring the Intrinsic Dimension of Objective Landscapes](https://openreview.net/forum?id=ryup8-WCW)
* [Eigenoption Discovery Through the Deep Successor Representation](https://openreview.net/forum?id=Bk8ZcAxR-)
* [Self-Ensembling for Visual Domain Adaptation](https://openreview.net/forum?id=rkpoTaxA-)
* [TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning](https://openreview.net/forum?id=HyiAuyb0b)
* [Online Learning Rate Adaptation with Hypergradient Descent](https://openreview.net/forum?id=BkrsAzWAb)
* [DORA The Explorer: Directed Outreaching Reinforcement Action-Selection](https://openreview.net/forum?id=ry1arUgCW)
* [Learning to Multi-Task by Active Sampling](https://openreview.net/forum?id=B1nZ1weCZ)


ICRA 2018
================================================================================

ICRA 2018 was my first robotics conference. I wasn't sure what to expect. I
started research as an ML person, and then sort of fell into robotics on the
side, so my interests are closer to learning-for-control instead of
making-new-robots. My ideal setup is one where I can run models on a real-world
robot, but the details of the real-world hardware / low-level software stack
are an abstraction. (Somewhere, a roboticist weeps.)

This meant I was unfamiliar with a lot of the topics, so less of the conference
appealed to me. Still, there were plenty of learning papers, and I'm glad I
went.

From the research side, it was interesting to see how many RL papers there
were. It was also interesting to see that almost none of those papers used
purely model-free RL. One thing about ICRA is that your paper has a much, much
better chance of getting accepted if it runs on a real-world robot. That
forces you to care about data efficiency. I kept seeing "We combine model-free
reinforcement learning with X", where X was model-based RL, or learning from
human demonstrations, or learning from existing motion planning solutions,
and so on.

At a broader level, it felt like there was a sense of practicality about a lot
of the research. This being research, plenty of it was still very speculative,
but it felt like another consequence of having to work with real hardware. You
can't ignore inference time if you need to run your model in real time. You
can't ignore data efficiency if you need to collect it from a real robot.
It has to work, and the real world doesn't care about a lot of things.

This surprises a lot of ML people I talk to, but robotics hasn't fully embraced
ML the way that people at NIPS / ICLR / ICML have. For a lot of people,
machine learning is just a solution that may make sense. The impression I got
was that only a few people actively wanted ML to fail, and everyone else wanted
it to prove itself first. Every perception paper I saw used CNNs in one way
or another, but significantly fewer people were using deep learning for control,
because that's the part that's uncertain. It was good to hear comments from
people who see deep learning as just a fad (even if I don't agree!).

I also liked the talks where researchers talked about their partnerships with
comapnies. At research conferences, the focus is on discussing research and
networking with other researchers, so it's easy to forget that these technologies
can produce clear economic value. I went to a talk about robotics in agriculture,
where they used computer vision to detect weeds, which lets you autonomously
spray weed-killer on only the plants that need to die. Rodney Brooks also
had a neat talk about the design decisions behind the original Roomba. He pointed
out that a couple hundreed dollars gives you very little leeway for fancy
sensors and hardware, which places tight limits on what you can do on-device.
(He also had a rant criticizing
[HRI research](http://humanrobotinteraction.org/). My recollection is that he
thought people were using too much notation to hide simple ideas, that sample
sizes for human studies were too small for the field's claims, and that the
models of human behavior weren't accurate enough to be useful.
It felt out of place, but hey, sometimes you have to rant.)

Papers I noted to follow-up on later:

* [Applying Asynchronous Deep Classification Network and Gaming Reinforcement Learning-Based Motion Planner to a Mobile Robot](http://ghryou.me/assets/pdf/ghryou_icra_2018.pdf)
* [OptLayer - Practical Constrained Optimization for Deep Reinforcement Learning in the Real World](https://arxiv.org/abs/1709.07643)
* [Synthetically Trained Neural Networks for Learning Human-Readable Plans from Real-World Demonstrations](https://arxiv.org/abs/1805.07054)
* [Semantic Robot Programming for Goal-Directed Manipulation in Cluttered Scenes](https://www.youtube.com/watch?v=kOcdqUmXRRo)
* [Interactive Perception: Leveraging Action in Perception and Perception in Action](https://arxiv.org/abs/1604.03670)
