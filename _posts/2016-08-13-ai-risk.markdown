---
layout: post
title:  "\"What's Your Opinion on AI Risk?\""
date:   2016-08-13 18:52:00 -0700
---

*These are my personal opinions, and do not reflect the opinions of
anybody I've worked with. AI risk is a touchy subject, so I want to
be very clear about this.*

I went to Effective Altruism Global this year, which was pretty great.
I wasn't sure I would fit into the effective altruist culture, since I
didn't count myself as part of EA, but it turns out I do fit in, and
EA attracts a lot of interesting people.

Now, I'm bad at starting conversation, but starting conversation at
EA Global is easy. All you have to do is ask what they do. And that
led to the following exchange, which I probably repeated about 10 times
over the entire weekend.

> Them: "What do you do?"
>
> Me: "Well, I just finished my undergrad at Berkeley. I'm now working
> at Google Brain, a machine learning research group."
>
> Them: "That's really cool! So, just wondering...(what's your opinion
> on the existential risk on AI/what's your timeline for AGI)?"


DROP THE ABOVE.


In the mindscape, an AI safety supporter and AI safety detractor meet
around a wooden table. Detractor pulls his arms forwards, and in one
smooth motion sweeps the stacks of paper and books onto the ground.

"Must you be so dramatic?"

"Oh be quiet. You've wanted to do that for ages. After all, we're just
aspects of the same person."

Supporter shrugs. "Fair. Why are we even framing this as a conversation?
Why can't we just write an essay, like a normal person?"

Detractor raises a hand, pointing an index finger towards the metaphorical
sky as he counts off the ways. "One. This isn't something I've figured
out yet. It's something I'm in the middle of figuring out. So, I need to
explain my contradictory feelings, and that's easier if I anthropomorphize
the two sides.

"Two. It's a writing challenge. Restrictions breed creativity. Forcing the
argument into a dialogue should learn to interesting results, and we need
the writing practice.

"And three? Well, that's a secret. But we both know the reason."

Supporter pauses for a bit, then shrugs. "Alright. Sounds fair. Let's do this."

"Let's."

Part 1: Laying Out the Ground Rules
===========================================================

Supporter and Detractor sit down at the table.

"Let's start from the beginning. Being as generous to the concerns of AI safety
as possible, is AGI potentially dangerous?"

The answer comes quickly. "Yes. If intelligence explosion turns out to be true,
and AI is created, and it can self-improve, then yes, it could be dangerous.
And to be clear, for now we're talking about killer robots ala Skynet."

"Okay. Well that was fast."

Part 2: Pulling Down the Activation Barrier
==========================================================

"Now, malevolent AI that is intentionally killing humans is bad. But that's not
what people worried about AI really mean."


