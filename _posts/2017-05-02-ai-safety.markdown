---
layout: post
title:  "Sometimes I Think About AI Safety and I Get Depressed For a Bit"
date:   2017-05-02 01:14:00 -0700
---

I've been trying to write this post for almost a year.

That doesn't mean I've been actively writing that whole time. It's more
like a constant nagging feeling, that refuses to go away.

I wrote a draft 6 months ago (CHECK TIME), then erased the whole thing out
of disgust. I have notes jotted down 8 months ago that I don't fully endorse
anymore.

Everything's jumbled, a giant cacophony of latent beliefs and hazy thoughts.
At this point, I just want it to be *done*, because I'm sick of bottling
up my monologues. The problem is that my opinions on the subject are tied
to so much of my life that it's hard to look at them, even subjectively.

I can't even write this unless I make the following very, very clear.

* These are my opinion as of the time of this post (May 29, 2017), and
my feelings are very likely to change.
* These are my personal opinions - they do not reflect the opinions of
anybody I have worked with or any organization that has employed me.
* No, seriously, **these opinions are mine.** If you don't realize how
sensitive talking about AI safety is...well, hopefully I can explain it.

\* \* \*
{: .centered }

The core premise of AI safety discussion is that artificial intelligence
is a transformative technology, that could either improve everyone's
lives, or perpetuate inequality, and in the worst case scenarios, end
the human race.

This is a controversial topic, for obvious reasons, and an annoying
topic to discuss, for less obvious reasons.

To me, it's been incredibly frustrating to read conversations about AGI.
They feel broken, in ways that make me actively avoid joining in.

The thing about AI is that it captures the imagination. At times it feels
like everyone has an opinion on what "intelligence" means, whether it
can really be created artificially, whether AGI is possible or not.
No wonder everyone says they do machine learning instead of AI - it's
less of a buzzword. (Although that's not saying much.)

So on one hand, there's a group of people who aren't very informed on the
field. On any article about AI, they bring up Skynet, Terminators, or HAL.
It gets old really fast, but I'm generally fine with this. Not everyone
is going to understand machine learning, and I broadly don't expect
uninformed people to know any better. In the same vein, I recognize that
my opinion on literary theory, biology, and any chemistry/physics past
the high school level is complete, utter garbage.

On the other hand, there are the debates about the existential risk of AI,
usually from people in some union of the effective altruism and rationalist
communities, and those debates are where most of my frustration comes from.

Broadly, it's because it feels like they should know better. It feels like
a lot of the discussion is at a high level, uses imprecise definitions to
make broad claims, and to me both those communities pride themselves on
carefully considering all sides of the argument - except, seemingly, for
AI safety.

I know this isn't a fair portrayal of the community. That's part
of the problem.

\* \* \*
{: .centered }

Like a lot of other people, I was introduced to the community
through Harry Potter and the Methods of Rationality. I was a bored
high school kid, and I liked Harry Potter, so it just kind of happened.
Ever since then, I've been circling the community - first by reading the
occasional LessWrong post, and nowadays through whatever shows up in
my Facebook feed.

Around that time was when I first started reading some arguments people had
about superhuman AI, that felt like neat stories, but didn't feel like things
I needed or wanted to care about.

But nowadays, when people talk about parables of AI safety, I just get
sick of it.

By this point I've read at least three "rationalist fanfics" that were basically
all AI safety parables. Oh look, you tell an AI to make lots of paperclips and
it invents nanotech and converts all matter to paperclips. Hence, paperclip
optimizer. Oh look, you tell an AI to write lots of notes and it hides its
intelligence to fool people into thinking it's not very smart, then kills the
human race with neurotoxin to stop it from writing lots of notes. Bam, the
scenario from Tim Urban's Wait but Why. Oh look, you tell an AI to
"maximize happiness of the game's players" and it manipulates all the humans until
they play the game. Oh look, Santa's elves are real, and if you ask them to make
people happen they create wireheading and have to be talked out of it by
The Sane Person Who Knows This Is a Bad Idea. *I get it, I got it the first
time.* And I know you're telling these parables because you think the ideas
are important, and I know some people are reading about paperclip optimizers
for the first time, but isn't it natural to get sick of paperclip optimizers
the 5th time through?

I watched Ex Machina because people hyped it up as great. It was a good movie,
but not for me. To me it felt like it was presenting a bunch of old ideas
as if they were new. I didn't need to hear Yet Another Debate About The Chinese
Room Thought Experiment because *I really don't care.*

Really, that's the crux of the problem I have about EAs and rationalists.
The organizing principle of EAs is that you can have much larger impact if
you donate according to careful argument instead of what feels right.
The organizing principle of rationalists is that the human brain is horribly
horribly broken, and it's worth trying to optimize your life, even if you're
never going to be perfect.

But in practice, it feels like the organizing principle is for people to
endlessly debate the merits of different causes, different organizations,
and the right way to live your life. That's fine, but sometimes I feel like
people are more interested in philosophy than the original question.
I can understand why. Philosophy is fun in moderation, but not much more than
that.

An aggressive reductionist desire to categorize and name everything about
the world. (It's not a coincidence that people who do math end up intersecting
in these circles a lot - all of math runs on this kind of thinking.)


\* \* \*
{: .centered }

Depending on the circumstance, I can pass as an EA or as a rationalist.
If I want people to think I'm an effective altruist, 
I talk about the marginal utility of money, or Against Malaria Foundation,
or animal suffering. If I want people to think I"m a rationalist, I can
play up cognitive biases, and talk about different x-risks, or ways
to exploit comparative advantages in the market.

The thing is, I'm neither? I'm not an effective altruist, and I'm not
a rationalist.

"But, you went to EAGxBerkeley, then went to EA Global, then went to
a Slate Star Codex meetup hosted at the CFAR office, *and then went
to a CFAR workshop*. How are you *not* an EA or a rationalist?"

And I mean, that's all true, but when I say I'm not an EA, and not
a rationalist, it feels right.
The thing is, I don't consider myself to be an effective altruist, and
I don't consider myself to be a rationalist. I'm merely a bystander to
both movements.

\* \* \*
{: .centered }

Throughout this essay, I'll use "the strawman rationalist". The strawman
rationalist will be denoted by *italics*. The strawman rationalist is not
fair. It does not match any self-described rationalist I know. It's more
like an amalgam of all the negative rationalist stereotypes I can think of.

Now, given how awful the strawman rationalist is, why do I keep him around?
He's always smug, always insufferable, always tries to find a new reason
to ignore arguments that machine learning could possibly end okay, and believesg[M Â™^
that he and his cohort are the only people who are doing anything meaningful
to protect people. To the strawman, everyone else is a well-meaning but
misguided idiot, but I'm not, oh no I'm not, because I've read the Sequences!

I keep him around because despite how crazy he is, he still manages to give
good insights.

The rough arguments for AI safety are

* At some point, there will be an AI that can do any task better than
a human can.
* There is some chance such an AI can repeatedly discover new technological
advances, and apply those advances to itself to improve itself at an
astonishing rate.
    * This doesn't have to be exponential, it just has to be fast enough.
    * It's assumed that a single
    technological advance follows a sigmoid curve - there is a slow increase
    as the technology is tested, then a rapid period of innovation, then a
    slowdown as people realize the limits of the technology. The argument
    is that if you achieve a lot of technological advances quickly, a
    chain of iterated sigmoids starts to approach a near-expoential curve.
* A human will eventually ask such an AI to do something.
* Almost all things are easier if you can think faster, so the AI will
decide to improve itself as much as it can.
* Almost all things are easier if you're alive, so the AI will decide to
minimize the chance anybody disables it.
* Therefore, if a sufficiently smart AI is asked to do something, it may
decide to kill all humans.

But really, this is a long chain of reasoning that can be better summarized
this way.

* Blindly optimizing an objective can lead to lots of unexpected behavior.
* When humans optimize for something, they usually follow some implicit
ethical rules.
* An AI that optimizes for something may not have those implicit ethics,
leading to lots of unexpected behavior, some of which could include
killing all humans.

My broad view at this time is that the argument behind AI safety is worth
taking seriously. MIRI's agenda seems focused on exclusively thinking
about ways to constrain superhuman systems, under the thesis that it's a very
different problem from contraining current ML systems. I'm skeptical any of
MIRI's work will end up being useful down the line, but that doesn't mean
I want them to stop.

What I do want to stop, however, is the vaguely adversarial way people on both
sides have framed the AI safety debate. It's not overt or anything, more like
a collection of snide remarks and jokes I hear now and then. A person from MIRI,
saying that at least 2 professors take them seriously, one of whom is
well-respected. Another person from MIRI, joking that giving DeepMind tons
of compute power may have been a bad idea. An effective altruist, contrasting
the money going into AI vs the money going into AI safety. Two people have
independently joked that if AGI is imminent, maybe we need kill all the AI
researchers, which really stops being funny when you're one of those
researchers.

On the flip side, a lot of established ML people have criticized people
for bad reasons. They have weird opinions on some basilisk thing,
why should I listen to these people. He's a professor of philosophy, I'll
listen to people who do more ML.

The thing is, I broadly agree with the argument for AI safety, if you assume
a sufficiently powerful AI. And furthermore, none of the ML researchers I know
dispute the argument, given the premise.
If you do enough reinforcement learning, it's
obvious that reward function design is really hard, because of how easy it
is to get stuck at local optima. If you blindly optimize training set loss,
you overfit to the data. If everyone designs algorithms and tests them on
32 x 32 CIFAR images, they're not going to generalize as well because people
have trial-and-errored there way to methods that only work well on CIFAR.
A common metric in NLP is BLEU, which you can optimize with RL, but if you
do so you don't get better human-graded performance.

People do things like debate the orthogonality thesis, or debate whether
an AI can convince a human to let it out of the box, but these are debates
about systems that don't exist yet, and so they quickly turn into unverifiable
philosophy.


Rough notes for things I want to bring up.

* Some people are first introduced to cognitive biases and Bayesian thinking
through the rationalist community. I think some then make the mistake of
assuming that these insights are *unique* to rationalists. The collection
of all the ideas in one place is special, but the ideas themselves are not.
I feel this is why I've been so skeptical of the community - it feels
too insular, in the way everybody cites everybody else because they only
want to cite trustworthy EAs or rationalists, with a seeming lack of awareness
of how ridiculous they look to outsiders.
* The general feeling of "this is totally cult behavior", social pressure
to agree with The Hot New Thing or to feel like A Member Of The Resistance.
* Why AI safety in particular, versus any other X-risk like nuclear proliferation,
climate change, alien invasion, bioterrorism, preventing creating of
nanotechnology, etc?
* It is okay for some people to be passionate about X-risk, and for some people
to be passionate about other things. The part I can't stand is "when people
mistake their cause for a religion"
* "Introduced to these ideas, and then you're stuck in the convex hull of ideas / the
convex hull of trust, and soon you don't trust anybody else."
    * Some people do this, but the loudest are the most fanatic, and it feels
    like the most fanatic don't. Or they've been so disappointed that they just
    give up on talking to any other people.
    * The example of the AI safety paper that has a citation of "I don't like
    discussing AI safety with people who haven't read Superintelligence"
* Concern that people advocating for AI safety are doing so for charlatan
reasons (that they don't actually believe in the dangers, they just want to play
up the dangers to get money.)
    * I'm pretty sure this isn't true for most people.
* The feeling that I had when reading the Open Phil grant - it sounds like
you're giving OpenAI money because in some philosophical thought experiment,
they agreed with your viewpoint. This seems like a really, really weak reason.
