---
layout: post
title:  "Sometimes I Think About AI Safety and Rationalists and I Get Depressed For a Bit"
date:   2017-05-02 01:14:00 -0700
---

*Warning: good chance I'll regret writing this. Opinion as of August
2017. Personal opinion, does not reflect opinion of anyone I've worked with.
I reserve the right to change my opinion later.*

*This post has been very difficult to write.
In the course of writing this, I've found notes from 8 months ago that I don't
agree with, and a draft from 6 months ago that was so revolting I deleted it out
of disgust. The general structure of this post has changed at least four times
and I'm still not that satisfied with it. Perfection is the enemy of the good, but
on this topic, I wanted to make sure I did it right, because of how much of my life
ends up intersecting with the topic.*

*I say all this to let you know that although I'm doing my best to explain my position,
it is almost certain that I haven't. I'm hoping that I got close enough.*


What Is AI Safety?
----------------------------------------------------------------------------------

There are several treatises on the subject, but I think there's value in showing
my frame on the problem.

Roughly, AI safety centers around this question:

> How do we make sure AI acts the way we want it to?

Very often, people narrow it down to this question:

> When artificial general intelligence arrives, how do we make sure AGI does
> what we want it to do?

Just in case you're unfamiliar withe the terms: AGI stands for artificial
general intelligence, a hypothetical machine learning system that can perform any
task at least as well as the average human could. This includes learning new tasks at
least as well as the average human, becuase learning is a thing humans can do.

Now, depending on how pessimistic you are, some people believe that
AGI will most likely kill all life on Earth. Depeding on your opinions on the limits
of technology, maybe it'll kill all life in the galaxy in a death wave that spreads
at close to the speed-of-light.

This is a controversial subject, to put things mildly. It has generated thousands of
pages worth of discussion all across the Internet. I'm not going to attempt to cover
all the arguments, because I don't know all the arguments. I'll cover the ones I know,
and then I'll get into my reservations.


The Existential Risk Inductive Chain
---------------------------------------------------------------------------------

This is the foundation of AI safety advocacy - pretty much everybody specifically
concerned about the x-risk aspects of machine learning knows a spiel for this
by heart.

* Suppose that at some point, there is an AGI that can do any task at a superhuman level.
* That AGI will eventually be given some goal.
* Almost all tasks are easier if you can think faster, so the AGI will
decide to self-improve as quickly as possible.
* Almost all tasks are easier if you have control, so the AGI will decide to
minimize the chance anybody can disable it. Redundant copies, metaphorically
"hiding the off-switch", etc.
* Combined, such an AGI may first self-improve past the point of humanity's ability
to deal with it, then decide to kill all humans, because humans can't turn you off
if all the humans are dead.
* Importantly, this could potentially happen for any task, even if the AGI's
goal sounds benign, because again - everything's easier if you're smarter and you're
alive.

(...at which point, I imagine someone saying, "And that is why you should donate at
least 10% of your income to a mix of MIRI and OpenAI". But we'll get into that later.)

This part of the argument is called *value alignment*, and people refer to "the
value alignment problem" in similar terms.

Here's an alternate way to form the argument.

* Blindly optimizing an objective can lead to lots of unexpected behavior.
* When humans optimize for something, they usually follow some implicit
ethics that stops them from doing anything too heinous.
* Superhuman AGI may not have those implicit ethics, at which point many
bad things could happen.

People like to focus on the value alignment part of the argument, but the other
component is the *intelligence explosion* component. This is just as pivotal
to the argument as value alignment.

In the above, this was abstracted as "the AGI will self-improve as quickly as
possible". Let's expand this point.

* Suppose there is an AGI that can do scientific research at human level or
above.
* Humans have discovered better machine learning algorithms by doing
research. By definition, such an AGI could do the save.
* Suppose that AGI discovers some advance. It could apply it to itself, which
speeds up its rate of discovery.
* Therefore, there's a chance that once AI reaches a certain point, it can
repeatedly discover new techniques to improve itself at a faster and faster
rate (i.e. recursive self-improvement leading to exponential gains.)
* Note that it doesn't have to be exponential. One common theory of technology
is that it follows a sigmoid curve. First it's used rarely, then there's a rapid
period of discovery, then the limitations of the technique are figured out and
discovery slows down. The concern is less about a single technique giving exponential
gains. It's more about discovering several techniques, each with their
own period of rapid discovery, making it look like exponential growth when zoomed
out.

I consider this pivotal because people use both the Sword of Value Alignment
and Sword of Intelligence Explosion to fend off the common counterarguments.

* If someone says AGI won't harm humans, you wave the Value Alignment sword and
say it's easy for AGI to accidentally end up hurting humans.
* If someone says it will be obvious when AGI will happen, you wave the Intelligence
Explosion sword and say that by the time it's obvious, it may be too late to do much
about it.
* If someone says AGI is too far off to be concerned about, you wave the Intelligence
EXplosion sword again and tell people they underestimate the potential of exponential
growth.


Thoughts on the Inductive Chain
---------------------------------------------------------------------------

Right now, my broad view is that this is a good argument for why AI safety
is worth thinking about, but past that it very heavily comes down to your beliefs
about technology, which is a very fuzzy subject that's hard to argue much about.

I find it very obvious that blindly maximizing a simple objective
can lead to weird, unintended behavior. This shows up all the time in machine
learning. Minimizing classification loss on your training set is not the same as
maximizing classification accuracy. Maximizing accuracy on your training set isn't
the same as maximizing accuracy in the real world.

On occasion, it helps to model machine learning algorithms as learning the
easiest thing necessary to get good performance, and no more.
In one famous story, the army asked researchers to label images as containing or not
containing a tank. They got good performance on their training data, and failed
miserably in the real world. After inspecting their data, they found that all the images
with tanks in them were taken during the daytime, so their model decided that every bright
image had to have a tank in it. Reward hacking happens when learning something
easy is enough to get good performance.

This is especially important to think about in RL, because in practice it's
very easy to design reward functions with bad local maxima. For example, one task in
OpenAI Gym is to move a two-segment arm to a point. You get reward based on the
distance to that point. When training models on this task, I found my networks would
often learn to spin the arm in a very fast circle. On average, the arm isn't that far
from the point, and it's easy to spin in a circle - just apply the maximum torque at
every joint.

This is one of the simplest RL tasks out there, and already I have to think about
whether there are simple behaviors that are hard to escape from. Once a network
discovers that spinning very fast isn't that bad, it's hard to get it to stop
spinning very fast.

In short, I accept that a superhuman AGI may not do what we expect it to do,
and I've yet to talk to a ML researcher who's said otherwise, because every
ML researcher has run into a model that didn't do what they expected it to do.
Value alignment is not a novel idea - it's telling that Isaac Asimov first wrote
the Three Laws of Robotics, then immediately wrote a short story of a Three-Laws
compliant robot killing humans. Then he came up with a Zeroth Law. Then he
wrote a short story of a Zeroth Law compliant robot that kills humans...

The follow-on argument is that people expect the safety problems to be
difficult, and therefore we should have people thinking about them now. This
also sounds reasonable to me. There's a lesson from computer security: the attack
surface is always larger than you think it is. (This is why security people like to
say that everything is broken.) If you're modeling superhuman AGI as an adversary
that's trying to misinterpret things as horribly as possible, it's easy to
figure something out.

(On occasion, people try to argue it's easy to constrain a superhuman AI system.
I mostly consider these arguments a distraction, because the AI-safety-apologist
can usually pretend the AGI is powerful enough to make these arguments invalid,
by saying "something something intelligence-explosion-then-AGI-argues-itself-out-of-the-box".)

(For similar reasons, I've never really liked reading people debate things like
the orthogonality thesis. In these kinds of arguments, it often feels like people
are leaning on too much philosophy, and are using imprecise or incomplete
descriptions of imaginary objects to argue something fundamental about the world.
And because the inputs are fuzzy, it quickly turns into unverifiable philosophy,
at which point I wonder why I should care.)

Overall, excluding some warts, I think being concerned is fair, if you believe
superhuman AGI is going to happen before people have figured out how to handle
it. My biggest disagreement is on when it happens.


Timelines (AKA "The Part Where Some AI Safety Advocates May Get Mad At Me")
------------------------------------------------------------------------------

I should start by saying that I think a lot of timeline discussion isn't
very useful.

People have given surveys to various ML researchers, asking when or if AGI will
happen. Consistently, across the board, the results have shown there's no
consensus. Some people say never. Others say it'll happen in 10 years with 90%
confidence. But overall, it's very hard to predict when a research problem will
get solved.

I've seen lots of excited discussion about the topic, and none of it feels very
useful to me. It's like people are reading tea leaves, and every resarch paper is
another leaf. Many ML researchers have a strong allergy against making predictions
too far into the future, and it feels like people characterize it as irrational.
To that, I'd say this: predicting research is hard, and people really don't want to
overpromise and underdeliver, because hype is so easy to create and so difficult
to keep in check.

It's not that ML researchers don't care. It's their research area, of course
they care! But not everyone revels in debating what the future is going to be like.
I don't. There's a frustrating tendency for the EA community to condense an entire ML
researcher into just their opinion about AGI.

(If you prefer a more stereotypically rationalist phrasing: the value of information in
hearing the answer is low enough that it's not worth the risk of giving people
weird misconceptions about the field.)

However, for people who are passionate about existential risk, timelines are
*everything*. They guide the value, they guide the strategy, and therefore any piece
of information, no matter how noisy, is important to hear.

So, to appease the second group, I'll give my estimate: 10% likelihood within 35 years,
50% likelihood within 50 years, 90% likelihood within 75 years. Again, these numbers
are arbitrary and don't mean much, but I'll explain them anyways.

* I believe neural nets are here to stay, but I'm skeptical that
deep learning by itself is enough. There's a lot of hype around deep learning,
and for good reason, but they can be a bit finicky and it feels like they
have fundamental limitations that are hard to deal with. (It's tricky for
me to articulate what those are.)
* I don't see a clear roadmap for making an AGI of any kind. Even if you
don't care about the safety problems, even if you use all the computers in the
world, and accounting for a Moore's law growth in computational power.
It feels like there are several large conceptual problems in the way,
and more that we can't see right now.
* People have been working on this literally from the start of
the field. I feel like people aren't pricing in what that implies about the
difficulty of the problem. Sure, computers have gotten a lot faster, but
I believe there are big conceptual problems in the way, and progress on
conceptual problems is less dependent on hardware. That the conceptual problems
still exist is a strong indicator that they're quite, quite difficult. (In the same
way that people have yet to give a good refutation to value alignment, despite
how long that problem has been around.)
* Successes get written into papers, presented at conferences, sent through
university and industry PR machines, and get discussed widely. Failures
do not. Because you hear about accomplishments a lot more than failures,
it's easy to extrapolate a faster trend than a slower one, and this makes me
inclined to downweight my guesses.

By that point, I expect enough safety research to be done.
To me, it often feels like EA-minded people frame the argument as
"researchers don't think AI safety is important and we do". The more common
belief I've heard is "I don't think it's worth spending a lot of resources
on safety right now, but I don't mind that people are working on it." Which
is around where I'm at. Right now, the money is in better translation,
better computer vision, and better recommendations. We're still very much at
the start of integrating machine learning into society. Over time I expect
safety problems to be more important for producing good products, which
will put more economic incentive into those lines of research.

For example, the Concrete Problems in AI Safety paper (which I like quite a lot)
mentions a thought experiment of a household robot getting reward if it can't
see any dust. It decides to cover all the furniture with a blanket, because
that's easier than cleaning the room. This is a thought experiment because
household robots don't exist, but if household robots did exist, the one that
does what people expect it to do will be a better product. This starting to
happen with the various voice assistants, like Google and Alexa and Siri and
Cortana, except right now people don't expect them to work all the time because
the tech is so new.

(Note: I'm very sure you can poke holes in this example, but I'm more
concerned about the holistic point: there are currently few economic incentives
for AI safety, and I expect there will be more economic incentives in the future.)

(Note 2: This might be why OpenAI has making a household robot as a research
goal.)

(Note 3: This is not a novel idea. Stuart Russell made a similar point in his
keynote talk at the 2016 Bay Area Robotics Symposium. Except his example was a
robot deciding to turn the family pet into food. I guess he wanted to go
for shock factor.)


Strategy And Culture (AKA "The Part Where Everyone Who Wasn't Mad At Me Gets Mad")
--------------------------------------------------------------------------------

To summarize: I do like that people are thinking about AI safety, I think they
have some room for growth, and although I'm not super enthusiastic about it,
I don't mind the existence of various orgs dedicating themselves soley to AGI
risk.

What I **do** mind, however, is how people have turned AI safety discussion into
a minefield that I just can't deal with.

Look. I do hesitate admitting it, but I fit into many aspects of EA culture and
rationalist culture. On average, whenever I bring enough to go to events with many
EAs or rationalists, I have a good time.

Except, the key point is *on average.* Because every now and then, I run into
something that's violently repellent, and I give up and check out.

In this post, I have tried to be nice, and I have tried to be diplomatic, and
I have tried to focus on the arguments and tried to play fair. Because like I
said: I approve of most of the culture, and I've met some cool people. And down
this road lies overgeneralization and nastiness.

But sometimes, I have to rant, and this is one of those times.

\* \* \*
{: .centered }

There is no one, single incident that turned me off of the rationalist and EA
communities. It's more like a series of events, building over time, until finally
it breaks.

Overall, my biggest gripe is that the discussion feels oddly centralizing. Let's
say an AI safety debate breaks out on Hacker News, because literally every article
about ML on Hacker News seems to devolve into AI safety discussion. Here's what
I expect to get brought up.

* Somebody insulting Nick Bostrom and saying they don't like his arguments.
* Somebody praising Nick Bostrom and arguing he has the right ideas.
* Somebody will mention Stuart Russell.
* Somebody will mention Andrew Ng and that one quote he made that time which
people *love* to hate on.
* Somebody will cite the Slate Star Codex post about AI researchers.
* Somebody will complain about EAs who donate to MIRI or OpenAI instead of Afria.
* Somebody will defend them by giving off-the-cuff Bayesian probabilities on
the impact.

Somehow, in the quest to consider novel, interesting ideas, it feels like EAs
and rationalists have lost sight of how to do the same around AI safety. They
lift up Bostrom and Yudkowsky as shining knights, send links to Paul's or Victoria's
writings, try to get you to read Superintelligence or some paper by Nate Soares,
tell you to donate to MIRI so that they can solve corrigibility and figure out
the details of constrained extrapolated volition. Recently it's changed to asking
people to donate to OpenAI.

To be very clear: I am not hating on Bostrom or Yudkowsky or Paul or Victoria or Nate.
I'm sure they're nice and reasonable once you get to know them. I'm hating on the
fandom that seems to have sprung up around them - a weird blend of AI voyuerism and
AI fanboyism, filtered exclusively through rationalists.

I once tried to read an arXiv paper where someone listed various achievements of
AI, and concluded that they were expecting AGI very soon and was hoping people
could give counterarguments. In the citations section, they cited Superintelligence,
saying they didn't like talking to people who hadn't read Superintelligence, because
he felt like he was repeating the same arguments. I wonder if that person ever
considered that if you only talk to people who've read Superintelligence, you're
going to get a very different view of AI than most people.

(For the record: I haven't read Superintelligence. I actually hope it was obvious
that I haven't read it, because it would be nice if people with similar beliefs to the
arXiv author have a justified reason to find this post boring.)

This kind of fanboyism feels like a latent problem in rationalist culture.
On Facebook, I've seen people say things like

> Yes, the headline is clickbait, but I swear this article is good. Look at
> who's sharing it.

Or

> You should read this article by Scott, because Scott is amazing.

Or

> You should listen to my prediction about the administration, because I bet on Trump
> winning the election, even though I didn't like him.

Or

> It is not a mistake that we put a leader of the EA movement in charge of this event.
> We did this because we knew they were well-trusted.

In other words, people trying to get you to read something by telling you about the
intelligence of the person writing it. **Most rationalists do not do this.** Out of the
ones I see in my Facebook feed, around 10-20% of them act like this. But it's annoying
enough that it pollutes the rest of my experience. To paraphrase a friend:
"I think every rationalist defines themselves by their intelligence. They all think
they're smart and they're proud of it." I wouldn't say it that strongly, but it has
kernels of truth.

I have a very intense aversion to this form of argument, and vastly
prefer people who let the work stand on its own and leave it to the reader to decide
whether the person writing it is smart.
To me, "smart" means "I think this was well-constructed and the argument is compelling
to me on some level", which makes the label very subjective. When somebody says

> You should read this article by Scott, because Scott is amazing.

I get annoyed, because I have no reason to believe Scott is amazing or not. Whereas
if they had said

> You should read this amazing article, written by Scott.

I'm totally fine, because I can read the article and decide on my own terms whether
I think Scott is amazing or not.

\* \* \*
{: .centered }

Here is a strawman rationalist counterpoint: "Does that really matter? I'm not convinced
it's a problem, and even if it is a problem, I don't think it's important."

Well Strawman Rationalist, I'm glad you asked! Let me tell you a story.

There's a card game forum I visit daily. One day, somebody linked a post
from Slate Star Codex. It was a good post, very relevant to the conversation.
One of the common forumgoers liked it a lot, and commented as such. The moment
I saw this, I felt a bit of dread, because I knew what was coming.

Within a week, he's added links to the Sequences and Slate Star Codex to his
signature. Within two weeks, he's dropping Bayesian probabilities in all his posts.
This ends with him starting a thread asking whether people's beliefs on
superintelligence are closer to Eliezer Yudkowsky's or Robin Hanson's.

None of this was particularly surprising. His pattern of
speaking was very similar to the pattern of speaking in the rationalist sphere. He
had mentioned GiveWell in the past, so he already had that in-road. It was very
obvious that he would like the rationalist community.

And at the same time, I was disappointed anybody could jump from
"guy who mentioned GiveWell once" to "ridiculously stereotypical rationalist"
in the span of two weeks.

He could have been talking about anything in the world! And instead he was talking
about Eliezer.

If a culture has a tendency to deify specific people, it'll turn into people debating
for and against ideas those people care about.

Is this bad? Well, not necessarily. There are times where it's okay (and even good)
to dive into a very narrow world view - if you recognize you're doing so. The
danger is that you circulate a group of very similar ideas. On AI risk in particular,
it feels like many of the same themes come up.

* Big companies are not listening to the little guys fighting the good fight
for AI safety.
* It is insane that people are not donating existential risk, or that the people
who do are donating to causes like global warming and nuclear non-proliferation
instead of AI.
* The world can only get better if people donate to support MIRI and OpenAI.

I have some disagreements with all three points.

As a further branching point from this, my definition of safety-like problems
seems broader than most people's.
To me, all of the following are relevant.

* Fairness: how do we ensure machine learning isn't discriminatory? People have
talked about using machine learning to profile criminals, which is incredibly
controversial in its own right - what if it makes racial profiling worse?
* Privacy: people have had some success with extracting training data out of
machine learning models. This isn't a big concern right now, but if you, say,
train a model on patient data for healthcare, solving the differential
privacy problems become a lot more important.
* Adversarial inputs: it's been shown that we can imperceptible noise to inputs
to fool models into mislabeling them with high confidence. These adversarial attacks
are both robust and generalizable - some results suggest they can transfer
between models, that a single noise perturbation can make all images misclassified,
that the noise perturbation works even if you print the image onto paper and take
a picture of the paper, and that you can do so even if
you only get black-box queries to the model (like you would through a computer
vision API). How do we stop adversaries from fooling trained models?
* Human-computer interaction: over time, computer agents will play a larger
role in society. How do humans model and interpret robot behavior? In turn, how
do we make sure robots model human behavior accurately? This is relevant to
self-driving cars, because there are a lot of implicit human conventions behind
signalling when you do and don't have right of way.
* Interpretability: decision trees are well-known for being interpretable.
Neural nets are well-known for being hard to interpret. This is a blocker
for areas like medicine, where doctors place a lot of value on understanding
what a model is doing. A doctor may choose a weak, interpretable model over
a strong, less interpretable one, which is an entirely reasonable choice, but
it limits ML's ability to save lives.

These are all important, approachable questions, that are considerably more
well-defined than the agent foundations work I've seen when I skimmed MIRI's
work.

I went to a presentation Nate Soares gave once. The argument went roughly
like this.

* Constraining the behavior of systems is hard.
* Constraining the behavior of systems with superhuman intelligence is going to
be *extra* hard. It's unlikely that safety efforts for current ML
systems will help once AI progresses enough.
* We aren't quite sure how to do it either, but we expect a mathematical, formal
model of cognition is likely to be useful.

I don't agree with the second point (research on current systems can give lessons
for the future, even if the methods themselves won't work.) I also don't agree
with the third point (hard for me to see the value in formalizing probabilistic
proof systems or self-reflective Turing Machines or whatever.)

(Next paragraph is awful, will fix later)

To me, the chance that working on fairness and adversarial inputs is relevant
to AGI, is around the same order of magnitude as the usefulness of MIRI's work.
(Read: quite small.) Except if you work on fairness and adversarial inputs, it's
useful right now, in which case you might as well help people now and help contribute
to future safety-like problems.

When I think of MIRI, I think about a group of people who really like math,
and really like thinking about cognition / decision theory. They've discovered an
argument that lets them get paid to do it, but they would do it for free, because
the problems are just that interesting to them. I don't think it'll be useful,
but I don't want them to stop. They're simply alright.

I certainly don't think of MIRI as a group of people who
are *literally* saving the world from itself in the *most cost-efficient* way, and how could
you possibly call yourself a rationalist if you disagree with that?

But then I read or hear something that implies something like that, because
they read some posts by Yudkowsky and just *have* to tell people about them...

No one in the rationalist-sphere has ever said something like that to me.
But it's how I model the hardcore segment that got into the movement
by falling in love with Yudkowsky's writing - a group of people who agreed with
someone so much, they forgot to do due diligence and ask themselves whether
they should. A group that spends its days discussing Newcomb problems and
timeless decision theory and prisoner's dilemnas and coordination problems,
warning people that an AGI can hijack a human's mind by writing a convincing
enough argument, all without considering the obvious corollary - a smart
enough person could do the same, and maybe they already have.

Artificial intelligence captures the imagination. Stories about unaligned AGI
do so even more. I've read three "rationalist fanfic" stories, all of which
independently decided to talk about unaligned AGI, cryonics, and the
many-worlds interpretation of quantum mechanics - which feels like an
exceptionally narrow range of topics for a group that I'd expect to pride
itself on novel, skeptical thinking. I get it - these ideas are interesting
to think about, and you think they're cool and important. But isn't it natural
to get sick of paperclip optimizers when you've read about them for the
5th time?

(Off the top of my head:

1. Superintelligence is told to create lots of notes, invents nanotech, uses up
all the oxygen to kill humans by asphixiation, turns humans into notes.
2. Superintelligence is told to make people happy, decides the best way to do
this is to inject people with a serum that constantly triggers their dopamine receptors.
3. Superintelligence is told to make people laugh by writing good jokes,
it writes one so good that people die laughing.
4. Superintelligence is told to maximize the happiness of players in an MMORPG,
invents brain uploading and manipulates people until everyone has uploaded into
the game.
5. And of course, superintelligence is told to make lots of paperclips, invents
nanotech, kills everyone, turns them into paperclips.

*It took me like, 2 minutes to remember all of these.* Can you blame me for
getting sick of these kinds of stories? I think it's entirely fair for me to get annoyed
with people when present the same argument pattern, over and over again.)

To paraphrase another friend: "I thought rationalists were supposed to
be about studying cognitive biases and figuring out how to exploit your
comparative advantages. Instead it's people asking me to sign up for cryonics
and try out polyamory."

\* \* \*
{: .centered }

In many ways, the rationalist community reminds me of the math contest
community - a group of people who, after much searching, have found people who
finally speak the same language they do, and all the consequences thereof.
It should come as no surprise that the two have a healthily large overlap.

They have the same virtues and they have the same failings. Both have smart
people, and both have assholes with no humility. Both have made people
finally have meaningful conversations, and both spawn ridiculous amounts
of gossip.

The large focus on AI safety that some people have is wearying and annoying.
I get annoyed when I read people arguing what precisely "intelligence" means.
I get further annoyed when people read a news article about a recent deep
learning paper, and herald it as a Potential Sign of the End Times.

Take, for instance, the Neural Cryptography paper. Here's roughly how it goes:
Alice and Bob are two neural networks with shared parameters. Each agent
has an encoder, which takes a message $$m$$ and key $$k$$, then outputs
some output $$m'$$. There's a decoder, which takes $$m'$$ and $$k$$ and
is trained to reconstruct $$m$$ as closely as possible.

In between the two is Eve, a neural network which receives just $$m'$$,
and tries to reconstruct $$m$$. The two are trained adversarially - Alice
and Bob are trained to maximize Eve's confusion (# of bits it gets wrong),
and Eve tries to maximize the # of bits it gets right.

It's a cute idea, but the experiments were very small - they give each
network an 8 bit message, and an 8 bit key, and the model didn't even learn
a one-time pad, it learned something that kinda worked. I went to a talk
the authors gave, and they talked about how that was the best they got,
even after lots of tuning.

So in short: a cute research idea, with results suggesting you could
*kind of* do this, but not much more than that. It was submitted to ICLR
and rejected for similar reasons, and I think that verdict is fair.

But in the media coverage of the paper....oh jeez, let me pull up the headlines.

HEADLINES HERE.

Meanwhile my transhumanist friends were discussing it as if it was a great
advance, extrapolating to AI agents using steganography to encode messages to
one another, wondering what the next plan was with neural cryptography, and
all sorts of massively oversized speculation given the results of the work.

On one hand, complaining that the media lies and never gives the nuanced
view. On the other hand, buying into the machine learning PR wholesale,
without reading the actual paper.

In moments like those, it's very hard to be fair towards the seeming
hypocrisy.

It's all the more frustrating because *I expect rationalist to do
better.* I'm not disappointed because they're stupid, I'm disappointed
because they're smart and they're falling into the same traps they tell
other people to avoid.

I know this isn't the fairest portrayal, but it's the one I have.

\* \* \*
{: .centered }


I watched Ex Machina because people hyped it up as great. It was a good movie,
but not for me. To me it felt like it was presenting a bunch of old ideas
as if they were new. I didn't need to hear Yet Another Debate About The Chinese
Room Thought Experiment because *I really don't care.*

The organizing principle of EAs is that you can have much larger impact if
you donate according to careful argument instead of what feels right.
The organizing principle of rationalists is that the human brain is horribly
horribly broken, and it's worth trying to optimize your life, even if you're
never going to be perfect.

But in practice, it feels like the organizing principle is for people to
endlessly debate the merits of different causes, different organizations,
and the right way to live your life. That's fine, but sometimes I feel like
people are more interested in philosophy than the original question.
I can understand why. Philosophy is fun in moderation, but not much more than
that.

An aggressive reductionist desire to categorize and name everything about
the world. (It's not a coincidence that people who do math end up intersecting
in these circles a lot - all of math runs on this kind of thinking.)


\* \* \*
{: .centered }

Depending on the circumstance, I can pass as an EA or as a rationalist.
If I want people to think I'm an effective altruist, 
I talk about the marginal utility of money, or Against Malaria Foundation,
or animal suffering. If I want people to think I"m a rationalist, I can
play up cognitive biases, and talk about different x-risks, or ways
to exploit comparative advantages in the market.

The thing is, I'm neither? I'm not an effective altruist, and I'm not
a rationalist.

\* \* \*
{: .centered }

Now, given how awful the strawman rationalist is, why do I keep him around?
He's always smug, always insufferable, always tries to find a new reason
to ignore arguments that machine learning could possibly end okay, and believes
that he and his cohort are the only people who are doing anything meaningful
to protect people. To the strawman, everyone else is a well-meaning but
misguided idiot, but I'm not, oh no I'm not, because I've read the Sequences!

I keep him around because despite how crazy he is, he still manages to give
good insights.

What I do want to stop, however, is the vaguely adversarial way people on both
sides have framed the AI safety debate. It's not overt or anything, more like
a collection of snide remarks and jokes I hear now and then. A person from MIRI,
saying that at least 2 professors take them seriously, one of whom is
well-respected. Another person from MIRI, joking that giving DeepMind tons
of compute power may have been a bad idea. An effective altruist, contrasting
the money going into AI vs the money going into AI safety. Two people have
independently joked that if AGI is imminent, maybe we need kill all the AI
researchers, which really stops being funny when you're one of those
researchers.

On the flip side, a lot of established ML people have criticized people
for bad reasons. They have weird opinions on some basilisk thing,
why should I listen to these people. He's a professor of philosophy, I'll
listen to people who do more ML.

Rough notes for things I want to bring up.

* Some people are first introduced to cognitive biases and Bayesian thinking
through the rationalist community. I think some then make the mistake of
assuming that these insights are *unique* to rationalists. The collection
of all the ideas in one place is special, but the ideas themselves are not.
I feel this is why I've been so skeptical of the community - it feels
too insular, in the way everybody cites everybody else because they only
want to cite trustworthy EAs or rationalists, with a seeming lack of awareness
of how ridiculous they look to outsiders.
* The general feeling of "this is totally cult behavior", social pressure
to agree with The Hot New Thing or to feel like A Member Of The Resistance.
* Why AI safety in particular, versus any other X-risk like nuclear proliferation,
climate change, alien invasion, bioterrorism, preventing creating of
nanotechnology, etc?
* It is okay for some people to be passionate about X-risk, and for some people
to be passionate about other things. The part I can't stand is "when people
mistake their cause for a religion"
* "Introduced to these ideas, and then you're stuck in the convex hull of ideas / the
convex hull of trust, and soon you don't trust anybody else."
    * Some people do this, but the loudest are the most fanatic, and it feels
    like the most fanatic don't. Or they've been so disappointed that they just
    give up on talking to any other people.
    * The example of the AI safety paper that has a citation of "I don't like
    discussing AI safety with people who haven't read Superintelligence"
* Concern that people advocating for AI safety are doing so for charlatan
reasons (that they don't actually believe in the dangers, they just want to play
up the dangers to get money.)
    * I'm pretty sure this isn't true for most people.
* The feeling that I had when reading the Open Phil grant - it sounds like
you're giving OpenAI money because in some philosophical thought experiment,
they agreed with your viewpoint. This seems like a really, really weak reason.

The general trend I've noticed is that some people like to rant about rationalists.
But over time, it turns out they rant specifically about Yudkowsky and people
with similarly weird beliefs to him. And that they appreciate the people who
don't seem to take the whole thing as seriously - people who don't feel the
need to talk about Bayes Theorem and utilitarianism all the time.

Once, a friend on Facebook asked (CHECK),

> Aliens have just visited Earth, and they ask for 10 people to represent all
> of humanity. Who do you pick?

One commenter said Bostrom should obviously be one of them. And I remember thinking
that this was an oddly controversial pick, unless you were in the in-group.

Yes, I know you've seen the skulls. Yes, I know that some people in these groups
are aware their beliefs are fluid. Yes, I've read CFAR's mission statement:
beliefs, weakly held, but acted on anyways.

The problem is that it's not clear these virtues are universalized, it's not clear
this is common knowledge, and people view rationalists as much more homogenous
than they actually are.

This isn't actually that weird, when you think about it. Let me put it this way:
I decided to read three "rationalist fanfic" stories, and they all indepedently
decided they needed to explain the many worlds interpretation of quantum
mechanics in laymans terms. They also all felt the need to talk about characters
"making updates about the world". Then there was simulation hypothesis, then
wireheading, then cryonics.

Do you realize how condescending it is, to see somebody say they're the only
people making a difference, and that the rest of us are just along for the ride,
like so much chaff? The implicit assumption that everyone outside of the rationalist
in-group is an idiot? 

Do you know what my strawman rationalist says in response? "We say that because
it's true. Don't you know? Denying the truth doesn't make it any less true."

\* \* \*
{: .centered }

It feels like people are reading tea leaves, and every paper's another leaf.
I've noticed a strong allergy against making any predictions too far out into
the future, because even the short term progress is hard, and it's hard to
estimate the difficluty of problems, and people don't want to overpromise
and underdeliver because hype is so easy to create and so difficult to keep
in check.

So, when I see articles about neural cryptography, and people talking about
steganography in AGI, or somebody releases WaveNet and somebody speculates on
when we'll see an AI-generated pop song hit the Billboard top 100, a part of
me dies.

I think a lot of times, when a big company releases a blog post on a neat
ML paper, people try to interpret what the point of it is, why it'll make
money, and so on. And in my experinece, most research happens because people
think it's really, really cool to see what's doable. This applies to all fields
of research, by the way. I was talking to one person about AI safety, and she
told me that although she kept seeing people say they wanted to do AI safety for
utilitarian reasons, she didn't feel she was utilitarian. She was just super,
super interested in mathematical models of cognition. AI safety let her
study those problems in a context that could be important, down the line.

This is my impression of a lot of AI safety research. People get incredibly
drawn to the problem, find it fascinating, and then try to funnel their
abilities into something that's vaguely safety-like. Researchers from MIRI
come from a pure math background, with focus on mathematical logic and
metamathematics, and that leads to them releasing work on self-reflective
Turing Machines and probabilistic proof systems.

This is the other reason I have trouble taking any arguments out of MIRI
too seriously - part of me always suspects that their arguments are partially
post hoc justifications to align their research abilities with their
research trajectory. Everyone does this - the joke in public health is that
everyone has a good argument for why their cause is important to deal with.
Researchers always work on problems that seem important to them, and they're
also always telling people why it's important to them, and the problem I have
is when people get too dogmatic on why *you and you and you and you* should
be working on the same thing.

\* \* \*

There are many reasonable arguments, many of which have competing conclusions.
I could just as easily believe something else, and it's mostly luck that changes
which argument I see first. So it feels wrong to commit too strongly to any
particular action, or to advocate for anything very strongly.

And, again, this isn't a unique viewpoint: Nick Bostrom gave a talk on this,
giving an example of how a person argues themself into and out of voting
for someone. (LOOK IT UP). Eventually he concludes with saying that you
probably should put more penalty on committing to specific arguments, especially
with regards to x-risk (CHECK IF THIS CONCLUSION ACTUALLY HOLDS UP.)

So, you know. I'd like to think that fundamentalist rationalists at least keep
things like this in mind, but I'm not sure they do.

\* \* \*

On average, the people I've met that have relations to LessWrong in some way
are generally alright.

Really, it seems like the problem is perception. For better or worse, the ideas
that pull the community together also attract a lot of weirdness. Is that bad?
Well, no, not if you're okay with weird. Some people aren't okay with weird.
And the weird people also tend to be the loudest or the most likely to speak
up, and soon you hit the classic filter bubble problem - the people who talk
aren't representive of the people who exist.



No true Scotsman problem.

The problem people have is mostly with Yudkowsky and beliefs close to those
ones.

The feeling that people from other areas of expertise are advocating for the
right thing to do, talking about something they don't understand, and knowing
that it's easy for people to be mislead about things, and the difficulty
in distinguishing between crackpot and valid point.

In rationalist circles, it feels like people very quickly decide to follow
whatever argument rings true with them, and then directly act on it. Whereas
other people are hesitant to predict anything too far out because of the
difficulty of the prediction, and don't see how it's supposed to change anything
or what the point of it is. "The best we can do" vs "not even worth the effort
right now"
