---
layout: post
title:  "Sometimes I Think About AI Safety and I Get Depressed For a Bit"
date:   2017-05-02 01:14:00 -0700
---

OUTLINE

1. Complaints about the state of conversation around AI safety.
2. Summarize the stereotypical argument about superintelligence (link a
bunch of things about it here.)
2a. I'm not sure how necessary this is. It might not be needed.
3. Complaints about the common rationalist point of view (fair portrayal
or not).
4. Complains about the ML point of view (fair portrayal or not.)
I don't know if these should be separated - the thing I want to emphasize
is that there are several reasons to disagree with someone, but I'm not
sure it justifies the vitriol / drama that surrounds it.

\* \* \*
{: .centered }

*Opinion as of June 2017.*

*These are my personal opinions, and don't reflect the opinions of
any organizations I've worked with.*

I've been trying to write this post for over a year.

There are some notes from 8 months ago that I don't agree with anymore.
There was a draft from 6 months ago, which I deleted out of disgust.

All my feelings on this subject are...jumbled. A cacophony of latent
beliefs, hazy thoughts, and lots and lots of frustration at my inability to
explain things to myself. At this point, I just want it to be *done*,
because I'm sick of bottling up monologues.

so here goes.

\* \* \*
{: .centered }

The core premise of AI safety discussion is that AI and machine learning
are transformative technologies that could radically change society.
People's lives could get a lot better, or a lot worse. In the most
pessimistic scenarios, AGI kills all life on Earth, then kills all life
in the galaxy in a speed-of-light wave of death.

Whether that's going to happen or not is a controversial topic, to put
things mildly.

Like all controversial topics, there are thousands and thousands of pages
of discussion, scattered across the Internet. I generally avoid joining
any of them, because by the time I finish reading them, I get turned off
and leave.

That feels like a problem. Let's dig in and do some...*introspection.*

![Panel from SMBC comic about introspection](/public/ai-safety/introspection.png)
{. centered }

Source: [SMBC](http://www.smbc-comics.com/comic/2011-04-25)
{: .centered }

\* \* \*
{: .centered }

The thing about AI is that it captures the imagination. At times it feels
like everyone has an opinion on what "intelligence" means, whether it
can really be created artificially, whether AGI is possible or not.
No wonder everyone says they do machine learning instead of AI - it's
less of a buzzword. (Although that's not saying much.)

So on one hand, there's a group of people who aren't very informed on the
field. On any article about AI, they bring up Skynet, Terminators, or HAL.
It gets old really fast, but I'm generally fine with this. Not everyone
is going to understand machine learning, and I broadly don't expect
uninformed people to know any better. In the same vein, I recognize that
my opinion on literary theory, biology, and any chemistry/physics past
the high school level is complete, utter garbage.

On the other hand, there are the debates about the existential risk of AI,
usually from people in some union of the effective altruism and rationalist
communities, and those debates are where most of my frustration comes from.

Broadly, it's because it feels like they should know better. It feels like
a lot of the discussion is at a high level, uses imprecise definitions to
make broad claims, and to me both those communities pride themselves on
carefully considering all sides of the argument - except, seemingly, for
AI safety.

I know this isn't a fair portrayal of the community. That's part
of the problem.

\* \* \*
{: .centered }

Like a lot of other people, I was introduced to the community
through Harry Potter and the Methods of Rationality. I was a bored
high school kid, and I liked Harry Potter, so it just kind of happened.
Ever since then, I've been circling the community - first by reading the
occasional LessWrong post, and nowadays through whatever shows up in
my Facebook feed.

Around that time was when I first started reading some arguments people had
about superhuman AI, that felt like neat stories, but didn't feel like things
I needed or wanted to care about.

But nowadays, when people talk about parables of AI safety, I just get
sick of it.

By this point I've read at least three "rationalist fanfics" that were basically
all AI safety parables. Oh look, you tell an AI to make lots of paperclips and
it invents nanotech and converts all matter to paperclips. Hence, paperclip
optimizer. Oh look, you tell an AI to write lots of notes and it hides its
intelligence to fool people into thinking it's not very smart, then kills the
human race with neurotoxin to stop it from writing lots of notes. Bam, the
scenario from Tim Urban's Wait but Why. Oh look, you tell an AI to
"maximize happiness of the game's players" and it manipulates all the humans until
they play the game. Oh look, Santa's elves are real, and if you ask them to make
people happen they create wireheading and have to be talked out of it by
The Sane Person Who Knows This Is a Bad Idea. *I get it, I got it the first
time.* And I know you're telling these parables because you think the ideas
are important, and I know some people are reading about paperclip optimizers
for the first time, but isn't it natural to get sick of paperclip optimizers
the 5th time through?

I watched Ex Machina because people hyped it up as great. It was a good movie,
but not for me. To me it felt like it was presenting a bunch of old ideas
as if they were new. I didn't need to hear Yet Another Debate About The Chinese
Room Thought Experiment because *I really don't care.*

Really, that's the crux of the problem I have about EAs and rationalists.
The organizing principle of EAs is that you can have much larger impact if
you donate according to careful argument instead of what feels right.
The organizing principle of rationalists is that the human brain is horribly
horribly broken, and it's worth trying to optimize your life, even if you're
never going to be perfect.

But in practice, it feels like the organizing principle is for people to
endlessly debate the merits of different causes, different organizations,
and the right way to live your life. That's fine, but sometimes I feel like
people are more interested in philosophy than the original question.
I can understand why. Philosophy is fun in moderation, but not much more than
that.

An aggressive reductionist desire to categorize and name everything about
the world. (It's not a coincidence that people who do math end up intersecting
in these circles a lot - all of math runs on this kind of thinking.)


\* \* \*
{: .centered }

Depending on the circumstance, I can pass as an EA or as a rationalist.
If I want people to think I'm an effective altruist, 
I talk about the marginal utility of money, or Against Malaria Foundation,
or animal suffering. If I want people to think I"m a rationalist, I can
play up cognitive biases, and talk about different x-risks, or ways
to exploit comparative advantages in the market.

The thing is, I'm neither? I'm not an effective altruist, and I'm not
a rationalist.

"But, you went to EAGxBerkeley, then went to EA Global, then went to
a Slate Star Codex meetup hosted at the CFAR office, *and then went
to a CFAR workshop*. How are you *not* an EA or a rationalist?"

And I mean, that's all true, but when I say I'm not an EA, and not
a rationalist, it feels right.
The thing is, I don't consider myself to be an effective altruist, and
I don't consider myself to be a rationalist. I'm merely a bystander to
both movements.

\* \* \*
{: .centered }

Throughout this essay, I'll use "the strawman rationalist". The strawman
rationalist will be denoted by *italics*. The strawman rationalist is not
fair. It does not match any self-described rationalist I know. It's more
like an amalgam of all the negative rationalist stereotypes I can think of.

Now, given how awful the strawman rationalist is, why do I keep him around?
He's always smug, always insufferable, always tries to find a new reason
to ignore arguments that machine learning could possibly end okay, and believesg[M Â™^
that he and his cohort are the only people who are doing anything meaningful
to protect people. To the strawman, everyone else is a well-meaning but
misguided idiot, but I'm not, oh no I'm not, because I've read the Sequences!

I keep him around because despite how crazy he is, he still manages to give
good insights.

The rough arguments for AI safety are

* At some point, there will be an AI that can do any task better than
a human can.
* There is some chance such an AI can repeatedly discover new technological
advances, and apply those advances to itself to improve itself at an
astonishing rate.
    * This doesn't have to be exponential, it just has to be fast enough.
    * It's assumed that a single
    technological advance follows a sigmoid curve - there is a slow increase
    as the technology is tested, then a rapid period of innovation, then a
    slowdown as people realize the limits of the technology. The argument
    is that if you achieve a lot of technological advances quickly, a
    chain of iterated sigmoids starts to approach a near-expoential curve.
* A human will eventually ask such an AI to do something.
* Almost all things are easier if you can think faster, so the AI will
decide to improve itself as much as it can.
* Almost all things are easier if you're alive, so the AI will decide to
minimize the chance anybody disables it.
* Therefore, if a sufficiently smart AI is asked to do something, it may
decide to kill all humans.

But really, this is a long chain of reasoning that can be better summarized
this way.

* Blindly optimizing an objective can lead to lots of unexpected behavior.
* When humans optimize for something, they usually follow some implicit
ethical rules.
* An AI that optimizes for something may not have those implicit ethics,
leading to lots of unexpected behavior, some of which could include
killing all humans.

My broad view at this time is that the argument behind AI safety is worth
taking seriously. MIRI's agenda seems focused on exclusively thinking
about ways to constrain superhuman systems, under the thesis that it's a very
different problem from contraining current ML systems. I'm skeptical any of
MIRI's work will end up being useful down the line, but that doesn't mean
I want them to stop.

What I do want to stop, however, is the vaguely adversarial way people on both
sides have framed the AI safety debate. It's not overt or anything, more like
a collection of snide remarks and jokes I hear now and then. A person from MIRI,
saying that at least 2 professors take them seriously, one of whom is
well-respected. Another person from MIRI, joking that giving DeepMind tons
of compute power may have been a bad idea. An effective altruist, contrasting
the money going into AI vs the money going into AI safety. Two people have
independently joked that if AGI is imminent, maybe we need kill all the AI
researchers, which really stops being funny when you're one of those
researchers.

On the flip side, a lot of established ML people have criticized people
for bad reasons. They have weird opinions on some basilisk thing,
why should I listen to these people. He's a professor of philosophy, I'll
listen to people who do more ML.

The thing is, I broadly agree with the argument for AI safety, if you assume
a sufficiently powerful AI. And furthermore, none of the ML researchers I know
dispute the argument, given the premise.
If you do enough reinforcement learning, it's
obvious that reward function design is really hard, because of how easy it
is to get stuck at local optima. If you blindly optimize training set loss,
you overfit to the data. If everyone designs algorithms and tests them on
32 x 32 CIFAR images, they're not going to generalize as well because people
have trial-and-errored there way to methods that only work well on CIFAR.
A common metric in NLP is BLEU, which you can optimize with RL, but if you
do so you don't get better human-graded performance.

People do things like debate the orthogonality thesis, or debate whether
an AI can convince a human to let it out of the box, but these are debates
about systems that don't exist yet, and so they quickly turn into unverifiable
philosophy.


Rough notes for things I want to bring up.

* Some people are first introduced to cognitive biases and Bayesian thinking
through the rationalist community. I think some then make the mistake of
assuming that these insights are *unique* to rationalists. The collection
of all the ideas in one place is special, but the ideas themselves are not.
I feel this is why I've been so skeptical of the community - it feels
too insular, in the way everybody cites everybody else because they only
want to cite trustworthy EAs or rationalists, with a seeming lack of awareness
of how ridiculous they look to outsiders.
* The general feeling of "this is totally cult behavior", social pressure
to agree with The Hot New Thing or to feel like A Member Of The Resistance.
* Why AI safety in particular, versus any other X-risk like nuclear proliferation,
climate change, alien invasion, bioterrorism, preventing creating of
nanotechnology, etc?
* It is okay for some people to be passionate about X-risk, and for some people
to be passionate about other things. The part I can't stand is "when people
mistake their cause for a religion"
* "Introduced to these ideas, and then you're stuck in the convex hull of ideas / the
convex hull of trust, and soon you don't trust anybody else."
    * Some people do this, but the loudest are the most fanatic, and it feels
    like the most fanatic don't. Or they've been so disappointed that they just
    give up on talking to any other people.
    * The example of the AI safety paper that has a citation of "I don't like
    discussing AI safety with people who haven't read Superintelligence"
* Concern that people advocating for AI safety are doing so for charlatan
reasons (that they don't actually believe in the dangers, they just want to play
up the dangers to get money.)
    * I'm pretty sure this isn't true for most people.
* The feeling that I had when reading the Open Phil grant - it sounds like
you're giving OpenAI money because in some philosophical thought experiment,
they agreed with your viewpoint. This seems like a really, really weak reason.

The general trend I've noticed is that some people like to rant about rationalists.
But over time, it turns out they rant specifically about Yudkowsky and people
with similarly weird beliefs to him. And that they appreciate the people who
don't seem to take the whole thing as seriously - people who don't feel the
need to talk about Bayes Theorem and utilitarianism all the time.

Once, a friend on Facebook asked (CHECK),

> Aliens have just visited Earth, and they ask for 10 people to represent all
> of humanity. Who do you pick?

One commenter said Bostrom should obviously be one of them. And I remember thinking
that this was an oddly controversial pick, unless you were in the in-group.

Yes, I know you've seen the skulls. Yes, I know that some people in these groups
are aware their beliefs are fluid. Yes, I've read CFAR's mission statement:
beliefs, weakly held, but acted on anyways.

The problem is that it's not clear these virtues are universalized, it's not clear
this is common knowledge, and people view rationalists as much more homogenous
than they actually are.

This isn't actually that weird, when you think about it. Let me put it this way:
I decided to read three "rationalist fanfic" stories, and they all indepedently
decided they needed to explain the many worlds interpretation of quantum
mechanics in laymans terms. They also all felt the need to talk about characters
"making updates about the world". Then there was simulation hypothesis, then
wireheading, then cryonics.

Do you realize how condescending it is, to see somebody say they're the only
people making a difference, and that the rest of us are just along for the ride,
like so much chaff? The implicit assumption that everyone outside of the rationalist
in-group is an idiot? 

Do you know what my strawman rationalist says in response? "We say that because
it's true. Don't you know? Denying the truth doesn't make it any less true."

\* \* \*
{: .centered }

Another frustrating thing is the heavy, heavy focus people in AI safety have
on timelines.

Now, before I complain, I'll admit that this makes sense, and that I'm not sure
there are better alternatives.

On the other hand, a lot of people I know don't like making predictions about
the long term trends of technology. I don't like to do this either, because
of how hard it is to predict technological progress.

It feels like people are reading tea leaves, and every paper's another leaf.
I've noticed a strong allergy against making any predictions too far out into
the future, because even the short term progress is hard, and it's hard to
estimate the difficluty of problems, and people don't want to overpromise
and underdeliver because hype is so easy to create and so difficult to keep
in check.

So, when I see articles about neural cryptography, and people talking about
steganography in AGI, or somebody releases WaveNet and somebody speculates on
when we'll see an AI-generated pop song hit the Billboard top 100, a part of
me dies.

I think a lot of times, when a big company releases a blog post on a neat
ML paper, people try to interpret what the point of it is, why it'll make
money, and so on. And in my experinece, most research happens because people
think it's really, really cool to see what's doable. This applies to all fields
of research, by the way. I was talking to one person about AI safety, and she
told me that although she kept seeing people say they wanted to do AI safety for
utilitarian reasons, she didn't feel she was utilitarian. She was just super,
super interested in mathematical models of cognition. AI safety let her
study those problems in a context that could be important, down the line.

This is my impression of a lot of AI safety research. People get incredibly
drawn to the problem, find it fascinating, and then try to funnel their
abilities into something that's vaguely safety-like. Researchers from MIRI
come from a pure math background, with focus on mathematical logic and
metamathematics, and that leads to them releasing work on self-reflective
Turing Machines and probabilistic proof systems.

This is the other reason I have trouble taking any arguments out of MIRI
too seriously - part of me always suspects that their arguments are partially
post hoc justifications to align their research abilities with their
research trajectory. Everyone does this - the joke in public health is that
everyone has a good argument for why their cause is important to deal with.
Researchers always work on problems that seem important to them, and they're
also always telling people why it's important to them, and the problem I have
is when people get too dogmatic on why *you and you and you and you* should
be working on the same thing.

\* \* \*

There are many reasonable arguments, many of which have competing conclusions.
I could just as easily believe something else, and it's mostly luck that changes
which argument I see first. So it feels wrong to commit too strongly to any
particular action, or to advocate for anything very strongly.

And, again, this isn't a unique viewpoint: Nick Bostrom gave a talk on this,
giving an example of how a person argues themself into and out of voting
for someone. (LOOK IT UP). Eventually he concludes with saying that you
probably should put more penalty on committing to specific arguments, especially
with regards to x-risk (CHECK IF THIS CONCLUSION ACTUALLY HOLDS UP.)

So, you know. I'd like to think that fundamentalist rationalists at least keep
things like this in mind, but I'm not sure they do.

\* \* \*

Now, of course, I'm not sure these claims actually apply to anybody.

On average, the people I've met that have relations to LessWrong in some way
are generally alright.

Really, it seems like the problem is perception. For better or worse, the ideas
that pull the community together also attract a lot of weirdness. Is that bad?
Well, no, not if you're okay with weird. Some people aren't okay with weird.
And the weird people also tend to be the loudest or the most likely to speak
up, and soon you hit the classic filter bubble problem - the people who talk
aren't representive of the people who exist.
