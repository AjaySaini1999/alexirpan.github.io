---
layout: post
title:  "Sometimes I Think About AI Safety and Rationalists and I Get Depressed For a Bit"
date:   2017-05-02 01:14:00 -0700
---

OUTLINE

1. Explain that I have problems with the current conversations around AI safety.
2. Summarize the streotypical argument for superintelligence.
3. Explain my opinion about it.
4. Use this to lead into the complains I have about the conversations.
3. Complaints about the common rationalist point of view (fair portrayal
or not).

The goal here is not necessarily to justify the point of view of ML researchers.
It's to explain why people get so exasperated when people talk about AI safety.

Trying not be vitriolic because I understand the arguments, but that isn't the
same thing as wanting to actually talk about them all the time.

\* \* \*
{: .centered }

*Warning: long, good chance I'll regret writing this. Opinion as of July
2017. Personal opinion, does not reflect opinion of anyone I've worked with.*

This post has been very difficult to write.

I've been trying to write it for well over a year. I found some notes
from 8 months ago that I don't agree with anymore. Six months ago,
I looked at a draft, and the writing was so revolting that I
deleted it out of disgust.

I've been trying to write this post for over a year.

There are some notes from 8 months ago that I don't agree with anymore.
There was a draft from 6 months ago, which I deleted out of disgust.

All my feelings on this subject are...jumbled. A cacophony of latent
beliefs, hazy thoughts, and lots and lots of frustration at my inability to
explain things to myself. At this point, I just want it to be *done*,
because I'm sick of bottling up monologues.

That feels like a problem. Let's dig in and do some...*introspection.*

![Panel from SMBC comic about introspection](/public/ai-safety/introspection.png)
{: .centered }

Source: [SMBC](http://www.smbc-comics.com/comic/2011-04-25)
{: .centered }

\* \* \*
{: .centered }

Roughly, AI safety centers around this question: "HOw do we make sure
AI acts the way we want it to?" The core premise is that machine learning
is a transformative technology, with a lot of potential for good and bad,
and it would be nice if we could avoid the bad stuff.

AGI stands for artificial general intelligence, a hypothetical machine
learning system that can perform any task at least as well as the average
human could. Depending on how pessimistic you are, some people believe that
AGI will most likely kill all life on Earth, and then kill all life in the
galaxy in a death wave that spreads at nearly the speed-of-light.

This is a controversial opinion, to put things mildly.

I'm not going to attempt to cover all the arguments.
Like all controversial topics, there are thousands of pages worth of discussion,
scattered across the Internet.

What I'm going to explain is a more personal problem: I usually avoid
participating in any of them, because very often, I get turned off for one reason
or another, then leave.

\* \* \*
{: .centered }

Now, I know I said I wasn't going to try to cover all the arguments, but I have
to cover at least some of them.

Here is the rough foundation for AI safety.

* At some point, there will be an AI that can do any task at a superhuman level.
* That AGI will be told to do something.
* Almost all tasks are easier if you can think faster, so the AI will
decide to improve itself as much as it can.
* Almost all tasks are easier if you're alive, so the AI will decide to
minimize the chance anybody disables it.
* Therefore, a sufficiently smart AI may decide to kill all humans, to stop
any humans from disabling it.
* And furthermore, this may happen even if the task it is trying to do is
simple.

This is the value alignment part of the argument, and is the core of pretty
much all concerns on the subject.

Here's an alternate way to form the argument.

* Blindly optimizing an objective can lead to lots of unexpected behavior.
* When humans optimize for something, they usually follow some implicit
ethics that stops them from doing anything too heinous.
* A superhuman AI may not have those implicit ethics, and the unexpected
behavior could include killing all humans.

The other core component is the intelligence explosion argument.

* Suppose there is an AI that can do scientific research at a human level.
* Humans have discovered new technology and inference algorithms by doing
research, and presumably such an AI could do the save.
* If that AI applies those advances to itself, it can speed up its own
rate of discovery.
* Therefore there's a chance that once AI reaches a certain point, it can
repeatedly discover new techniques to improve itself, faster and faster,
until it reaches the level of superhuman performance that people worry about
for the value alignment argument.
* And furthermore, by a similar value alignment argument, if an AI is asked
to do any task, it may decide to pursue intelligence explosion, because it'll
make performing the task easier.

Intelligence explosion is foundational to many of the arguments people like
to advocate for, because if you assume that a system can go from barely
capable to superhuman very quickly, then you can say that it'll be unclear
when an AI has the potential to become superhuman, which makes it harder for
people to tell when they should be careful. Or you can say that an AI can go
from dumb to actually-smart-but-pretending-to-be-dumb. I'm sure there are
other arguments I'm missing.

It's also foundational because it lets you argue that people who disagree about
AI safety are simply falling prey to underestimating exponential growth.

It's also foundational because it pushes the timeline for when AGI happens to
be a lot sooner.

\* \* \*
{: .centered }

Right now, my broad view is that this is a good argument for why AI safety
is worth thinking about, but past that it comes down to your beliefs about
the world. To me, it is very obvious that blindly maximizing a simple objective
can lead to weird, unintended behavior. This shows up in reinforcement learning
all the time - it's not a novel idea. I think most ML researchers also find this
obvious, it shows up all over the place. If you want to produce
lots of papers, you do different research than if you want to produce good
work.

The follow-on argument is that people expect safety problems to be difficult,
which I also find reasonable. Once you exit mathematics, it's usually very
hard to prove something can never happen. The vague intuition from computer
security is that systems are often much more vulnerable than you'd expect,
even if they're based on a core kernel of extremely well-tested theory, just
because of software bugs, human error, and social engineering. The attack
surfaace is almost always larger than you imagine it to be.

However, both of these are only arguments for the difficulty of the problem.
They don't argue anything about when/if AGI is going to happen. My guesses are
fairly pessimistic, for a few reasons.

* I believe neural nets are here to stay, but I'm skeptical deep learning
is enough for AGI. The hype around deep learning is well-founded, but they still
have heavy limitations. Intuitively, many of these limitations feel incredibly
foundational.
* I don't see a clear roadmap for making an AGI of any kind, even if you didn't
care about the safety problems, even if you used all the computers in the world
in some way. I know people like to talk about exponential
growth, or iterated sigmoids that look exponential, but even then I don't see
it.

At this point, I feel like people immediately jump to saying you should support
MIRI or OpenAI or Open Philantrhophy Project, or FHI. You know, the
stereotypical rationalist / EA options.

This is roughly the point where I have the largest mental disconnect. This
probably isn't fair to AI safety enthusiasts, but it feels like people jump
from a reasonable argument to advocacy of surprisingly few options, and
discussion of topics that don't feel important to me.

Let's take MIRI, for example. I have skimmed some of MIRI's work, and gone to
some presentations by people from MIRI. It's all very theoretical, and I have
a hard time believing any of it is going to be useful. They've picked such a
theoretical view by this argument:

* Constraining the behavior of systems is hard.
* Constraining the behavior of systems with superhuman intelligence is going to
be *extra* hard, and it's unlikely that safety efforts for current ML
systems are going to help once AI progresses enough.
* We aren't quite sure how to do it either, but a formal, mathematical model
of cognition is likely to be useful for reasoning about cognition in artificial
intelligence.

I don't agree with all of these points, but all the surveys on AI timelines
show there's close to no agreement on whether AGI is possible or when it's
going to happen. In the face of uncertainty, the proper approach is for people
to try many things. MIRI's research agenda is reasonable as One Of The Things
People Can Try.

So I'm going about my day, trying to stay reasonable about the whole thing, and
then imagine somebody declaring that you should be donating all your money to
MIRI, because they are *literally* saving the world in the most
cost-efficient way, and how could you possibly disagree with that if you want
to call yourself a rationalist?

At which point I find it very easy to reject the label and not call myself
a rationalist.

No, no one has ever said that to me. But it's how I model the group of
hardcore rationalists that got into the movement by falling in love with
Yudkowsky's writing - a group of people who agreed with someone so much they
forgot to ask themselves whether they should. People who are busying warning
people that an AGI in a box can convince a human to do anything, without
considering the obvious follow-up - a smart enough person convince another
to believe anything they wanted to.

(Try to be fairer to Eliezer here because the framing right now is very
unfair.)

AI captures the imagination. Stories about unaligned AGI even more so. By
now, I've read several allegories about how AGI with a simple value function
will kill us all. I've read three "rationalist fanfic" stories, all of which
indepdently decided to digress into the simulation hypothesis, cryonics,
and layman explanations of quantum mechanics. This is an exceedingly
narrow range of topics for a group that I'd expect to pride itself on novel
thinking. To paraphrase a friend, "I thought rationalists were supposed to
be about thinking carefully about your comparative advantages, and instead
people keep saying I should try modafinil, sign up for cryonics, and enter
polyamorous relationships."

I play a lot of Dominion, an exceedingly narrow interest by any metric.
Somebody linked a post from Slate Star Codex. One of the posters was amazed by
it. Within a week, he added links to the Sequences and SSC to his signature,
drops random Bayesian probabilities in all his posts, and starts a thread
on superintelligence in the general discussion forum, asking people whether
they're closer to Eliezer Yudkowsky or Robin Hanson.

This was an astoundingly quick change, from "guy who has mentioned
effective altruism", to "most stereotypical rationalist I've ever seen",
and it feels weird that anybody would shift how they act so quickly.

\* \* \*
{: .centered }

(Can't figure out this transition.)

If

It hurts more when people have varying opinions of what "intelligence" means,
and treat every new deep learning paper as a Potential Sign of the End Times.

So on one hand, there's a group of people who aren't very informed on the
field. On any article about AI, they bring up Skynet, Terminators, or HAL.
It gets old really fast, but I'm generally fine with this. Not everyone
is going to understand machine learning, and I broadly don't expect
uninformed people to know any better. In the same vein, I recognize that
my opinion on literary theory, biology, and any chemistry/physics past
the high school level is complete, utter garbage.

Broadly, it's because it feels like they should know better. It feels like
a lot of the discussion is at a high level, uses imprecise definitions to
make broad claims, and to me both those communities pride themselves on
carefully considering all sides of the argument - except, seemingly, for
AI safety.

I know this isn't a fair portrayal of the community. That's part
of the problem.

\* \* \*
{: .centered }

over time, my enthusiasm for these topics has died down. it's not that i think
they're bad ideas. it's that it feels like some people are *significantly*
more excited about them than they should be, and every time i see it i get a
little bit sick of it.

it's hit that point that i now have a model of "the strawman rationalist".
The strawman rationalist is not fair. The strawman rationalist does not match
anyone I know, and most likely doesn't match anybody in the world, not even
the most hardcore rationalist (if such a thing can even be said to exist.)
It is an amalgam of every negative rationalist stereotype I can think of.
I find it easy to feed it with more ammunition, which I find incredibly
disappointing.

\* \* \*
{: .centered }


Nowadays, when people bring up the same topics,
talk about parables of AI safety, I just get
sick of it.

By this point I've read at least three "rationalist fanfics" that were basically
all AI safety parables. Oh look, you tell an AI to make lots of paperclips and
it invents nanotech and converts all matter to paperclips. Hence, paperclip
optimizer. Oh look, you tell an AI to write lots of notes and it hides its
intelligence to fool people into thinking it's not very smart, then kills the
human race with neurotoxin to stop it from writing lots of notes. Bam, the
scenario from Tim Urban's Wait but Why. Oh look, you tell an AI to
"maximize happiness of the game's players" and it manipulates all the humans until
they play the game. Oh look, Santa's elves are real, and if you ask them to make
people happen they create wireheading and have to be talked out of it by
The Sane Person Who Knows This Is a Bad Idea. *I get it, I got it the first
time.* And I know you're telling these parables because you think the ideas
are important, and I know some people are reading about paperclip optimizers
for the first time, but isn't it natural to get sick of paperclip optimizers
the 5th time through?

I watched Ex Machina because people hyped it up as great. It was a good movie,
but not for me. To me it felt like it was presenting a bunch of old ideas
as if they were new. I didn't need to hear Yet Another Debate About The Chinese
Room Thought Experiment because *I really don't care.*

Really, that's the crux of the problem I have about EAs and rationalists.
The organizing principle of EAs is that you can have much larger impact if
you donate according to careful argument instead of what feels right.
The organizing principle of rationalists is that the human brain is horribly
horribly broken, and it's worth trying to optimize your life, even if you're
never going to be perfect.

But in practice, it feels like the organizing principle is for people to
endlessly debate the merits of different causes, different organizations,
and the right way to live your life. That's fine, but sometimes I feel like
people are more interested in philosophy than the original question.
I can understand why. Philosophy is fun in moderation, but not much more than
that.

An aggressive reductionist desire to categorize and name everything about
the world. (It's not a coincidence that people who do math end up intersecting
in these circles a lot - all of math runs on this kind of thinking.)


\* \* \*
{: .centered }

Depending on the circumstance, I can pass as an EA or as a rationalist.
If I want people to think I'm an effective altruist, 
I talk about the marginal utility of money, or Against Malaria Foundation,
or animal suffering. If I want people to think I"m a rationalist, I can
play up cognitive biases, and talk about different x-risks, or ways
to exploit comparative advantages in the market.

The thing is, I'm neither? I'm not an effective altruist, and I'm not
a rationalist.

"But, you went to EAGxBerkeley, then went to EA Global, then went to
a Slate Star Codex meetup hosted at the CFAR office, *and then went
to a CFAR workshop*. How are you *not* an EA or a rationalist?"

And I mean, that's all true, but when I say I'm not an EA, and not
a rationalist, it feels right.
The thing is, I don't consider myself to be an effective altruist, and
I don't consider myself to be a rationalist. I'm merely a bystander to
both movements.

\* \* \*
{: .centered }

Now, given how awful the strawman rationalist is, why do I keep him around?
He's always smug, always insufferable, always tries to find a new reason
to ignore arguments that machine learning could possibly end okay, and believes
that he and his cohort are the only people who are doing anything meaningful
to protect people. To the strawman, everyone else is a well-meaning but
misguided idiot, but I'm not, oh no I'm not, because I've read the Sequences!

I keep him around because despite how crazy he is, he still manages to give
good insights.

What I do want to stop, however, is the vaguely adversarial way people on both
sides have framed the AI safety debate. It's not overt or anything, more like
a collection of snide remarks and jokes I hear now and then. A person from MIRI,
saying that at least 2 professors take them seriously, one of whom is
well-respected. Another person from MIRI, joking that giving DeepMind tons
of compute power may have been a bad idea. An effective altruist, contrasting
the money going into AI vs the money going into AI safety. Two people have
independently joked that if AGI is imminent, maybe we need kill all the AI
researchers, which really stops being funny when you're one of those
researchers.

On the flip side, a lot of established ML people have criticized people
for bad reasons. They have weird opinions on some basilisk thing,
why should I listen to these people. He's a professor of philosophy, I'll
listen to people who do more ML.

The thing is, I broadly agree with the argument for AI safety, if you assume
a sufficiently powerful AI. And furthermore, none of the ML researchers I know
dispute the argument, given the premise.
If you do enough reinforcement learning, it's
obvious that reward function design is really hard, because of how easy it
is to get stuck at local optima. If you blindly optimize training set loss,
you overfit to the data. If everyone designs algorithms and tests them on
32 x 32 CIFAR images, they're not going to generalize as well because people
have trial-and-errored there way to methods that only work well on CIFAR.
A common metric in NLP is BLEU, which you can optimize with RL, but if you
do so you don't get better human-graded performance.

People do things like debate the orthogonality thesis, or debate whether
an AI can convince a human to let it out of the box, but these are debates
about systems that don't exist yet, and so they quickly turn into unverifiable
philosophy.


Rough notes for things I want to bring up.

* Some people are first introduced to cognitive biases and Bayesian thinking
through the rationalist community. I think some then make the mistake of
assuming that these insights are *unique* to rationalists. The collection
of all the ideas in one place is special, but the ideas themselves are not.
I feel this is why I've been so skeptical of the community - it feels
too insular, in the way everybody cites everybody else because they only
want to cite trustworthy EAs or rationalists, with a seeming lack of awareness
of how ridiculous they look to outsiders.
* The general feeling of "this is totally cult behavior", social pressure
to agree with The Hot New Thing or to feel like A Member Of The Resistance.
* Why AI safety in particular, versus any other X-risk like nuclear proliferation,
climate change, alien invasion, bioterrorism, preventing creating of
nanotechnology, etc?
* It is okay for some people to be passionate about X-risk, and for some people
to be passionate about other things. The part I can't stand is "when people
mistake their cause for a religion"
* "Introduced to these ideas, and then you're stuck in the convex hull of ideas / the
convex hull of trust, and soon you don't trust anybody else."
    * Some people do this, but the loudest are the most fanatic, and it feels
    like the most fanatic don't. Or they've been so disappointed that they just
    give up on talking to any other people.
    * The example of the AI safety paper that has a citation of "I don't like
    discussing AI safety with people who haven't read Superintelligence"
* Concern that people advocating for AI safety are doing so for charlatan
reasons (that they don't actually believe in the dangers, they just want to play
up the dangers to get money.)
    * I'm pretty sure this isn't true for most people.
* The feeling that I had when reading the Open Phil grant - it sounds like
you're giving OpenAI money because in some philosophical thought experiment,
they agreed with your viewpoint. This seems like a really, really weak reason.

The general trend I've noticed is that some people like to rant about rationalists.
But over time, it turns out they rant specifically about Yudkowsky and people
with similarly weird beliefs to him. And that they appreciate the people who
don't seem to take the whole thing as seriously - people who don't feel the
need to talk about Bayes Theorem and utilitarianism all the time.

Once, a friend on Facebook asked (CHECK),

> Aliens have just visited Earth, and they ask for 10 people to represent all
> of humanity. Who do you pick?

One commenter said Bostrom should obviously be one of them. And I remember thinking
that this was an oddly controversial pick, unless you were in the in-group.

Yes, I know you've seen the skulls. Yes, I know that some people in these groups
are aware their beliefs are fluid. Yes, I've read CFAR's mission statement:
beliefs, weakly held, but acted on anyways.

The problem is that it's not clear these virtues are universalized, it's not clear
this is common knowledge, and people view rationalists as much more homogenous
than they actually are.

This isn't actually that weird, when you think about it. Let me put it this way:
I decided to read three "rationalist fanfic" stories, and they all indepedently
decided they needed to explain the many worlds interpretation of quantum
mechanics in laymans terms. They also all felt the need to talk about characters
"making updates about the world". Then there was simulation hypothesis, then
wireheading, then cryonics.

Do you realize how condescending it is, to see somebody say they're the only
people making a difference, and that the rest of us are just along for the ride,
like so much chaff? The implicit assumption that everyone outside of the rationalist
in-group is an idiot? 

Do you know what my strawman rationalist says in response? "We say that because
it's true. Don't you know? Denying the truth doesn't make it any less true."

\* \* \*
{: .centered }

Another frustrating thing is the heavy, heavy focus people in AI safety have
on timelines.

Now, before I complain, I'll admit that this makes sense, and that I'm not sure
there are better alternatives.

On the other hand, a lot of people I know don't like making predictions about
the long term trends of technology. I don't like to do this either, because
of how hard it is to predict technological progress.

It feels like people are reading tea leaves, and every paper's another leaf.
I've noticed a strong allergy against making any predictions too far out into
the future, because even the short term progress is hard, and it's hard to
estimate the difficluty of problems, and people don't want to overpromise
and underdeliver because hype is so easy to create and so difficult to keep
in check.

So, when I see articles about neural cryptography, and people talking about
steganography in AGI, or somebody releases WaveNet and somebody speculates on
when we'll see an AI-generated pop song hit the Billboard top 100, a part of
me dies.

I think a lot of times, when a big company releases a blog post on a neat
ML paper, people try to interpret what the point of it is, why it'll make
money, and so on. And in my experinece, most research happens because people
think it's really, really cool to see what's doable. This applies to all fields
of research, by the way. I was talking to one person about AI safety, and she
told me that although she kept seeing people say they wanted to do AI safety for
utilitarian reasons, she didn't feel she was utilitarian. She was just super,
super interested in mathematical models of cognition. AI safety let her
study those problems in a context that could be important, down the line.

This is my impression of a lot of AI safety research. People get incredibly
drawn to the problem, find it fascinating, and then try to funnel their
abilities into something that's vaguely safety-like. Researchers from MIRI
come from a pure math background, with focus on mathematical logic and
metamathematics, and that leads to them releasing work on self-reflective
Turing Machines and probabilistic proof systems.

This is the other reason I have trouble taking any arguments out of MIRI
too seriously - part of me always suspects that their arguments are partially
post hoc justifications to align their research abilities with their
research trajectory. Everyone does this - the joke in public health is that
everyone has a good argument for why their cause is important to deal with.
Researchers always work on problems that seem important to them, and they're
also always telling people why it's important to them, and the problem I have
is when people get too dogmatic on why *you and you and you and you* should
be working on the same thing.

\* \* \*

There are many reasonable arguments, many of which have competing conclusions.
I could just as easily believe something else, and it's mostly luck that changes
which argument I see first. So it feels wrong to commit too strongly to any
particular action, or to advocate for anything very strongly.

And, again, this isn't a unique viewpoint: Nick Bostrom gave a talk on this,
giving an example of how a person argues themself into and out of voting
for someone. (LOOK IT UP). Eventually he concludes with saying that you
probably should put more penalty on committing to specific arguments, especially
with regards to x-risk (CHECK IF THIS CONCLUSION ACTUALLY HOLDS UP.)

So, you know. I'd like to think that fundamentalist rationalists at least keep
things like this in mind, but I'm not sure they do.

\* \* \*

On average, the people I've met that have relations to LessWrong in some way
are generally alright.

Really, it seems like the problem is perception. For better or worse, the ideas
that pull the community together also attract a lot of weirdness. Is that bad?
Well, no, not if you're okay with weird. Some people aren't okay with weird.
And the weird people also tend to be the loudest or the most likely to speak
up, and soon you hit the classic filter bubble problem - the people who talk
aren't representive of the people who exist.



No true Scotsman problem.

The problem people have is mostly with Yudkowsky and beliefs close to those
ones.

The feeling that people are arguing in circles.

The feeling that people from other areas of expertise are advocating for the
right thing to do, talking about something they don't understand, and knowing
that it's easy for people to be mislead about things, and the difficulty
in distinguishing between crackpot and valid point.

In rationalist circles, it feels like people very quickly decide to follow
whatever argument rings true with them, and then directly act on it. Whereas
other people are hesitant to predict anything too far out because of the
difficulty of the prediction, and don't see how it's supposed to change anything
or what the point of it is. "The best we can do" vs "not even worth the effort
right now"
