---
layout: post
title:  "Sometimes I Think About AI Safety and Rationalists and I Get Depressed For a Bit"
date:   2017-05-02 01:14:00 -0700
---

*Warning: good chance I'll regret writing this. Opinion as of August
2017. Personal opinion, does not reflect opinion of anyone I've worked with.
I reserve the right to change my opinion later.*

*This post has been very difficult to write.
In the course of writing this, I've found notes from 8 months ago that I don't
agree with, and a draft from 6 months ago that was so revolting I deleted it out
of disgust. The general structure of this post has changed at least four times
and I'm still not that satisfied with it. Perfection is the enemy of the good, but
on this topic, I wanted to make sure I did it right, because of how much of my life
ends up intersecting with the topic.*

*I say all this to let you know that although I'm doing my best to explain my position,
it is almost certain that I haven't. I'm hoping that I got close enough.*


What Is AI Safety?
----------------------------------------------------------------------------------

There are several treatises on the subject, but I think there's value in showing
my frame on the problem.

Roughly, AI safety centers around this question:

> How do we make sure AI acts the way we want it to?

Very often, people narrow it down to this question:

> When artificial general intelligence arrives, how do we make sure AGI does
> what we want it to do?

Just in case you're unfamiliar withe the terms: AGI stands for artificial
general intelligence, a hypothetical machine learning system that can perform any
task at least as well as the average human could. This includes learning new tasks at
least as well as the average human, becuase learning is a thing humans can do.

Now, depending on how pessimistic you are, some people believe that
AGI will most likely kill all life on Earth. Depeding on your opinions on the limits
of technology, maybe it'll kill all life in the galaxy in a death wave that spreads
at close to the speed-of-light.

This is a controversial subject, to put things mildly. It has generated thousands of
pages worth of discussion all across the Internet. I'm not going to attempt to cover
all the arguments, because I don't know all the arguments. I'll cover the ones I know,
and then I'll get into my reservations.


The Existential Risk Inductive Chain
---------------------------------------------------------------------------------

This is the foundation of AI safety advocacy - pretty much everybody specifically
concerned about the x-risk aspects of machine learning knows a spiel for this
by heart.

* Suppose that at some point, there is an AGI that can do any task at a superhuman level.
* That AGI will eventually be given some goal.
* Almost all tasks are easier if you can think faster, so the AGI will
decide to self-improve as quickly as possible.
* Almost all tasks are easier if you have control, so the AGI will decide to
minimize the chance anybody can disable it. Redundant copies, metaphorically
"hiding the off-switch", etc.
* Combined, such an AGI may first self-improve past the point of humanity's ability
to deal with it, then decide to kill all humans, because humans can't turn you off
if all the humans are dead.
* Importantly, this could potentially happen for any task, even if the AGI's
goal sounds benign, because again - everything's easier if you're smarter and you're
alive.

(...at which point, I imagine someone saying, "And that is why you should donate at
least 10% of your income to a mix of MIRI and OpenAI". But we'll get into that later.)

This part of the argument is called *value alignment*, and people refer to "the
value alignment problem" in similar terms.

Here's an alternate way to form the argument.

* Blindly optimizing an objective can lead to lots of unexpected behavior.
* When humans optimize for something, they usually follow some implicit
ethics that stops them from doing anything too heinous.
* Superhuman AGI may not have those implicit ethics, at which point many
bad things could happen.

People like to focus on the value alignment part of the argument, but the other
component is the *intelligence explosion* component. This is just as pivotal
to the argument as value alignment.

In the above, this was abstracted as "the AGI will self-improve as quickly as
possible". Let's expand this point.

* Suppose there is an AGI that can do scientific research at human level or
above.
* Humans have discovered better machine learning algorithms by doing
research. By definition, such an AGI could do the save.
* Suppose that AGI discovers some advance. It could apply it to itself, which
speeds up its rate of discovery.
* Therefore, there's a chance that once AI reaches a certain point, it can
repeatedly discover new techniques to improve itself at a faster and faster
rate (i.e. recursive self-improvement leading to exponential gains.)
* Note that it doesn't have to be exponential. One common theory of technology
is that it follows a sigmoid curve. First it's used rarely, then there's a rapid
period of discovery, then the limitations of the technique are figured out and
discovery slows down. The concern is less about a single technique giving exponential
gains. It's more about discovering several techniques, each with their
own period of rapid discovery, making it look like exponential growth when zoomed
out.

I consider this pivotal because people use both the Sword of Value Alignment
and Sword of Intelligence Explosion to fend off the common counterarguments.

* If someone says AGI won't harm humans, you wave the Value Alignment sword and
say it's easy for AGI to accidentally end up hurting humans.
* If someone says it will be obvious when AGI will happen, you wave the Intelligence
Explosion sword and say that by the time it's obvious, it may be too late to do much
about it.
* If someone says AGI is too far off to be concerned about, you wave the Intelligence
EXplosion sword again and tell people they underestimate the potential of exponential
growth.


Thoughts on the Inductive Chain
---------------------------------------------------------------------------

Right now, my broad view is that this is a good argument for why AI safety
is worth thinking about, but past that it very heavily comes down to your beliefs
about technology, which is a very fuzzy subject that's hard to argue much about.

I find it very obvious that blindly maximizing a simple objective
can lead to weird, unintended behavior. This shows up all the time in machine
learning. Minimizing classification loss on your training set is not the same as
maximizing classification accuracy. Maximizing accuracy on your training set isn't
the same as maximizing accuracy in the real world.

On occasion, it helps to model machine learning algorithms as learning the
easiest thing necessary to get good performance, and no more.
In one famous story, the army asked researchers to label images as containing or not
containing a tank. They got good performance on their training data, and failed
miserably in the real world. After inspecting their data, they found that all the images
with tanks in them were taken during the daytime, so their model decided that every bright
image had to have a tank in it. Reward hacking happens when learning something
easy is enough to get good performance.

This is especially important to think about in RL, because in practice it's
very easy to design reward functions with bad local maxima. For example, one task in
OpenAI Gym is to move a two-segment arm to a point. You get reward based on the
distance to that point. When training models on this task, I found my networks would
often learn to spin the arm in a very fast circle. On average, the arm isn't that far
from the point, and it's easy to spin in a circle - just apply the maximum torque at
every joint.

This is one of the simplest RL tasks out there, and already I have to think about
whether there are simple behaviors that are hard to escape from. Once a network
discovers that spinning very fast isn't that bad, it's hard to get it to stop
spinning very fast.

In short, I accept that a superhuman AGI may not do what we expect it to do,
and I've yet to talk to a ML researcher who's said otherwise, because every
ML researcher has run into a model that didn't do what they expected it to do.
Value alignment is not a novel idea - it's telling that Isaac Asimov first wrote
the Three Laws of Robotics, then immediately wrote a short story of a Three-Laws
compliant robot killing humans. Then he came up with a Zeroth Law. Then he
wrote a short story of a Zeroth Law compliant robot that kills humans...

The follow-on argument is that people expect the safety problems to be
difficult, and therefore we should have people thinking about them now. This
also sounds reasonable to me. There's a lesson from computer security: the attack
surface is always larger than you think it is. (This is why security people like to
say that everything is broken.) If you're modeling superhuman AGI as an adversary
that's trying to misinterpret things as horribly as possible, it's easy to
figure something out.

(On occasion, people try to argue it's easy to constrain a superhuman AI system.
I mostly consider these arguments a distraction, because the AI-safety-apologist
can usually pretend the AGI is powerful enough to make these arguments invalid,
by saying "something something intelligence-explosion-then-AGI-argues-itself-out-of-the-box".)

(For similar reasons, I've never really liked reading people debate things like
the orthogonality thesis. In these kinds of arguments, it often feels like people
are leaning on too much philosophy, and are using imprecise or incomplete
descriptions of imaginary objects to argue something fundamental about the world.
And because the inputs are fuzzy, it quickly turns into unverifiable philosophy,
at which point I wonder why I should care.)

Overall, excluding some warts, I think being concerned is fair, if you believe
superhuman AGI is going to happen before people have figured out how to handle
it. My biggest disagreement is on when it happens.


Timelines (AKA "The Part Where Some AI Safety Advocates May Get Mad At Me")
------------------------------------------------------------------------------

I should start by saying that I think a lot of timeline discussion isn't
very useful.

Over the past few years, people have given out surveys to various ML researchers,
asking when or if AGI will happen. Fairly consistently across the board, the
results have shown that there is massive variance. Some people say never, some
people say it'll happen in 10 years with 90% confidence. But if we know that
there's massive variance in the outcome, what's the point in even asking?

When asked, many machine learning researchers don't like giving estimates of
when AI will be able to do something. That's not because they don't care - it's
their research area, of course they care! It's more that they know it's very
hard to predict research progress, so there's not much point in giving an
answer. I fall into a similar view.

(If you prefer more rationalist / decision theory terms: the value of
information in hearing the answer is so low that it's not worth the risk of
giving people weird misconceptions about the field.)

By contrast, I feel like the mainstream rationalist argument is that
existential risk is massively, massively important. And therefore, any piece
of information on timelines, no matter how noisy, is important to hear.

To appease the second group, I'll give my estimate: 10% likelihood within 35 years,
50% likelihood within 50 years, 90% likelihood within 75 years. These numbers
are fairly arbitrary and don't mean much, but I'll explain them anyways.

* I believe neural nets are here to stay, but I'm skeptical that
deep learning by itself is enough. There's a lot of hype around deep learning,
and for good reason, but they can be a bit finicky and it feels like they
have fundamental limitations that are hard to deal with. (It's tricky for
me to articulate what those are.)
* I don't see a clear roadmap for making an AGI of any kind. Even if you
don't care about the safety problems, even if you use all the computers in the
world, and accounting for a Moore's law growth in computational power.
It feels like there are several large conceptual problems in the way,
and more that we can't see right now.
* People have been working on this literally from the start of
the field. I feel like people aren't pricing in what that implies about the
difficulty of the problem. Sure, computers have gotten a lot faster, but
I believe there are big conceptual problems in the way, and progress on
conceptual problems is less dependent on hardware. That the conceptual problems
still exist is a strong indicator that they're quite, quite difficult. (In the same
way that people have yet to give a good refutation to value alignment, despite
how long that problem has been around.)
* Successes get written into papers, presented at conferences, sent through
university and industry PR machines, and get discussed widely. Failures
do not. Because you hear about accomplishments a lot more than failures,
it's easy to extrapolate a faster trend than a slower one, and this makes me
inclined to downweight my guesses.

By that point, I expect enough safety research to be done.
To me, it often feels like EA-minded people frame the argument as
"researchers don't think AI safety is important and we do". The more common
belief I've heard is "I don't think it's worth spending a lot of resources
on safety right now, but I don't mind that people are working on it." Which
is around where I'm at. Right now, the money is in better translation,
better computer vision, and better recommendations. We're still very much at
the start of integrating machine learning into society. Over time I expect
safety problems to be more important for producing good products, which
will put more economic incentive into those lines of research.

For example, the Concrete Problems in AI Safety paper (which I like quite a lot)
mentions a thought experiment of a household robot getting reward if it can't
see any dust. It decides to cover all the furniture with a blanket, because
that's easier than cleaning the room. This is a thought experiment because
household robots don't exist, but if household robots did exist, the one that
does what people expect it to do will be a better product. This starting to
happen with the various voice assistants, like Google and Alexa and Siri and
Cortana, except right now people don't expect them to work all the time because
the tech is so new.

(Aside: I'm very sure you can poke holes in this example, but I'm more
concerned about the holistic point: there are currently few economic incentives
for AI safety, and I expect there will be more economic incentives in the future.)

(Aside 2: I'm guessing this is why OpenAI has a household robot as a research
goal. It creates low-risk scenarios where an agent can't do alot of damage

(Aside 3: This is not a novel idea. Stuart Russell made a similar point in his
keynote talk at the 2016 Bay Area Robotics Symposium. Except his example was a
robot deciding to turn the family pet into food. I guess he wanted to be
shocking.)


Strategy (AKA "The Part Where Even More AI Safety Advocates May Get Mad At Me")
--------------------------------------------------------------------------------

Moving apart from direct theories about AGI, I also have some reservations about
the arguments presented by various organizations.

* Some organiziations (like 80,000 Hours and OpenPhil) have stated that they
believe more people should work on AI safety. I like that people are thinking
about AI safety, and I like that both DeepMind and OpenAI have safety teams.
Both have some room for growth, but I dont think they should grow by much more,
because it's unclear to me that there's enough potential impact.

As a further branching point from this, I think my definition of safety-like
problems is broader than the standard one. To me, all of the following are
safety-like problems.

* Fairness: how do we ensure machine learning isn't discriminatory? Important
in discussiona about racial profiling, eligibility of home ownership,
learned associations between genders and roles.
* Privacy: machine learning techniques can memorize parts of their training
data, which can be extractable by an adversary. This is less of a concern with
images, but is more concerning if you want to apply ML to healthcare - how do
you best prevent leaks of patient data? See differential privacy.
* Adversarial inputs: it's been shown that we can create inputs that get
mislabled with high confidence. Furthermore there are papers showing that
some of these perturbations are universal across all images, that this is
achievable even if you only get black-box queries to the model (like through
a computer vision API), and with proper
training the results are robust to various image transformations. When ML
becomes more prevalent, how do we deal with these concerns?
* Human-computer interaction: how do humans interact with computer agents?
How do humans interpret robot behavior, and in turn, how can we make sure
robots accurately model human behavior?
* Interpretability: decision trees are well-known for being interpretable.
Neural nets are well-known for being harder to interpret. This is a blocker
for areas like medicine, where doctors place a lot of value on understanding
what a model is doing.

These all feel like necessary questions to answer if we want machine learning
to play a larger role in life. I'm not a big x-risk person, but if I preteded
to be a big x-risk person, and bought the AI x-risk argument, I could still
see these approaches as giving useful lessons for the future...except on the
side, it's also more useful in the present day.

Let's take MIRI, for example. I have skimmed some of MIRI's work, and gone to
some presentations by people from MIRI. It's all very theoretical, and I have
a hard time believing any of it is going to be useful. They've picked such a
theoretical view by this argument:

* Constraining the behavior of systems is hard.
* Constraining the behavior of systems with superhuman intelligence is going to
be *extra* hard, and it's unlikely that safety efforts for current ML
systems are going to help once AI progresses enough.
* We aren't quite sure how to do it either, but we expect a mathematical, formal
model of cognition is likely to be useful.

I don't agree with all of this, but this is a reasonable enough argument.
All the surveys on AI timelines suggest there's no agreement on when AGI will
happen, or if it's even possible, and the range is even large within MIRI's
staff. In the face of uncertainty, the proper approach is
to try many things. MIRI's research agenda is reasonable as A Thing
People Can Try.

So I'm going about my day, trying to stay reasonable about the whole thing, and
then start imagining somebody declaring that you should be donating all your money to
MIRI, because they are *literally* saving the world in the most
cost-efficient way, and how could you possibly disagree with that if you want
to call yourself a rationalist?

At which point I get incredibly weary, slightly depressed, and annoyed at the
state of the world.

No one in the rationalist-sphere has ever said something like that to me. But
it's how I model the hardcore segment that got into the movement
by falling in love with Yudkowsky's writing - a group of people who agreed with
someone so much, they forgot to do due diligence and ask themselves whether
they should. A group that spends its days discussing Newcomb problems and
warning people that an AGI can convince a human to let it out of the box,
without considering the obvious corollary - a smart enough person could do
the same, and maybe they already have.

Artificial intelligence captures the imagination. Stories about unaligned AGI
do so even more. I've read three "rationalist fanfic" stories, all of which
independently decided to talk about unaligned AGI, cryonics, and the
many-worlds interpretation of quantum mechanics - which feels like an
exceptionally narrow range of topics for a group that I'd expect to pride
itself on novel, skeptical thinking. I get it - these ideas are interesting
to think about, and you think they're cool and important. But isn't it natural
to get sick of paperclip optimizers when you've read about them for the
5th time?

(Off the top of my head:

1. Superintelligence is told to create lots of notes, invents nanotech, uses up
all the oxygen to kill humans by asphixiation, turns humans into notes.
2. Superintelligence is told to make people happy, decides the best way to do
this is to inject people with a serum that constantly triggers their dopamine receptors.
3. Superintelligence is told to make people laugh by writing good jokes,
it writes one so good that people die laughing.
4. Superintelligence is told to maximize the happiness of players in an MMORPG,
invents brain uploading and manipulates people until everyone has uploaded into
the game.
5. And of course, superintelligence is told to make lots of paperclips, invents
nanotech, kills everyone, turns them into paperclips.

*It took me like, 2 minutes to remember all of these.* Can you blame me for
getting sick of these kinds of stories? I think it's entirely fair for me to get annoyed
with people when present the same argument pattern, over and over again.)

To paraphrase a friend: "I thought rationalists were supposed to
be about studying cognitive biases and figuring out how to exploit your
comparative advantages. Instead everyone asks me to sign up for cryonics,
advocate for AI safety, and try out polyamory."

There's a card game I frequent pretty often, and one day somebody linked a post
from Slate Star Codex. One poster liked it a lot. Within a week, he adds
links to the Sequences and SSC to his signature,
drops Bayesian probabilities in all his posts, and starts a thread asking
whether people's beliefs on superintelligence are closer to Eliezer
Yudkowsky or Robin Hanson.

Honestly, none of this was particularly surprising, because his pattern of
speaking was very similar to how people act in the rationalist sphere.
And at the same time, I think it's sad that it *wasn't* surprising that
anybody could jump from "guy who mentioned EA once" to
"ridiculously stereotypical rationalist" that quickly.

\* \* \*
{: .centered }

In many ways, the rationalist community reminds me of the math contest
community - a group of people who, after much searching, have found people who
finally speak the same language they do, and all the consequences thereof.
It should come as no surprise that the two have a healthily large overlap.

The large focus on AI safety that some people have is wearying and annoying.
I get annoyed when I read people arguing what precisely "intelligence" means.
I get further annoyed when people read a news article about a recent deep
learning paper, and herald it as a Potential Sign of the End Times.

Take, for instance, the Neural Cryptography paper. Here's roughly how it goes:
Alice and Bob are two neural networks with shared parameters. Each agent
has an encoder, which takes a message $$m$$ and key $$k$$, then outputs
some output $$m'$$. There's a decoder, which takes $$m'$$ and $$k$$ and
is trained to reconstruct $$m$$ as closely as possible.

In between the two is Eve, a neural network which receives just $$m'$$,
and tries to reconstruct $$m$$. The two are trained adversarially - Alice
and Bob are trained to maximize Eve's confusion (# of bits it gets wrong),
and Eve tries to maximize the # of bits it gets right.

It's a cute idea, but the experiments were very small - they give each
network an 8 bit message, and an 8 bit key, and the model didn't even learn
a one-time pad, it learned something that kinda worked. I went to a talk
the authors gave, and they talked about how that was the best they got,
even after lots of tuning.

So in short: a cute research idea, with results suggesting you could
*kind of* do this, but not much more than that. It was submitted to ICLR
and rejected for similar reasons, and I think that verdict is fair.

But in the media coverage of the paper....oh jeez, let me pull up the headlines.

HEADLINES HERE.

Meanwhile my transhumanist friends were discussing it as if it was a great
advance, extrapolating to AI agents using steganography to encode messages to
one another, wondering what the next plan was with neural cryptography, and
all sorts of massively oversized speculation given the results of the work.

In moments like these, it's very hard to be fair towards the seeming
hypocrisy - proclaiming the importance of listening to experts and actually
reading the paper, and then selectively ignoring it when talking about AI,
and when confronted about it, pulling up some general heuristic like "well,
you're just underestimating the trend of research, here's a Wikipedia article
for it."

This isn't even a wrong thing to say! It's a valid point! But at the same time
it feels like people aren't aware of the limitations of deep learning, and
are selectively building a worldview based only on its successes, because that's
what they hear about the most.

Broadly, it's because it feels like they should know better. It feels like
a lot of the discussion is at a high level, uses imprecise definitions to
make broad claims, and to me both those communities pride themselves on
carefully considering all sides of the argument - except, seemingly, for
AI safety.

I know this isn't a fair portrayal of the community. That's part
of the problem.

\* \* \*
{: .centered }

it's hit that point that i now have a model of "the strawman rationalist".
The strawman rationalist is not fair. The strawman rationalist does not match
anyone I know, and most likely doesn't match anybody in the world, not even
the most hardcore rationalist (if such a thing can even be said to exist.)
It is an amalgam of every negative rationalist stereotype I can think of.
I find it easy to feed it with more ammunition, which I find incredibly
disappointing.

\* \* \*
{: .centered }


I watched Ex Machina because people hyped it up as great. It was a good movie,
but not for me. To me it felt like it was presenting a bunch of old ideas
as if they were new. I didn't need to hear Yet Another Debate About The Chinese
Room Thought Experiment because *I really don't care.*

Really, that's the crux of the problem I have about EAs and rationalists.
The organizing principle of EAs is that you can have much larger impact if
you donate according to careful argument instead of what feels right.
The organizing principle of rationalists is that the human brain is horribly
horribly broken, and it's worth trying to optimize your life, even if you're
never going to be perfect.

But in practice, it feels like the organizing principle is for people to
endlessly debate the merits of different causes, different organizations,
and the right way to live your life. That's fine, but sometimes I feel like
people are more interested in philosophy than the original question.
I can understand why. Philosophy is fun in moderation, but not much more than
that.

An aggressive reductionist desire to categorize and name everything about
the world. (It's not a coincidence that people who do math end up intersecting
in these circles a lot - all of math runs on this kind of thinking.)


\* \* \*
{: .centered }

Depending on the circumstance, I can pass as an EA or as a rationalist.
If I want people to think I'm an effective altruist, 
I talk about the marginal utility of money, or Against Malaria Foundation,
or animal suffering. If I want people to think I"m a rationalist, I can
play up cognitive biases, and talk about different x-risks, or ways
to exploit comparative advantages in the market.

The thing is, I'm neither? I'm not an effective altruist, and I'm not
a rationalist.

"But, you went to EAGxBerkeley, then went to EA Global, then went to
a Slate Star Codex meetup hosted at the CFAR office, *and then went
to a CFAR workshop*. How are you *not* an EA or a rationalist?"

And I mean, that's all true, but when I say I'm not an EA, and not
a rationalist, it feels right.
The thing is, I don't consider myself to be an effective altruist, and
I don't consider myself to be a rationalist. I'm merely a bystander to
both movements.

\* \* \*
{: .centered }

Now, given how awful the strawman rationalist is, why do I keep him around?
He's always smug, always insufferable, always tries to find a new reason
to ignore arguments that machine learning could possibly end okay, and believes
that he and his cohort are the only people who are doing anything meaningful
to protect people. To the strawman, everyone else is a well-meaning but
misguided idiot, but I'm not, oh no I'm not, because I've read the Sequences!

I keep him around because despite how crazy he is, he still manages to give
good insights.

What I do want to stop, however, is the vaguely adversarial way people on both
sides have framed the AI safety debate. It's not overt or anything, more like
a collection of snide remarks and jokes I hear now and then. A person from MIRI,
saying that at least 2 professors take them seriously, one of whom is
well-respected. Another person from MIRI, joking that giving DeepMind tons
of compute power may have been a bad idea. An effective altruist, contrasting
the money going into AI vs the money going into AI safety. Two people have
independently joked that if AGI is imminent, maybe we need kill all the AI
researchers, which really stops being funny when you're one of those
researchers.

On the flip side, a lot of established ML people have criticized people
for bad reasons. They have weird opinions on some basilisk thing,
why should I listen to these people. He's a professor of philosophy, I'll
listen to people who do more ML.

Rough notes for things I want to bring up.

* Some people are first introduced to cognitive biases and Bayesian thinking
through the rationalist community. I think some then make the mistake of
assuming that these insights are *unique* to rationalists. The collection
of all the ideas in one place is special, but the ideas themselves are not.
I feel this is why I've been so skeptical of the community - it feels
too insular, in the way everybody cites everybody else because they only
want to cite trustworthy EAs or rationalists, with a seeming lack of awareness
of how ridiculous they look to outsiders.
* The general feeling of "this is totally cult behavior", social pressure
to agree with The Hot New Thing or to feel like A Member Of The Resistance.
* Why AI safety in particular, versus any other X-risk like nuclear proliferation,
climate change, alien invasion, bioterrorism, preventing creating of
nanotechnology, etc?
* It is okay for some people to be passionate about X-risk, and for some people
to be passionate about other things. The part I can't stand is "when people
mistake their cause for a religion"
* "Introduced to these ideas, and then you're stuck in the convex hull of ideas / the
convex hull of trust, and soon you don't trust anybody else."
    * Some people do this, but the loudest are the most fanatic, and it feels
    like the most fanatic don't. Or they've been so disappointed that they just
    give up on talking to any other people.
    * The example of the AI safety paper that has a citation of "I don't like
    discussing AI safety with people who haven't read Superintelligence"
* Concern that people advocating for AI safety are doing so for charlatan
reasons (that they don't actually believe in the dangers, they just want to play
up the dangers to get money.)
    * I'm pretty sure this isn't true for most people.
* The feeling that I had when reading the Open Phil grant - it sounds like
you're giving OpenAI money because in some philosophical thought experiment,
they agreed with your viewpoint. This seems like a really, really weak reason.

The general trend I've noticed is that some people like to rant about rationalists.
But over time, it turns out they rant specifically about Yudkowsky and people
with similarly weird beliefs to him. And that they appreciate the people who
don't seem to take the whole thing as seriously - people who don't feel the
need to talk about Bayes Theorem and utilitarianism all the time.

Once, a friend on Facebook asked (CHECK),

> Aliens have just visited Earth, and they ask for 10 people to represent all
> of humanity. Who do you pick?

One commenter said Bostrom should obviously be one of them. And I remember thinking
that this was an oddly controversial pick, unless you were in the in-group.

Yes, I know you've seen the skulls. Yes, I know that some people in these groups
are aware their beliefs are fluid. Yes, I've read CFAR's mission statement:
beliefs, weakly held, but acted on anyways.

The problem is that it's not clear these virtues are universalized, it's not clear
this is common knowledge, and people view rationalists as much more homogenous
than they actually are.

This isn't actually that weird, when you think about it. Let me put it this way:
I decided to read three "rationalist fanfic" stories, and they all indepedently
decided they needed to explain the many worlds interpretation of quantum
mechanics in laymans terms. They also all felt the need to talk about characters
"making updates about the world". Then there was simulation hypothesis, then
wireheading, then cryonics.

Do you realize how condescending it is, to see somebody say they're the only
people making a difference, and that the rest of us are just along for the ride,
like so much chaff? The implicit assumption that everyone outside of the rationalist
in-group is an idiot? 

Do you know what my strawman rationalist says in response? "We say that because
it's true. Don't you know? Denying the truth doesn't make it any less true."

\* \* \*
{: .centered }

Another frustrating thing is the heavy, heavy focus people in AI safety have
on timelines.

Now, before I complain, I'll admit that this makes sense, and that I'm not sure
there are better alternatives.

On the other hand, a lot of people I know don't like making predictions about
the long term trends of technology. I don't like to do this either, because
of how hard it is to predict technological progress.

It feels like people are reading tea leaves, and every paper's another leaf.
I've noticed a strong allergy against making any predictions too far out into
the future, because even the short term progress is hard, and it's hard to
estimate the difficluty of problems, and people don't want to overpromise
and underdeliver because hype is so easy to create and so difficult to keep
in check.

So, when I see articles about neural cryptography, and people talking about
steganography in AGI, or somebody releases WaveNet and somebody speculates on
when we'll see an AI-generated pop song hit the Billboard top 100, a part of
me dies.

I think a lot of times, when a big company releases a blog post on a neat
ML paper, people try to interpret what the point of it is, why it'll make
money, and so on. And in my experinece, most research happens because people
think it's really, really cool to see what's doable. This applies to all fields
of research, by the way. I was talking to one person about AI safety, and she
told me that although she kept seeing people say they wanted to do AI safety for
utilitarian reasons, she didn't feel she was utilitarian. She was just super,
super interested in mathematical models of cognition. AI safety let her
study those problems in a context that could be important, down the line.

This is my impression of a lot of AI safety research. People get incredibly
drawn to the problem, find it fascinating, and then try to funnel their
abilities into something that's vaguely safety-like. Researchers from MIRI
come from a pure math background, with focus on mathematical logic and
metamathematics, and that leads to them releasing work on self-reflective
Turing Machines and probabilistic proof systems.

This is the other reason I have trouble taking any arguments out of MIRI
too seriously - part of me always suspects that their arguments are partially
post hoc justifications to align their research abilities with their
research trajectory. Everyone does this - the joke in public health is that
everyone has a good argument for why their cause is important to deal with.
Researchers always work on problems that seem important to them, and they're
also always telling people why it's important to them, and the problem I have
is when people get too dogmatic on why *you and you and you and you* should
be working on the same thing.

\* \* \*

There are many reasonable arguments, many of which have competing conclusions.
I could just as easily believe something else, and it's mostly luck that changes
which argument I see first. So it feels wrong to commit too strongly to any
particular action, or to advocate for anything very strongly.

And, again, this isn't a unique viewpoint: Nick Bostrom gave a talk on this,
giving an example of how a person argues themself into and out of voting
for someone. (LOOK IT UP). Eventually he concludes with saying that you
probably should put more penalty on committing to specific arguments, especially
with regards to x-risk (CHECK IF THIS CONCLUSION ACTUALLY HOLDS UP.)

So, you know. I'd like to think that fundamentalist rationalists at least keep
things like this in mind, but I'm not sure they do.

\* \* \*

On average, the people I've met that have relations to LessWrong in some way
are generally alright.

Really, it seems like the problem is perception. For better or worse, the ideas
that pull the community together also attract a lot of weirdness. Is that bad?
Well, no, not if you're okay with weird. Some people aren't okay with weird.
And the weird people also tend to be the loudest or the most likely to speak
up, and soon you hit the classic filter bubble problem - the people who talk
aren't representive of the people who exist.



No true Scotsman problem.

The problem people have is mostly with Yudkowsky and beliefs close to those
ones.

The feeling that people from other areas of expertise are advocating for the
right thing to do, talking about something they don't understand, and knowing
that it's easy for people to be mislead about things, and the difficulty
in distinguishing between crackpot and valid point.

In rationalist circles, it feels like people very quickly decide to follow
whatever argument rings true with them, and then directly act on it. Whereas
other people are hesitant to predict anything too far out because of the
difficulty of the prediction, and don't see how it's supposed to change anything
or what the point of it is. "The best we can do" vs "not even worth the effort
right now"
