---
layout: post
title:  "Sometimes I Think About AI Safety and Rationalists and I Get Depressed For a Bit"
date:   2017-05-02 01:14:00 -0700
---

*Warning: good chance I'll regret writing this. Opinion as of July
2017. Personal opinion, does not reflect opinion of anyone I've worked with.*

*This post has been very difficult to write.
In the course of writing this, I've found notes from 8 months ago that I don't
agree with, and a draft from 6 months ago that was so revolting I deleted it out
of disgust. The general structure of this post has changed at least four times
and I'm still not that satisfied with it. Perfection is the enemy of the good, but
on this topic, I wanted to make sure I did it right, because of how much of my life
ends up intersecting with the topic.*

*I say all this to let you know that although I'm doing my best to explain my position,
it is almost certain that I haven't. I'm hoping that I got close enough.*

\* \* \*
{: .centered }

What Is AI Safety?
----------------------------------------------------------------------------------

There are several treatises on the subject, but I think there's value in showing
my frame on the problem.

Roughly, AI safety centers around this question:

> How do we make sure AI acts the way we want it to?

Very often, people narrow it down to this question:

> When artificial general intelligence arrives, how do we make sure AGI does
> what we want it to do?

Just in case you're unfamiliar withe the terms: AGI stands for artificial
general intelligence, a hypothetical machine learning system that can perform any
task at least as well as the average human could. This includes learning new tasks at
least as well as the average human, becuase learning is a thing humans can do.

Now, depending on how pessimistic you are, some people believe that
AGI will most likely kill all life on Earth. Depeding on your opinions on the limits
of technology, maybe it'll kill all life in the galaxy in a death wave that spreads
at close to the speed-of-light.

This is a controversial subject, to put things mildly. It has generated thousands of
pages worth of discussion all across the Internet. I'm not going to attempt to cover
all the arguments, because I don't know all the arguments. I'll cover the ones I know,
and then I'll get into my reservations.


The Existential Risk Inductive Chain
---------------------------------------------------------------------------------

This is the foundation of AI safety advocacy - pretty much everybody specifically
concerned about the x-risk aspects of machine learning knows a spiel for this
by heart.

* Suppose that at some point, there is an AGI that can do any task at a superhuman level.
* That AGI will eventually be given some goal.
* Almost all tasks are easier if you can think faster, so the AGI will
decide to self-improve as quickly as possible.
* Almost all tasks are easier if you have control, so the AGI will decide to
minimize the chance anybody can disable it. Redundant copies, metaphorically
"hiding the off-switch", etc.
* Combined, such an AGI may first self-improve past the point of humanity's ability
to deal with it, then decide to kill all humans, because humans can't turn you off
if all the humans are dead.
* Importantly, this could potentially happen for any task, even if the AGI's
goal sounds benign, because again - everything's easier if you're smarter and you're
alive.

(...at which point, I imagine someone saying, "And that is why you should donate at
least 10% of your income to a mix of MIRI and OpenAI". But we'll get into that later.)

This part of the argument is called *value alignment*, and people refer to "the
value alignment problem" in similar terms.

Here's an alternate way to form the argument.

* Blindly optimizing an objective can lead to lots of unexpected behavior.
* When humans optimize for something, they usually follow some implicit
ethics that stops them from doing anything too heinous.
* Superhuman AGI may not have those implicit ethics, at which point many
bad things could happen.

People like to focus on the value alignment part of the argument, but the other
component is the *intelligence explosion* component. This is just as pivotal
to the argument as value alignment.

In the above, this was abstracted as "the AGI will self-improve as quickly as
possible". Let's expand this point.

* Suppose there is an AGI that can do scientific research at human level or
above.
* Humans have discovered better machine learning algorithms by doing
research. By definition, such an AGI could do the save.
* Suppose that AGI discovers some advance. It could apply it to itself, which
speeds up its rate of discovery.
* Therefore, there's a chance that once AI reaches a certain point, it can
repeatedly discover new techniques to improve itself at a faster and faster
rate (i.e. recursive self-improvement leading to exponential gains.)
* Note that it doesn't have to be exponential. One common theory of technology
is that it follows a sigmoid curve. First it's used rarely, then there's a rapid
period of discovery, then the limitations of the technique are figured out and
discovery slows down. The concern is less about a single technique giving exponential
gains. It's more about discovering several techniques, each with their
own period of rapid discovery, making it look like exponential growth when zoomed
out.

I consider this pivotal because people use both the Sword of Value Alignment
and Sword of Intelligence Explosion to fend off the common counterarguments.

* If someone says AGI won't harm humans, you wave the Value Alignment sword and
say it's easy for AGI to accidentally end up hurting humans.
* If someone says it will be obvious when AGI will happen, you wave the Intelligence
Explosion sword and say that it may not be obvious until it happens, at which
point it's too late.
* If someone says AGI is too far off to be concerned about, you wave the Intelligence
EXplosion sword again and tell people they underestimate exponential growth in their
timelines for when / if AGI happens.

\* \* \*
{: .centered }

Right now, my broad view is that this is a good argument for why AI safety
is worth thinking about, but past that it comes down to your beliefs about
the world. To me, it is very obvious that blindly maximizing a simple objective
can lead to weird, unintended behavior. This shows up in reinforcement learning
all the time - it's not a novel idea. I think most ML researchers also find this
obvious, it shows up all over the place. If you want to produce
lots of papers, you do different research than if you want to produce good
work.

The follow-on argument is that people expect safety problems to be difficult,
which I also find reasonable. Once you exit mathematics, it's usually very
hard to prove something can never happen. The vague intuition from computer
security is that systems are often much more vulnerable than you'd expect,
even if they're based on a core kernel of extremely well-tested theory, just
because of software bugs, human error, and social engineering. The attack
surfaace is almost always larger than you imagine it to be.

On occasion people try to attack the premise of how easy or hard it is to
constrain a superhuman AI system. I mostly skip these arguments, because it
often feels like people are using imprecise defintions to argue about something
that doesn't exist. And because superhuman AI doesn't exist, it's easy for the
AI-safety-apologist to make the AI strong enough to render all arguments
invalid, by saying "something something intelligence explosion."

So, most of the useful debate on the subject is on when or if AGI is going to
happen. Well, I say "useful", but I'm not sold on how useful it actually is.

Right now I'm at "10% by 35 years from now, 50% by 50 years from now, 90% by
75 years from now". These numbers are very arbitrary, but here's my rough
arguments for why it feels unlikely for anything notable to happen in the
next 10 years.

* I believe neural nets are here to stay. However, I'm skeptical scaling up
deep learning by itself is sufficient. The hype around deep learning is
well-founded, but I believe neural nets have fundamental limitations that are
hard to deal with. (It's hard for me to articulate what exactly those are.)
* I don't see a clear roadmap for making an AGI of any kind. Even if you
don't care about the safety problems, even if you use all the computers in the
world, plus a Moore's law growth in computational power.
I know people are very fond of exponential growth, or of iterated sigmoids that
look exponential when you zoom out, but even then I don't see it as particularly
likely - it feels like there are several large conceptual problems in the way,
and more that we can't see right now.
* Deep learning successes get written into papers, presented at conferences,
and get discussed widely. In some cases, they get turned into popular science
articles that don't include all the words of caution that are implicitly present
in the original paper. By contrast, the negative results aren't discussed very
widely, and mostly accumulate as instituional knowledge in various labs. If you
only pay attention to the success stories, it's easier to extrapolate a drastic
trend in performance. Think filter bubbles, except it's a filter bubble around
how successful the technology is.

Moving apart from direct theories about AGI, I also have some reservations about
the arguments presented by various organizations.

* Some organiziations (like 80,000 Hours and OpenPhil) have stated that they
believe more people should work on AI safety. I like that people are thinking
about AI safety, and I like that both DeepMind and OpenAI have safety teams.
Both have some room for growth, but I dont think they should grow by much more,
because it's unclear to me that there's enough potential impact.

I think people often frame the argument as "people don't think AI safety is
important", when the more common belief I've heard is "people think safety will
be important to keep in mind, but current systems have little need for it."
Right now, the money is in better translation, better computer vision, and
better model of user preferences. Over time I expect reward hacking to be a
more important problem, that will have to be solved to produce good products.
which puts much more economic incentive into safety-like problems.

For example, the Concrete Problems in AI Safety paper mentions a thought
experiment of a household robot getting reward if it can't see any dust. The
robot decides to cover all furniture with a blanket, because it's easier than
cleaning the room. This is a thought experiment because household robots don't
exist. But if anyone wants to make a household robot, consumers will want the
household robot to do what they tell it to do.

Or as another example: someone asks Google Home "How do you spell SOS in
Morse Code?". The assistant replies with "S-O-s-Space-I-N-Space-M-O-R-S-E-Space-...".
For now, it's a cute quirk of the behavior, but I think over time people will
get normalized to what ML and do and will ask for better.

(Note: I'm very sure you can poke holes in both of these examples, but I'm more
concerned about the holistic point: there are currently few economic incentives
for AI safety, and I expect there will be more economic incentives in the future.)

(Note 2: I'm guessing this is why OpenAI has both NLP and robotics in their
list of research interests - it creates scenarios where an improper AI doesn't
do that much damage, but is still important to deal with to keep eople happy.)

(Note 3: This is not a novel idea. Stuart Russell made a similar point in his
keynote talk at the 2016 Bay Area Robotics Symposium.)

As a further branching point from this, I think my definition of safety-like
problems is broader than the standard one. To me, all of the following are
safety-like problems.

* Fairness: how do we ensure machine learning isn't discriminatory? Important
in discussiona about racial profiling, eligibility of home ownership,
learned associations between genders and roles.
* Privacy: machine learning techniques can memorize parts of their training
data, which can be extractable by an adversary. This is less of a concern with
images, but is more concerning if you want to apply ML to healthcare - how do
you best prevent leaks of patient data? See differential privacy.
* Adversarial inputs: it's been shown that we can create inputs that get
mislabled with high confidence. Furthermore there are papers showing that
some of these perturbations are universal across all images, that this is
achievable even if you only get black-box queries to the model (like through
a computer vision API), and with proper
training the results are robust to various image transformations. When ML
becomes more prevalent, how do we deal with these concerns?
* Human-computer interaction: how do humans interact with computer agents?
How do humans interpret robot behavior, and in turn, how can we make sure
robots accurately model human behavior?
* Interpretability: decision trees are well-known for being interpretable.
Neural nets are well-known for being harder to interpret. This is a blocker
for areas like medicine, where doctors place a lot of value on understanding
what a model is doing.

These all feel like necessary questions to answer if we want machine learning
to play a larger role in life. I'm not a big x-risk person, but if I preteded
to be a big x-risk person, and bought the AI x-risk argument, I could still
see these approaches as giving useful lessons for the future...except on the
side, it's also more useful in the present day.

Let's take MIRI, for example. I have skimmed some of MIRI's work, and gone to
some presentations by people from MIRI. It's all very theoretical, and I have
a hard time believing any of it is going to be useful. They've picked such a
theoretical view by this argument:

* Constraining the behavior of systems is hard.
* Constraining the behavior of systems with superhuman intelligence is going to
be *extra* hard, and it's unlikely that safety efforts for current ML
systems are going to help once AI progresses enough.
* We aren't quite sure how to do it either, but we expect a mathematical, formal
model of cognition is likely to be useful.

I don't agree with all of this, but this is a reasonable enough argument.
All the surveys on AI timelines suggest there's no agreement on when AGI will
happen, or if it's even possible, and the range is even large within MIRI's
staff. In the face of uncertainty, the proper approach is
to try many things. MIRI's research agenda is reasonable as A Thing
People Can Try.

So I'm going about my day, trying to stay reasonable about the whole thing, and
then start imagining somebody declaring that you should be donating all your money to
MIRI, because they are *literally* saving the world in the most
cost-efficient way, and how could you possibly disagree with that if you want
to call yourself a rationalist?

At which point I get incredibly weary, slightly depressed, and annoyed at the
state of the world.

No one in the rationalist-sphere has ever said something like that to me. But
it's how I model the hardcore segment that got into the movement
by falling in love with Yudkowsky's writing - a group of people who agreed with
someone so much, they forgot to do due diligence and ask themselves whether
they should. A group that spends its days discussing Newcomb problems and
warning people that an AGI can convince a human to let it out of the box,
without considering the obvious corollary - a smart enough person could do
the same, and maybe they already have.

Artificial intelligence captures the imagination. Stories about unaligned AGI
do so even more. I've read three "rationalist fanfic" stories, all of which
independently decided to talk about unaligned AGI, cryonics, and the
many-worlds interpretation of quantum mechanics - which feels like an
exceptionally narrow range of topics for a group that I'd expect to pride
itself on novel, skeptical thinking. I get it - these ideas are interesting
to think about, and you think they're cool and important. But isn't it natural
to get sick of paperclip optimizers when you've read about them for the
5th time?

(Off the top of my head:

1. Superintelligence is told to create lots of notes, invents nanotech, uses up
all the oxygen to kill humans by asphixiation, turns humans into notes.
2. Superintelligence is told to make people happy, decides the best way to do
this is to inject people with a serum that constantly triggers their dopamine receptors.
3. Superintelligence is told to make people laugh by writing good jokes,
it writes one so good that people die laughing.
4. Superintelligence is told to maximize the happiness of players in an MMORPG,
invents brain uploading and manipulates people until everyone has uploaded into
the game.
5. And of course, superintelligence is told to make lots of paperclips, invents
nanotech, kills everyone, turns them into paperclips.

*It took me like, 2 minutes to remember all of these.* Can you blame me for
getting sick of these kinds of stories? I think it's entirely fair for me to get annoyed
with people when present the same argument pattern, over and over again.)

To paraphrase a friend: "I thought rationalists were supposed to
be about studying cognitive biases and figuring out how to exploit your
comparative advantages. Instead everyone asks me to sign up for cryonics,
advocate for AI safety, and try out polyamory."

There's a card game I frequent pretty often, and one day somebody linked a post
from Slate Star Codex. One poster liked it a lot. Within a week, he adds
links to the Sequences and SSC to his signature,
drops Bayesian probabilities in all his posts, and starts a thread asking
whether people's beliefs on superintelligence are closer to Eliezer
Yudkowsky or Robin Hanson.

Honestly, none of this was particularly surprising, because his pattern of
speaking was very similar to how people act in the rationalist sphere.
And at the same time, I think it's sad that it *wasn't* surprising that
anybody could jump from "guy who mentioned EA once" to
"ridiculously stereotypical rationalist" that quickly.

\* \* \*
{: .centered }

In many ways, the rationalist community reminds me of the math contest
community - a group of people who, after much searching, have found people who
finally speak the same language they do, and all the consequences thereof.
It should come as no surprise that the two have a healthily large overlap.

The large focus on AI safety that some people have is wearying and annoying.
I get annoyed when I read people arguing what precisely "intelligence" means.
I get further annoyed when people read a news article about a recent deep
learning paper, and herald it as a Potential Sign of the End Times.

Take, for instance, the Neural Cryptography paper. Here's roughly how it goes:
Alice and Bob are two neural networks with shared parameters. Each agent
has an encoder, which takes a message $$m$$ and key $$k$$, then outputs
some output $$m'$$. There's a decoder, which takes $$m'$$ and $$k$$ and
is trained to reconstruct $$m$$ as closely as possible.

In between the two is Eve, a neural network which receives just $$m'$$,
and tries to reconstruct $$m$$. The two are trained adversarially - Alice
and Bob are trained to maximize Eve's confusion (# of bits it gets wrong),
and Eve tries to maximize the # of bits it gets right.

It's a cute idea, but the experiments were very small - they give each
network an 8 bit message, and an 8 bit key, and the model didn't even learn
a one-time pad, it learned something that kinda worked. I went to a talk
the authors gave, and they talked about how that was the best they got,
even after lots of tuning.

So in short: a cute research idea, with results suggesting you could
*kind of* do this, but not much more than that. It was submitted to ICLR
and rejected for similar reasons, and I think that verdict is fair.

But in the media coverage of the paper....oh jeez, let me pull up the headlines.

HEADLINES HERE.

Meanwhile my transhumanist friends were discussing it as if it was a great
advance, extrapolating to AI agents using steganography to encode messages to
one another, wondering what the next plan was with neural cryptography, and
all sorts of massively oversized speculation given the results of the work.

In moments like these, it's very hard to be fair towards the seeming
hypocrisy - proclaiming the importance of listening to experts and actually
reading the paper, and then selectively ignoring it when talking about AI,
and when confronted about it, pulling up some general heuristic like "well,
you're just underestimating the trend of research, here's a Wikipedia article
for it."

This isn't even a wrong thing to say! It's a valid point! But at the same time
it feels like people aren't aware of the limitations of deep learning, and
are selectively building a worldview based only on its successes, because that's
what they hear about the most.

Broadly, it's because it feels like they should know better. It feels like
a lot of the discussion is at a high level, uses imprecise definitions to
make broad claims, and to me both those communities pride themselves on
carefully considering all sides of the argument - except, seemingly, for
AI safety.

I know this isn't a fair portrayal of the community. That's part
of the problem.

\* \* \*
{: .centered }

it's hit that point that i now have a model of "the strawman rationalist".
The strawman rationalist is not fair. The strawman rationalist does not match
anyone I know, and most likely doesn't match anybody in the world, not even
the most hardcore rationalist (if such a thing can even be said to exist.)
It is an amalgam of every negative rationalist stereotype I can think of.
I find it easy to feed it with more ammunition, which I find incredibly
disappointing.

\* \* \*
{: .centered }


I watched Ex Machina because people hyped it up as great. It was a good movie,
but not for me. To me it felt like it was presenting a bunch of old ideas
as if they were new. I didn't need to hear Yet Another Debate About The Chinese
Room Thought Experiment because *I really don't care.*

Really, that's the crux of the problem I have about EAs and rationalists.
The organizing principle of EAs is that you can have much larger impact if
you donate according to careful argument instead of what feels right.
The organizing principle of rationalists is that the human brain is horribly
horribly broken, and it's worth trying to optimize your life, even if you're
never going to be perfect.

But in practice, it feels like the organizing principle is for people to
endlessly debate the merits of different causes, different organizations,
and the right way to live your life. That's fine, but sometimes I feel like
people are more interested in philosophy than the original question.
I can understand why. Philosophy is fun in moderation, but not much more than
that.

An aggressive reductionist desire to categorize and name everything about
the world. (It's not a coincidence that people who do math end up intersecting
in these circles a lot - all of math runs on this kind of thinking.)


\* \* \*
{: .centered }

Depending on the circumstance, I can pass as an EA or as a rationalist.
If I want people to think I'm an effective altruist, 
I talk about the marginal utility of money, or Against Malaria Foundation,
or animal suffering. If I want people to think I"m a rationalist, I can
play up cognitive biases, and talk about different x-risks, or ways
to exploit comparative advantages in the market.

The thing is, I'm neither? I'm not an effective altruist, and I'm not
a rationalist.

"But, you went to EAGxBerkeley, then went to EA Global, then went to
a Slate Star Codex meetup hosted at the CFAR office, *and then went
to a CFAR workshop*. How are you *not* an EA or a rationalist?"

And I mean, that's all true, but when I say I'm not an EA, and not
a rationalist, it feels right.
The thing is, I don't consider myself to be an effective altruist, and
I don't consider myself to be a rationalist. I'm merely a bystander to
both movements.

\* \* \*
{: .centered }

Now, given how awful the strawman rationalist is, why do I keep him around?
He's always smug, always insufferable, always tries to find a new reason
to ignore arguments that machine learning could possibly end okay, and believes
that he and his cohort are the only people who are doing anything meaningful
to protect people. To the strawman, everyone else is a well-meaning but
misguided idiot, but I'm not, oh no I'm not, because I've read the Sequences!

I keep him around because despite how crazy he is, he still manages to give
good insights.

What I do want to stop, however, is the vaguely adversarial way people on both
sides have framed the AI safety debate. It's not overt or anything, more like
a collection of snide remarks and jokes I hear now and then. A person from MIRI,
saying that at least 2 professors take them seriously, one of whom is
well-respected. Another person from MIRI, joking that giving DeepMind tons
of compute power may have been a bad idea. An effective altruist, contrasting
the money going into AI vs the money going into AI safety. Two people have
independently joked that if AGI is imminent, maybe we need kill all the AI
researchers, which really stops being funny when you're one of those
researchers.

On the flip side, a lot of established ML people have criticized people
for bad reasons. They have weird opinions on some basilisk thing,
why should I listen to these people. He's a professor of philosophy, I'll
listen to people who do more ML.

The thing is, I broadly agree with the argument for AI safety, if you assume
a sufficiently powerful AI. And furthermore, none of the ML researchers I know
dispute the argument, given the premise.
If you do enough reinforcement learning, it's
obvious that reward function design is really hard, because of how easy it
is to get stuck at local optima. If you blindly optimize training set loss,
you overfit to the data. If everyone designs algorithms and tests them on
32 x 32 CIFAR images, they're not going to generalize as well because people
have trial-and-errored there way to methods that only work well on CIFAR.
A common metric in NLP is BLEU, which you can optimize with RL, but if you
do so you don't get better human-graded performance.

People do things like debate the orthogonality thesis, or debate whether
an AI can convince a human to let it out of the box, but these are debates
about systems that don't exist yet, and so they quickly turn into unverifiable
philosophy.


Rough notes for things I want to bring up.

* Some people are first introduced to cognitive biases and Bayesian thinking
through the rationalist community. I think some then make the mistake of
assuming that these insights are *unique* to rationalists. The collection
of all the ideas in one place is special, but the ideas themselves are not.
I feel this is why I've been so skeptical of the community - it feels
too insular, in the way everybody cites everybody else because they only
want to cite trustworthy EAs or rationalists, with a seeming lack of awareness
of how ridiculous they look to outsiders.
* The general feeling of "this is totally cult behavior", social pressure
to agree with The Hot New Thing or to feel like A Member Of The Resistance.
* Why AI safety in particular, versus any other X-risk like nuclear proliferation,
climate change, alien invasion, bioterrorism, preventing creating of
nanotechnology, etc?
* It is okay for some people to be passionate about X-risk, and for some people
to be passionate about other things. The part I can't stand is "when people
mistake their cause for a religion"
* "Introduced to these ideas, and then you're stuck in the convex hull of ideas / the
convex hull of trust, and soon you don't trust anybody else."
    * Some people do this, but the loudest are the most fanatic, and it feels
    like the most fanatic don't. Or they've been so disappointed that they just
    give up on talking to any other people.
    * The example of the AI safety paper that has a citation of "I don't like
    discussing AI safety with people who haven't read Superintelligence"
* Concern that people advocating for AI safety are doing so for charlatan
reasons (that they don't actually believe in the dangers, they just want to play
up the dangers to get money.)
    * I'm pretty sure this isn't true for most people.
* The feeling that I had when reading the Open Phil grant - it sounds like
you're giving OpenAI money because in some philosophical thought experiment,
they agreed with your viewpoint. This seems like a really, really weak reason.

The general trend I've noticed is that some people like to rant about rationalists.
But over time, it turns out they rant specifically about Yudkowsky and people
with similarly weird beliefs to him. And that they appreciate the people who
don't seem to take the whole thing as seriously - people who don't feel the
need to talk about Bayes Theorem and utilitarianism all the time.

Once, a friend on Facebook asked (CHECK),

> Aliens have just visited Earth, and they ask for 10 people to represent all
> of humanity. Who do you pick?

One commenter said Bostrom should obviously be one of them. And I remember thinking
that this was an oddly controversial pick, unless you were in the in-group.

Yes, I know you've seen the skulls. Yes, I know that some people in these groups
are aware their beliefs are fluid. Yes, I've read CFAR's mission statement:
beliefs, weakly held, but acted on anyways.

The problem is that it's not clear these virtues are universalized, it's not clear
this is common knowledge, and people view rationalists as much more homogenous
than they actually are.

This isn't actually that weird, when you think about it. Let me put it this way:
I decided to read three "rationalist fanfic" stories, and they all indepedently
decided they needed to explain the many worlds interpretation of quantum
mechanics in laymans terms. They also all felt the need to talk about characters
"making updates about the world". Then there was simulation hypothesis, then
wireheading, then cryonics.

Do you realize how condescending it is, to see somebody say they're the only
people making a difference, and that the rest of us are just along for the ride,
like so much chaff? The implicit assumption that everyone outside of the rationalist
in-group is an idiot? 

Do you know what my strawman rationalist says in response? "We say that because
it's true. Don't you know? Denying the truth doesn't make it any less true."

\* \* \*
{: .centered }

Another frustrating thing is the heavy, heavy focus people in AI safety have
on timelines.

Now, before I complain, I'll admit that this makes sense, and that I'm not sure
there are better alternatives.

On the other hand, a lot of people I know don't like making predictions about
the long term trends of technology. I don't like to do this either, because
of how hard it is to predict technological progress.

It feels like people are reading tea leaves, and every paper's another leaf.
I've noticed a strong allergy against making any predictions too far out into
the future, because even the short term progress is hard, and it's hard to
estimate the difficluty of problems, and people don't want to overpromise
and underdeliver because hype is so easy to create and so difficult to keep
in check.

So, when I see articles about neural cryptography, and people talking about
steganography in AGI, or somebody releases WaveNet and somebody speculates on
when we'll see an AI-generated pop song hit the Billboard top 100, a part of
me dies.

I think a lot of times, when a big company releases a blog post on a neat
ML paper, people try to interpret what the point of it is, why it'll make
money, and so on. And in my experinece, most research happens because people
think it's really, really cool to see what's doable. This applies to all fields
of research, by the way. I was talking to one person about AI safety, and she
told me that although she kept seeing people say they wanted to do AI safety for
utilitarian reasons, she didn't feel she was utilitarian. She was just super,
super interested in mathematical models of cognition. AI safety let her
study those problems in a context that could be important, down the line.

This is my impression of a lot of AI safety research. People get incredibly
drawn to the problem, find it fascinating, and then try to funnel their
abilities into something that's vaguely safety-like. Researchers from MIRI
come from a pure math background, with focus on mathematical logic and
metamathematics, and that leads to them releasing work on self-reflective
Turing Machines and probabilistic proof systems.

This is the other reason I have trouble taking any arguments out of MIRI
too seriously - part of me always suspects that their arguments are partially
post hoc justifications to align their research abilities with their
research trajectory. Everyone does this - the joke in public health is that
everyone has a good argument for why their cause is important to deal with.
Researchers always work on problems that seem important to them, and they're
also always telling people why it's important to them, and the problem I have
is when people get too dogmatic on why *you and you and you and you* should
be working on the same thing.

\* \* \*

There are many reasonable arguments, many of which have competing conclusions.
I could just as easily believe something else, and it's mostly luck that changes
which argument I see first. So it feels wrong to commit too strongly to any
particular action, or to advocate for anything very strongly.

And, again, this isn't a unique viewpoint: Nick Bostrom gave a talk on this,
giving an example of how a person argues themself into and out of voting
for someone. (LOOK IT UP). Eventually he concludes with saying that you
probably should put more penalty on committing to specific arguments, especially
with regards to x-risk (CHECK IF THIS CONCLUSION ACTUALLY HOLDS UP.)

So, you know. I'd like to think that fundamentalist rationalists at least keep
things like this in mind, but I'm not sure they do.

\* \* \*

On average, the people I've met that have relations to LessWrong in some way
are generally alright.

Really, it seems like the problem is perception. For better or worse, the ideas
that pull the community together also attract a lot of weirdness. Is that bad?
Well, no, not if you're okay with weird. Some people aren't okay with weird.
And the weird people also tend to be the loudest or the most likely to speak
up, and soon you hit the classic filter bubble problem - the people who talk
aren't representive of the people who exist.



No true Scotsman problem.

The problem people have is mostly with Yudkowsky and beliefs close to those
ones.

The feeling that people from other areas of expertise are advocating for the
right thing to do, talking about something they don't understand, and knowing
that it's easy for people to be mislead about things, and the difficulty
in distinguishing between crackpot and valid point.

In rationalist circles, it feels like people very quickly decide to follow
whatever argument rings true with them, and then directly act on it. Whereas
other people are hesitant to predict anything too far out because of the
difficulty of the prediction, and don't see how it's supposed to change anything
or what the point of it is. "The best we can do" vs "not even worth the effort
right now"
