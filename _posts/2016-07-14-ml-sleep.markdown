---
layout: post
title:  "Machine Learning Workaholism"
date:   2016-07-15 00:55:00 -0700
---

Machine learning is the study of algorithms that let a computer
learn insights from data in a semi-autonomous way.

Machine learning *research* is the process by which researchers
get addicted to gambling their time in the name of improved
results.

\*\*\*
{: .centered }

Despite its foundations in math and statistics, ML is mostly an
experimental science.

That's not saying there's no theory. There's
plenty of theory. A short list: multi-armed bandits, contextual bandits,
non-convex optimization, directed graphical models,
and Markov random fields. The proofs are there, if you look for them.
I'm also not saying there's no place for theory. Not everyone wants
to spend the time to learn why their algorithm is guaranteed to
converge, but everyone wants the proof to exist.

However, in the applications-driven domains that drive AI hype,
people care about results first, theoretical justification second.
That means a lot of heuristics. Often, those heuristics are bound together
in an unsatisfying way that works empirically, but has very little
founding theoretically.

How do we discover those heuristics? Well, by the scientific method.

1. Make hypothesis.
2. Design an experiment to test that hypothesis.
3. Implement and run the experiment.
4. Interpret the analytics.
5. Refine the hypothesis with more informed experiment designs.
6. Repeat until enlightenment.

In machine learning, hypotheses are promising algorithms,
and experiments are benchmarks of those algorithms.

Hey, what's the issue? Just run experiments, until something works!

Well, gather round. I've got a whole plate of beef to share.

\*\*\*
{: .centered }

By and large, ML algorithms are both probabilistic and incredibly
customizable. Empirically, probabilistic approaches work the best on large
datasets, and currently, datasets are very very large.

Their customizability means you have to try many different
settings to get good performance,
Have you tried tweaking your hyperparameters?
Whitening your data? Using a different optimization algorithm?
Making your model simpler? Making your model more complex? Using batch norm?
Changing your nonlinearity?
Unfortunately, standard practice in ML is to publish the one setting that
worked, and none of the settings that failed. This would be insane in other
fields, but in ML, it's just how things are.

It doesn't help that the beauty of an idea is basically uncorrelated
with its real-world performance.
Here's a conversation I overheard once, between a computer vision
professor and one of his students.

> Student: It doesn't work.
>
> Professor: No! It doesn't work? The theory's too beautiful for it not
> to work!
>
> Student: I know. The argument is very elegant, but it doesn't work in practice.
> Not even on [Lenna](https://en.wikipedia.org/wiki/Lenna).
>
> Professor: (in a half-joking tone) Maybe if we run it on a million images,
> in parallel, it'll magically start working.
>
> Student: If it doesn't work on a single image of Lenna, it's not going to work
> on a million copies of Lenna.
>
> Professor: Ahhhh, I suppose so. What a shame.

I feel their pain.

After training enough machine learning models, you gain an intuition for which
knobs are most important to turn, but it never quite hits the level of guaranteed
success. I like to joke that one day, the theorists will catch up and recommend
an approach for reasons better than "it works empirically", but I don't think
it'll happen anytime soon. The theory is very hard.

(What theory *has* done is produce the [no free lunch theorem](http://www.no-free-lunch.org/).
Informally, it says that no algorithm can beat another algorithm on
every possible problem. In other words, there will never be One Algorithm To
Rule Them All. Having a formal proof of our guaranteed failure is nice.)

\*\*\*
{: .centered }

I still haven't explained why machine learning research can easily take over
your life.

Well, I suppose I have, in a roundabout way. Machine learning
experiments can be very random
and very arbitrary, working or failing at a drop of a hat. Not even the legends
in the field can get away with avoiding hyperparameter tuning. It's a necessary
evil.

It makes the field feel like one huge casino. You pull the lever of the slot machine, and
hope it works. Sometimes, it does. Or it doesn't, at which point
somebody walks up to tell you that slot machine hasn't worked in 10 years
and you should try the new slot machine everyone's
excited about. Machine learning is very much like folklore, with tips and tricks
passed down over the generations.

We understand many things - the folklore is surprisingly helpful.
But the slot machine's still a slot machine. There's some unavoidable randomness
that always seems to ruin your day.
And worse, it's a slot machine where your job or
funding or graduation is on the line.

In the game of machine learning, you get lucky, or try
so many times you *have* to get lucky. The only way to guarantee success
is to do the latter.

And that means experiments. Tons and tons of experiments, eating up tons and
tons of time. It doesn't help that
the best time to run experiments is when you're about to take a break.
Going out for lunch? Start an experiment, see how it's doing when
you get back. Heading out for the day? Run an experiment overnight, check the
results tomorrow morning. Don't want to work over the weekend? Well, your computer
won't mind. We're in a nice regime where we can run our experiments
unattended, which is great, as long as your code is working.
If your code breaks, good luck fixing it - every day your code doesn't work is
one fewer day of computation time, one fewer batch of experiments.

It's that kind of pressure which pulls in workaholic mindsets.
You don't *have* to fix your code tonight, just like you don't *have* to maximize
the number of darts you get to throw at the dartboard. **_It will be fine_**.

If there was a way to make machine algorithms Just Work by, I don't know,
sacrificing a goat under the light of the full moon,
I'd do it in a heartbeat. Because if machine learning algorithms Just Worked,
there are plenty of ways I can make up for killing an innocent goat.

Good thing goat sacrifices don't do that, because I really don't want to
add that to my list of "things that work for no good reason.". That list is
plenty full.

\*\*\*
{: .centered }

At this point, you might be wondering why I even bother working in machine
learning.

Truth is, all the bullshit is worth it. There are so many exciting things
happening, and by this point my tolerance for these issues has grown by enough
that I'm okay with it.

The theoretical computer science friends I know probably think I'm insane
for putting up with these conditions. Oh well! I'm insane. What else is new.

Here's us, on the raggedy edge. If that's the price to pay, I'll pay it.

> The javelin is far ahead of her and moving far faster. The colonists will have plenty of time to get comfortable. There will be something at Sirius by the time she gets there. Maybe. It'll be friendly, maybe. And if not, she can keep improvising. 

([Ra](https://qntm.org/ra))
{: .centered }

