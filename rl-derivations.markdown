---
layout: page
title: A List of Reinforcement Learning Derivations
permalink: /rl-derivations/
---

*Status: Draft*

*Last updated May 22, 2017*

A place for me to store the derivations of reinforcement learning
results, because I keep forgetting them.

TODO: Add paper links for where I first encountered these proofs.

Equivalent Formulations of Policies and Reward
-----------------------------------------------

(Adapted from a mix of Generative Adversarial Imitation Learning
and Trust-Region Policy Optimization.)

The state visitation frequency is defined as the discounted
sum of probabilities of visiting a given state.

$$
    \rho_\pi(s) = \sum_{t=0}^\infty \gamma^t P(s_t=s|\pi)
$$

Note that up to a constant factor, you can think of this as
a probability distribution over states.

$$
    \sum_s \rho_\pi(s) = \sum_{t=0}^\infty \sum_{s \in S} \gamma^t P(s_t=s|\pi) = \sum_{t=1}^\infty \gamma^t \sum_{s \in S} P(s_t=s|\pi) = \sum_t \gamma^t = \frac{1}{1-\gamma}
$$

More precisely, multiplying $$\rho_\pi(s)$$ by $$(1-\gamma)$$ gives a
probability distribution.

The occupency measure $$\rho_\pi(s,a)$$ is defined similarly, except it's the
discounted sum of probabilityes of visiting state-action pairs $$(s,a)$$,
instead of just states $$s$$.

$$
    \rho_\pi(s,a) = \pi(a|s) \rho_\pi(s)
$$

(I know this is an abuse of notation. I'm sorry.)


Theorem: there is a bijection between occupency measures and policies.

Proof: is actually somewhat involved, refer to Syed et al 2008 for details.


Theorem: the expected reward of policy $$\pi$$ can be written as

$$
    \sum_{s,a} \rho_\pi(s,a)r(s,a)
$$

where $$r(s,a)$$ is the reward for taking action $$a$$ from state $$s$$.

Proof intuition: Consider triples $$(s, a, t)$$. When computing the
expected reward of $$\pi$$, imagine placing reward $$\gamma^t r(s,a)$$
at $$(s,a,t)$$.


REINFORCE
-----------------------------------------------

Let $$X$$ be a random variable with known p.d.f. $$p_\theta(X)$$.
(From here on, $$\theta$$ will be omitted. In general, $$\theta$$
will be specified the first time, and will be implicit all other times.)
Let $$J(\theta)$$ be some cost function defined as

$$
    J(\theta) = \mathbb{E}[f(X)] = \int_x f(x)p(x)\, dx
$$

for some arbitrary function $$f(x)$$ that does not depend on $$\theta$$.

We want to compute
$$\nabla_\theta J(\theta)$$. Using the log derivative trick, we can
change the gradient of the expectation to the expectation of the gradient.

$$
    \nabla_\theta J(\theta) = \int_x f(x) \nabla_\theta p(x) \, dx
    = \int_x f(x) p(x) \nabla_\theta \log(p(x)) \, dx
    = \mathbb{E}[f(x) \nabla_\theta \log(p(x))]
$$

Now that this is an expectation, we can get an approximate gradient
by sampling $$x$$.

So far, none of the derivation is RL specific. Let's now suppose
the random variable is trajectories $$\tau = (s_0,a_0,s_1,a_1,s_2,\cdots)$$,
generated by policy $$\pi_\theta$$ in an MDP. Let's then have
$$f(\tau)$$ be the reward over the trajectory. $$J(\theta)$$ is
now the expected reward of the policy, and the gradient gives a
way to update the policy to increased reward.

Once we assume we're in an MDP, we get some magic cancellations.

$$
    p(\tau) = p(s_0) \prod_{i=0}^\infty \pi(a_i|s_i)p(s_{i+1}|s_i)
$$

$$
    \log p(\tau) = \log p(s_0) + \sum_{i} \left(\log \pi(a_i|s_i) + \log p(s_{i+1}|s_i) \right)
$$

Note that the dynamics ($$p(s_0)$$ and $$p(s_{i+1}|s_i)$$) do not depend on
$$\theta$$, so when we compute the gradient, they get cancelled out, leaving.

$$
    g = \nabla_\theta J(\theta) = \mathbb{E}\left[f(\tau) \sum_{i} \nabla_\theta \log \pi_\theta(a_i|s_i)\right]
$$


Expectation of Score Function is Zero.
===============================================

This will be important to variance reduction.

$$
    \mathbb{E}_{a_i|s_i}\left[\nabla_\theta \log \pi(a_i | s_i)\right] =
    \int_{a_i} \pi(a_i|s_i)\nabla_\theta \log \pi(a_i | s_i) \,da_i =
    \int_{a_i} \nabla_\theta \pi(a_i|s_i) \,da_i =
    \nabla_\theta \int_{a_i} \pi(a_i|s_i) \,da_i =
    \nabla_\theta 1 = 0
$$

Any time we can move everything except the score function out of the expectation,
we can cancel that term.

The equality above is true for any distribution, but the $$a_i\vert s_i$$ distribution
is especially helpful for MDPs. By the Markov property, $$a_i\vert s_i$$
is the same as $$a_i \vert s_{0:i},a_{0:i-1}$$.


REINFORCE Variance Reduction
===============================================

Now that we're married to the MDP framework, there are ways to quickly
reduce variance without adding any bias.

**Observation 1:** When we take an action at timestep $$t$$, it can only
affect the rewards from timesteps $$t$$ and onward. Assuming
$$f(\tau) = R(\tau)$$, let

$$
    R_{a:b} = \sum_{i=a}^b r_i
$$

Note $$R(\tau) = R_{0:\infty}$$.

We'll use similar notation to denote trajectories. $$\tau_{a:b}$$
indicates states, actions, and rewards from time $$a$$ to $$b$$.

Slightly reorder the gradient as the sum of expectations

$$
    g = \sum_i \mathbb{E}\left[R(\tau)\nabla_\theta \log \pi(a_i|s_i)\right]
$$


Then split $$R_{0:\infty}$$ into the sum of rewards before and after time $$i$$.

$$
    g = \sum_i \left(\mathbb{E}\left[R_{0:i-1}\nabla_\theta \log \pi(a_i|s_i)\right]
             + \mathbb{E}\left[R_{i:\infty}\nabla_\theta \log \pi(a_i|s_i)\right] \right)
$$

We can move $$R_{0:i-1}$$ out of the first expectation, since none of those rewards
depends on $$a_i$$. By the earlier lemma, this gives

$$
    g = \sum_i \left(R_{0:i-1} \cdot 0 + \mathbb{E}[R_{i:\infty}\nabla_\theta \log \pi(a_i|s_i)]   \right)
    = \sum_i \mathbb{E}[R_{i:\infty}\nabla_\theta \log \pi(a_i|s_i)]
$$

(Can show this formally by writing out the conditional expectations, but it
gets messy fast.)

Right away, this lets us drop a few terms from the gradient.

**Observation 2:** We can subtract a baseline function without biasing the gradient,
as long as the baseline has certain properties.

Recall that the expectation of the score functions is zero.

$$
    \mathbb{E}_{a_i|s_i}\left[\nabla_\theta \log \pi(a_i | s_i)\right] = 0
$$

Let $$b(s_i)$$ be a baseline function that depends on $$s_i$$. Since the
expectation is conditioned on $$s_i$$, we get

$$
    \mathbb{E}_{a_i|s_i}\left[b(s_i)\nabla_\theta \log \pi(a_i | s_i)\right] =
    b(s_i)\mathbb{E}_{a_i|s_i}\left[\nabla_\theta \log \pi(a_i | s_i)\right] = 0
$$

Because of this, we can subtract a state-dependent baseline from the cumulative
rewards without changing the expectation.

$$
    \sum_i \left(\mathbb{E}[(R_{i:\infty} - b(s_i))\nabla_\theta \log \pi(a_i|s_i)]   \right)
    = g - \sum_i \left(b(s_i)\nabla_\theta \log \pi(a_i|s_i)]   \right)
    = g
$$

In practice, people usually try to estimate $$V^{\pi}(s_i)$$ and use that as their
baseline. Intuitively, this "centers" the scaling of
$$\nabla_\theta \log \pi(a_i|s_i)$$. If we estimated $$V^{\pi}$$ exactly, the
scaling is positive if and only if the reward for taking action $$a_i$$
is better than the average.


Q-Learning
-----------------------------------------------

Bellman Operators
===============================================

Operators map functions to functions. We can think of each application of an
operator as one step of an optimization.

Use $$\mathcal{T}$$ to denote operators.

Proof of Convergence For Tabular Problems
===============================================

Let $$\mathcal{T}^{\pi}$$ be the Bellman operator for $$\pi$$. Define this
as

$$
    (\mathcal{T}^{\pi}Q)(s, a) = r(s,a) + \gamma \mathbb{E}_{\pi}[Q(s',a')]
$$

We now prove that if we repeatedly apply $$T^{\pi}$$ to any $$Q$$, we converge
to $$Q^{\pi}$$.

First, show that $$Q^{\pi}$$ is a fixed point. Follows by definitions.

$$
    (\mathcal{T}^{\pi}Q^{\pi})(s, a) = r(s,a) + \gamma \mathbb{E}_{\pi}[Q^{\pi}(s',a')]
    = r(s,a) + \gamma V^{\pi}(s') = Q^{\pi}(s,a)
$$

Now, prove $$\mathcal{T}^{\pi}$$ is a contraction.
An operator is a contraction if

$$
    \| \mathcal{T}^{\pi}Q_1 - \mathcal{T}^{\pi}Q_2 \| \le c \|Q_1 - Q_2\|
$$
for some $$0 \le c < 1$$ and some distance function.

Why do we want to prove this?
By the Banach fixed-point theorem, for any contraction,

* Repeatedly applying the contraction converges to a fixed point.
* That fixed point is unique.

We use max norm as the distance function. $$\|f\|_\infty = \sup_x |f(x)|$$.
In this case, the supremum is over state-action pairs $$(s,a)$$. For any
$$(s,a)$$,

$$
    \mathcal{T}^{\pi}Q_1(s,a) - \mathcal{T}^{\pi}Q_2(s,a) =
    r(s,a) + \gamma\mathbb{E}_\pi[Q_1(s',a') - (r(s,a) + \gamma\mathbb{E}_\pi(Q_2(s',a'))] =
    \gamma \mathbb{E}_\pi[ Q_1(s',a') - Q_2(s',a') ] \le
    \gamma \sup_{s',a'} (Q_1(s',a') - Q_2(s',a'))
$$

Therefore, $$\| \mathcal{T}^{\pi}Q_1 - \mathcal{T}^{\pi}Q_2 \|_\infty \le \gamma \|Q_1 - Q_2\|_\infty$$,
and since $$ 0 \le \gamma < 1$$, operator $$\mathcal{T}^{\pi}$$ is a contraction.

We've now shown the Bellman operator $$\mathcal{T}^{\pi}$$ converges to $$\pi$$.
Let $$\pi^*$$ be the optimal policy. What does the Bellman operator for that
look like?

$$
    (\mathcal{T}^*Q)(s, a) = r(s,a) + \gamma \mathbb{E}_{\pi^*}[Q(s',a')]
$$

The optimal policy $$\pi^*$$ will take the action $$a'$$ that maximizes
$$Q(s', a')$$, which converts the expectation into a max.

$$
    (\mathcal{T}^*Q)(s, a) = r(s,a) + \gamma \max_{a'}Q(s',a')
$$

This recovers the 1-step return that appears all over the place. Since this
is a special case of $$\mathcal{T}^{\pi}$$, by earlier argument this converges
to $$Q^*(s,a)$$.

In practice, we can never apply the Bellman operator exactly, except for
very small problems where we can easily iterate over all $$(s,a)$$.
Instead, we have parametrized Q-functions $$Q_\theta(s,a)$$,
then take gradient steps to minimze the TD-error

$$
    \frac{1}{2}\left(Q(s,a) - (r(s,a) + \gamma \max_{a'}Q(s', a')\right)^2
$$

With parametrized Q-functions, we are no longer guaranteed to converge,

TODO: add soft Q-Learning.

Natural Policy Gradient
-----------------------------------------------

(Argument summarized from [https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf](original paper).)

Natural policy gradient is a special case of natural gradient, so let's
explain that first.

Suppose we have some objective function $$\eta(\theta)$$ that's
differentiable. When optimizing $$\eta(\theta)$$, Why do we step in
direction $$\nabla_\theta \eta(\theta)$$? It's because the direction
of steepest descent is the gradient. Quick argument follows.

The direction of steepest descent is the solution to 

$$
\lim_{c\to 0} \max_{d\theta, \|d\theta\|^2 \le c} \eta(\theta) - \eta(\theta + d\theta)
$$

As $$c \to 0$$, the first order Taylor approximation gets more accurate, giving

$$
    \lim_{c\to 0} \max_{d\theta, \|d\theta\|^2 \le c} \eta(\theta) - \eta(\theta + d\theta)
    =\lim_{c\to 0} \max_{d\theta, \|d\theta\|^2 \le c} \eta(\theta) - (\eta(\theta) + \nabla_\theta \eta(\theta) \cdot d\theta)
    =\lim_{c\to 0} \max_{d\theta, \|d\theta\|^2 \le c} -\nabla_\theta \eta(\theta) \cdot d\theta
    =\lim_{c\to 0} \min_{d\theta, \|d\theta\|^2 \le c} \nabla_\theta \eta(\theta) \cdot d\theta
$$

We can alternatively write the dot product as

$$
    \|\nabla_\theta \eta(\theta)\|\|d\theta\| \cos\alpha
$$

where $$\alpha$$ is the angle between the two vectors. Since the gradient norm
is constant, this is minimized when $$\|d\theta\| = c$$ and $$\cos\alpha = -1$$.
This gives step direction $$-\nabla \eta(\theta)$$.

$$
    \theta_{n+1} = \theta - \nabla \eta(\theta)
$$

In this derivation, we implicitly assumed $$\|d\theta\|$$ was defined as
the Euclidean norm. We could alternatively define norm as

$$
    \|v\|_G^2 = v^TGv
$$

where $$G$$ is some symmetric positive definite matrix. The Euclidean norm is a special
case of this, where $$G$$ is the identity matrix.

Suppose we required the step direction to be $$\|d\theta\|_G^2 < c$$ instead.
Now what's the optimal step direction? Note that different directions have
different maximum Euclidean norms, so the direction aligned with
$$\nabla \eta(\theta)$$ may no longer be optimal.

(Linear algebra interlude because I keep forgetting basic linear algebra.)

Lemma: Every positive definite matrix $$G$$ is invertible.

Proof: Suppose not. Then there exist $$u,v$$ such that $$Gu = Gv$$ and
$$u \neq v$$. Equivalently, $$G(u-v) = 0$$ for some non-zero vector $$u - v$$.
But then $$(u-v)^TG(u-v) = 0$$, contradicting the positive definite definition,
so $$G$$ must be invertible.

$$
    \min_{d\theta, d\theta^TGd\theta \le c} \nabla_\theta \eta(\theta)^Td\theta
$$

Solve this constrained optimization problem through Lagrange multipliers.
To simplify notation let $$g = \nabla_\theta \eta(\theta)$$.
At the solution $$d\theta$$, we must have

$$
    \nabla_{d\theta} (g \cdot d\theta - \lambda(d\theta^TGd\theta - c)) = 0
$$

Which gives

$$
    g - \lambda 2Gd\theta = 0
$$

Solving this gives

$$
    d\theta = \frac{1}{2\lambda}G^{-1}g
$$

So the optimal step direction is $$G^{-1}g$$.

Okay, so what?
In regular gradient descent, we always take small steps in parameter
space, defined by the Euclidean distance in parameter space. Natural gradient
argues that we should instead take small steps in the space of probability
distributions. We can do this by measuring distance with the
Fisher information matrix.

(At least, I think. I'm pretty sure this is an approximation of the actual
argument, but it's the one that makes sense to me.)

Natural policy gradient is the idea of natural gradient, applied to the
policy gradient from REINFORCE. At a high level it's actually pretty
simple.

1. Decide on some learning rate $$\alpha$$.
2. Approximate the inverse Fisher $$F^{-1}$$ from the data.
3. Compute the gradient $$g$$ REINFORCE would have given.
4. Update with $$\theta_{n+1} \gets \theta_n + \alpha * F^{-1}g$$.

In practice, you need to use conjugate gradients to compute $$F^{-1}g$$
effeciently - computing $$F^{-1}$$ explicitly takes $$O(n^2)$$ time,
where $$n$$ is the number of parameters, and conjugate gradients
let you approximate $$F^{-1}g$$ in linear time.


Trust Region Policy Optimization
-----------------------------------------------

TRPO is natural policy gradient, with an adaptive step size to make sure
it's taking the largest possible step that lies within the trust region
(a bound on the KL-divergence between the policy before and after the update.)

