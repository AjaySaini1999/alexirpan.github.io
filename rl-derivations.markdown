---
layout: page
title: A List of Reinforcement Learning Derivations
permalink: /rl-derivations/
---

I keep forgetting how to derive the famous reinforcement learning
results. Here's a place for me to store the proofs. They are very rough.


REINFORCE

Let $$X$$ be a random variable with known p.d.f. $$p_\theta(X)$$.
(From here on, $$\theta$$ will be omitted. In general, $$\theta$$
will be specified the first time, and will be implicit all other times.)
Let $$J(\theta)$$ be some cost function defined as

$$
    J(\theta) = \mathbb{E}[f(X)] = \int_x f(x)p(x)\, dx
$$

for some arbitrary function $$f(x)$$ that does not depend on $$\theta$$.

We want to compute
$$\nabla_\theta J(\theta)$$. Using the log derivative trick, we can
change the gradient of the expectation to the expectation of the gradient.

$$
    \nabla_\theta J(\theta) = \int_x f(x) \nabla_\theta p(x) \, dx
    = \int_x f(x) p(x) \nabla_\theta \log(p(x)) \, dx
    = \mathbb{E}[f(x) \nabla_\theta \log(p(x))]
$$

Now that this is an expectation, we can get an approximate gradient
by sampling $$x$$.

So far, none of the derivation is RL specific. Let's now suppose
the random variable is trajectories $$\tau = (s_0,a_0,s_1,a_1,s_2,\cdots)$$,
generated by policy $$\pi_\theta$$ in an MDP. Let's then have
$$f(\tau)$$ be the reward over the trajectory. $$J(\theta)$$ is
now the expected reward of the policy, and the gradient gives a
way to update the policy to increased reward.

Once we assume we're in an MDP, we get some magic cancellations.

$$
    p(\tau) = p(s_0) \prod_{i=0}^\infty \pi(a_i|s_i)p(s_{i+1}|s_i)
$$

$$
    \log p(\tau) = \log p(s_0) + \sum_{i} \left(\log \pi(a_i|s_i) + \log p(s_{i+1}|s_i) \right)
$$

Note that the dynamics ($$p(s_0)$$ and $$p(s_{i+1}|s_i)$$) do not depend on
$$\theta$$, so when we compute the gradient, they get cancelled out, leaving.

$$
    g = \nabla_\theta J(\theta) = \mathbb{E}\left[f(\tau) \sum_{i} \nabla_\theta \log \pi_\theta(a_i|s_i)\right]
$$


Expectation of Score Function is Zero.

This will be important to variance reduction.

$$
    \mathbb{E}_{a_i|s_i}\left[\nabla_\theta \log \pi(a_i | s_i)\right] =
    \int_{a_i} \pi(a_i|s_i)\nabla_\theta \log \pi(a_i | s_i) \,da_i =
    \int_{a_i} \nabla_\theta \pi(a_i|s_i) \,da_i =
    \nabla_\theta \int_{a_i} \pi(a_i|s_i) \,da_i =
    \nabla_\theta 1 = 0
$$

Any time we can move everything except the score function out of the expectation,
we can cancel that term.

The equality above is true for any distribution, but the $$a_i|s_i$$ distribution
is especially helpful for MDPs. By the Markov property, $$a_i|s_i$$
is the same as $$a_i | s_{0:i},a_{0:i-1}$$.


REINFORCE Variance Reduction

Now that we're married to the MDP framework, there are ways to quickly
reduce variance without adding any bias.

**Observation 1:** When we take an action at timestep $$t$$, it can only
affect the rewards from timesteps $$t$$ and onward. Assuming
$$f(\tau) = R(\tau)$$, let

$$
    R_{a:b} = \sum_{i=a}^b r_i
$$

Note $$R(\tau) = R_{0:\infty}$$.

We'll use similar notation to denote trajectories. $$\tau_{a:b}$$
indicates states, actions, and rewards from time $$a$$ to $$b$$.

Slightly reorder the gradient as the sum of expectations

$$
    g = \sum_i \mathbb{E}\left[R(\tau)\nabla_\theta \log \pi(a_i|s_i)\right]
$$


Then split $$R_{0:\infty}$$ into the sum of rewards before and after time $$i$$.

$$
    g = \sum_i \left(\mathbb{E}\left[R_{0:i-1}\nabla_\theta \log \pi(a_i|s_i)\right]
             + \mathbb{E}\left[R_{i:\infty}\nabla_\theta \log \pi(a_i|s_i)\right] \right)
$$

We can move $$R_{0:i-1}$$ out of the first expectation, since none of those rewards
depends on $$a_i$$. By the earlier lemma, this gives

$$
    g = \sum_i \left(R_{0:i-1} \cdot 0 + \mathbb{E}[R_{i:\infty}\nabla_\theta \log \pi(a_i|s_i)]   \right)
    = \sum_i \mathbb{E}[R_{i:\infty}\nabla_\theta \log \pi(a_i|s_i)]
$$

(Can show this formally by writing out the conditional expectations, but it
gets messy fast.)

Right away, this lets us drop a few terms from the gradient.

**Observation 2:** We can subtract a baseline function without biasing the gradient,
as long as the baseline has certain properties.

Recall that the expectation of the score functions is zero.

$$
    \mathbb{E}_{a_i|s_i}\left[\nabla_\theta \log \pi(a_i | s_i)\right] = 0
$$

Let $$b(s_i)$$ be a baseline function that depends on $$s_i$$. Since the
expectation is conditioned on $$s_i$$, we get

$$
    \mathbb{E}_{a_i|s_i}\left[b(s_i)\nabla_\theta \log \pi(a_i | s_i)\right] =
    b(s_i)\mathbb{E}_{a_i|s_i}\left[\nabla_\theta \log \pi(a_i | s_i)\right] = 0
$$

Because of this, we can subtract a state-dependent baseline from the cumulative
rewards without changing the expectation.

$$
    \sum_i \left(\mathbb{E}[(R_{i:\infty} - b(s_i))\nabla_\theta \log \pi(a_i|s_i)]   \right)
    = g - \sum_i \left(b(s_i)\nabla_\theta \log \pi(a_i|s_i)]   \right)
    = g
$$

In practice, people usually try to estimate $$V^{\pi}(s_i)$$ and use that as their
baseline. Intuitively, this "centers" the scaling of
$$\nabla_\theta \log \pi(a_i|s_i)$$. If we estimated $$V^{\pi}$$ exactly, the
scaling is positive if and only if the reward for taking action $$a_i$$
is better than the average.


Q-Learning

Bellman Operators

Operators map functions to functions. We can think of each application of an
operator as one step of an optimization.

Use $$\mathcal{T}$$ to denote operators.

Proofs of Convergence in Tabular Problems.

Let $$\mathcal{T}^{\pi}$$ be the Bellman operator for $$\pi$$. Define this
as

$$
    (\mathcal{T}^{\pi}Q)(s, a) = r(s,a) + \gamma \mathbb{E}_{\pi}[Q(s',a')]
$$

We now prove that if we repeatedly apply $$T^{\pi}$$ to any $$Q$$, we converge
to $$Q^{\pi}$$.

First, show that $$Q^{\pi}$$ is a fixed point. Follows by definitions.

$$
    (\mathcal{T}^{\pi}Q^{\pi})(s, a) = r(s,a) + \gamma \mathbb{E}_{\pi}[Q^{\pi}(s',a')]
    = r(s,a) + \gamma V^{\pi}(s') = Q^{\pi}(s,a)
$$

Now, prove $$\mathcal{T}^{\pi}$$ is a contraction.
An operator is a contraction if

$$
    \| \mathcal{T}^{\pi}Q_1 - \mathcal{T}^{\pi}Q_2 \| \le c \|Q_1 - Q_2\|
$$
for some $$0 \le c < 1$$ and some distance function.

Why do we want to prove this?
By the Banach fixed-point theorem, for any contraction,

* Repeatedly applying the contraction converges to a fixed point.
* That fixed point is unique.

We use max norm as the distance function. $$\|f\|_\infty = \sup_x |f(x)|$$.
In this case, the supremum is over state-action pairs $$(s,a)$$. For any
$$(s,a)$$,

$$
    \mathcal{T}^{\pi}Q_1(s,a) - \mathcal{T}^{\pi}Q_2(s,a) =
    r(s,a) + \gamma\mathbb{E}_\pi[Q_1(s',a') - (r(s,a) + \gamma\mathbb{E}_\pi(Q_2(s',a'))] =
    \gamma \mathbb{E}_\pi[ Q_1(s',a') - Q_2(s',a') ] \le
    \gamma \sup_{s',a'} (Q_1(s',a') - Q_2(s',a'))
$$

Therefore, $$\| \mathcal{T}^{\pi}Q_1 - \mathcal{T}^{\pi}Q_2 \|_\infty \le \gamma \|Q_1 - Q_2\|_\infty$$,
and since $$ 0 \le \gamma < 1$$, operator $$\mathcal{T}^{\pi}$$ is a contraction.

We've now shown the Bellman operator $$\mathcal{T}^{\pi}$$ converges to $$\pi$$.
Let $$\pi^*$$ be the optimal policy. What does the Bellman operator for that
look like?

$$
    (\mathcal{T}^*Q)(s, a) = r(s,a) + \gamma \mathbb{E}_{\pi^*}[Q(s',a')]
$$

The optimal policy $$\pi^*$$ will take the action $$a'$$ that maximizes
$$Q(s', a')$$, which converts the expectation into a max.

$$
    (\mathcal{T}^*Q)(s, a) = r(s,a) + \gamma \max_{a'}Q(s',a')
$$

This recovers the 1-step return that appears all over the place. Since this
is a special case of $$\mathcal{T}^{\pi}$$, by earlier argument this converges
to $$Q^*(s,a)$$.

In practice, we can never apply the Bellman operator exactly, except for
very small problems where we can easily iterate over all $$(s,a)$$.
Instead, we have parametrized Q-functions $$Q_\theta(s,a)$$,
then take gradient steps to minimze the TD-error

$$
    \frac{1}{2}\left(Q(s,a) - (r(s,a) + \gamma \max_{a'}Q(s', a')\right)^2
$$

With parametrized Q-functions, we are no longer guaranteed to converge,

(Soft Q-Learning is a TODO here)

Natural Policy Gradient

(Argument summarized from [https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf](original paper).)

Natural policy gradient is a special case of natural gradient, so let's
explain that first.

Suppose we have some objective function $$\eta(\theta)$$ that's
differentiable. When optimizing $$\eta(\theta)$$, Why do we step in
direction $$\nabla_\theta \eta(\theta)$$? It's because the direction
of steepest descent is the gradient. Quick argument follows.

The direction of steepest descent is the solution to 

$$
\max_{d\theta, \|d\theta\|^2 \le c} \eta(\theta) - \eta(\theta + d\theta)
$$

for some small constant $$c$$. AS $$c \to 0$$, the first order Taylor
approximation gets more accurate.

$$
    \eta(\theta + d\theta) \approx \eta(\theta) + \nabla_\theta \eta(\theta) \cdot d\theta
$$
