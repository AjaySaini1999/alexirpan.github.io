---
layout: page
title: A List of Reinforcement Learning Derivations
permalink: /rl-derivations/
---

I keep forgetting how to derive the famous reinforcement learning
results. Here's a place for me to store the proofs. They are very rough.


REINFORCE

Let $$X$$ be a random variable with known p.d.f. $$p_\theta(X)$$.
(From here on, $$\theta$$ will be omitted. In general, $$\theta$$
will be specified the first time, and will be implicit all other times.)
Let $$J(\theta)$$ be some cost function defined as

$$
    J(\theta) = \mathbb{E}[f(X)] = \int_x f(x)p(x)\, dx
$$

for some arbitrary function $$f(x)$$ that does not depend on $$\theta$$.

We want to compute
$$\nabla_\theta J(\theta)$$. Using the log derivative trick, we can
change the gradient of the expectation to the expectation of the gradient.

$$
    \nabla_\theta J(\theta) = \int_x f(x) \nabla_\theta p(x) \, dx
    = \int_x f(x) p(x) \nabla_\theta \log(p(x)) \, dx
    = \mathbb{E}[f(x) \nabla_\theta \log(p(x))]
$$

Now that this is an expectation, we can get an approximate gradient
by sampling $$x$$.

So far, none of the derivation is RL specific. Let's now suppose
the random variable is trajectories $$\tau = (s_0,a_0,s_1,a_1,s_2,\cdots)$$,
generated by policy $$\pi_\theta$$ in an MDP. Let's then have
$$f(\tau)$$ be the reward over the trajectory. $$J(\theta)$$ is
now the expected reward of the policy, and the gradient gives a
way to update the policy to increased reward.

Once we assume we're in an MDP, we get some magic cancellations.

$$
    p(\tau) = p(s_0) \prod_{i=0}^\infty \pi(a_i|s_i)p(s_{i+1}|s_i)
$$

$$
    \log p(\tau) = \log p(s_0) + \sum_{i} \left(\log \pi(a_i|s_i) + \log p(s_{i+1}|s_i) \right)
$$

Note that the dynamics ($$p(s_0)$$ and $$p(s_{i+1}|s_i)$$) do not depend on
$$\theta$$, so when we compute the gradient, they get cancelled out, leaving.

$$
    g = \nabla_\theta J(\theta) = \mathbb{E}\left[f(\tau) \sum_{i} \nabla_\theta \log \pi_\theta(a_i|s_i)\right]
$$


Expectation of Score Function is Zero.

This will be important to variance reduction.

$$
    \mathbb{E}_{a_i|s_i}\left[\nabla_\theta \log \pi(a_i | s_i)\right] =
    \int_{a_i} \pi(a_i|s_i)\nabla_\theta \log \pi(a_i | s_i) \,da_i =
    \int_{a_i} \nabla_\theta \pi(a_i|s_i) \,da_i =
    \nabla_\theta \int_{a_i} \pi(a_i|s_i) \,da_i =
    \nabla_\theta 1 = 0
$$

Any time we can move everything except the score function out of the expectation,
we can cancel that term.


REINFORCE Variance Reduction

Now that we're married to the MDP framework, there are ways to quickly
reduce variance without adding any bias.

**Observation 1:** When we take an action at timestep $$t$$, it can only
affect the rewards from timesteps $$t$$ and onward. Assuming
$$f(\tau) = R(\tau)$$, let

$$
    R_{a:b} = \sum_{i=a}^b r_i
$$

Note $$R(\tau) = R_{0:\infty}$$.

We'll use similar notation to denote trajectories. $$\tau_{a:b}$$
indicates states, actions, and rewards from time $$a$$ to $$b$$.

Slightly reorder the gradient as the sum of expectations

$$
    g = \sum_i \mathbb{E}\left[R(\tau)\nabla_\theta \log \pi(a_i|s_i)\right]
$$


Then split $$R_{0:\infty}$$ into the sum of rewards before and after time $$i$$.

$$
    g = \sum_i \left(\mathbb{E}\left[R_{0:i-1}\nabla_\theta \log \pi(a_i|s_i)\right]
             + \mathbb{E}\left[R_{i:\infty}\nabla_\theta \log \pi(a_i|s_i)\right] \right)
$$

We can move $$R_{0:i-1}$$ out of the first expectation, since none of those rewards
depends on $$a_i$$. By the earlier lemma, this gives

$$
    g = \sum_i \left(R_{0:i-1} \cdot 0 + \mathbb{E}[R_{i:\infty}\nabla_\theta \log \pi(a_i|s_i)]   \right)
    = \sum_i \mathbb{E}[R_{i:\infty}\nabla_\theta \log \pi(a_i|s_i)]
$$

(Can show this formally by writing out the conditional expectations, but it
gets messy fast.)

Right away, this lets us drop a few terms from the gradient.

**Observation 2:** We can subtract a baseline function without biasing the gradient,
as long as the baseline has certain properties.

Recall that the expectation of the score functions is zero.

$$
    \mathbb{E}_{a_i|s_i}\left[\nabla_\theta \log \pi(a_i | s_i)\right] = 0
$$

Let $$b(s_i)$$ be a baseline function that depends on $$s_i$$. Since the
expectation is conditioned on $$s_i$$, we get

$$
    \mathbb{E}_{a_i|s_i}\left[b(s_i)\nabla_\theta \log \pi(a_i | s_i)\right] =
    b(s_i)\mathbb{E}_{a_i|s_i}\left[\nabla_\theta \log \pi(a_i | s_i)\right] = 0
$$

Because of this, we can subtract a state-dependent baseline from the cumulative
rewards without changing the expectation.

$$
    g = \sum_i \left(R_{0:i-1} \cdot 0 + \mathbb{E}[R_{i:\infty}\nabla_\theta \log \pi(a_i|s_i)]   \right)

$$
